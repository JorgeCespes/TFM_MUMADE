---
title: "espa√±ol"
author: "Jorge C√©spedes Rico"
date: "2024-05-31"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(pdftools)
library(tm)
library(wordcloud)
library(RColorBrewer)
library(topicmodels)
library(textstem)
library(dplyr)
library(tidyr)
library(tidytext)
library(igraph)
library(ggraph)
library(cluster)
library(factoextra)
library(Rtsne)
library(viridis)
```

```{r}
# Tema personalizado para ggplot2
theme_elegante <- function(base_size = 10,
                           base_family = "Raleway"
) {
  color.background <- "#FFFFFF" # Fondo del gr√°fico
  color.grid.major <- "#D9D9D9" # L√≠neas de la cuadr√≠cula
  color.axis.text <- "#666666" # Texto del eje
  color.axis.title <- "#666666" # T√≠tulo del eje
  color.title <- "#666666" # T√≠tulo principal
  color.subtitle <- "#666666" # Subt√≠tulo
  strip.background.color <- '#9999CC' # Fondo de la franja
  
  ret <- theme_bw(base_size=base_size) +
    # Establecer el color de fondo del panel
    theme(panel.background=element_rect(fill=color.background, color=color.background)) +
    theme(plot.background=element_rect(fill=color.background, color=color.background)) +
    theme(panel.border=element_rect(color=color.background)) +
    # Formato de la cuadr√≠cula
    theme(panel.grid.major=element_line(color=color.grid.major,size=.55, linetype="dotted")) +
    theme(panel.grid.minor=element_line(color=color.grid.major,size=.55, linetype="dotted")) +
    theme(axis.ticks=element_blank()) +
    # Formato de la leyenda
    theme(legend.position="none") +
    theme(legend.background = element_rect(fill=color.background)) +
    theme(legend.text = element_text(size=base_size-3,color=color.axis.title, family = base_family)) +
    theme(strip.text.x = element_text(size=base_size,color=color.background, family = base_family)) +
    theme(strip.text.y = element_text(size=base_size,color=color.background, family = base_family)) +
    theme(strip.background = element_rect(fill = "grey70", colour = NA)) +
    theme(plot.title=element_text(color=color.title, 
                                  size=20, 
                                  vjust=1.25, 
                                  family=base_family, 
                                  hjust = 0.5
    )) +
    theme(plot.subtitle=element_text(color=color.subtitle, size=base_size+2, family = base_family,  hjust = 0.5))  +
    theme(axis.text.x=element_text(size=base_size,color=color.axis.text, family = base_family)) +
    theme(axis.text.y=element_text(size=base_size,color=color.axis.text, family = base_family)) +
    theme(text=element_text(size=base_size, color=color.axis.text, family = base_family)) +
    theme(axis.title.x=element_text(size=base_size+2,color=color.axis.title, vjust=0, family = base_family)) +
    theme(axis.title.y=element_text(size=base_size+2,color=color.axis.title, vjust=1.25, family = base_family)) +
    theme(plot.caption=element_text(size=base_size-2,color=color.axis.title, vjust=1.25, family = base_family)) +
    # Formato de la leyenda
    theme(legend.text=element_text(size=base_size,color=color.axis.text, family = base_family)) +
    theme(legend.title=element_text(size=base_size,color=color.axis.text, family = base_family)) +
    theme(legend.key=element_rect(colour = color.background, fill = color.background)) +
    theme(legend.position="bottom", 
          legend.box = "horizontal", 
          legend.title = element_blank(),
          legend.key.width = unit(.75, "cm"),
          legend.key.height = unit(.75, "cm"),
          legend.spacing.x = unit(.25, 'cm'),
          legend.spacing.y = unit(.25, 'cm'),
          legend.margin = margin(t=0, r=0, b=0, l=0, unit="cm")) +
    # M√°rgenes del gr√°fico
    theme(plot.margin = unit(c(.5, .5, .5, .5), "cm"))
  
  ret
}
```

```{r}
# Listar todos los archivos PDF en el directorio de trabajo
files <- list.files(pattern = "pdf$")
files

# Aplicar la funci√≥n pdf_text a cada archivo PDF
espa√±ol <- lapply(files, pdf_text)

# Combinar todos los textos en un √∫nico vector de caracteres
espa√±ol_text <- unlist(espa√±ol)

# Crear un corpus de texto a partir del texto extra√≠do
corpus <- Corpus(VectorSource(espa√±ol_text))
```

```{r}
# Limpiar los datos de texto
additional_stopwords <- c("et al", "o o", "p p", "por ciento", "https doi", "doi org", "https doi org", "ser ser ser", "t t t", "s√≠ s√≠ s√≠", "a√±o edad edad", "o o o", "lad claimant count", "‚Äù", "‚Äú", "and", "the", "‚Äì", "work")
clean_corpus <- corpus %>%
  tm_map(content_transformer(tolower)) %>%
  tm_map(removePunctuation) %>%
  tm_map(removeNumbers) %>%
  tm_map(removeWords, c(stopwords("spanish"), additional_stopwords)) %>%
  tm_map(stripWhitespace) %>%
  tm_map(content_transformer(lemmatize_strings))
```

```{r}
# Crear una matriz de t√©rminos del documento (DTM)
dtm <- DocumentTermMatrix(clean_corpus)
```

```{r}
# Calcular TF-IDF
tfidf <- weightTfIdf(dtm)
tfidf_matrix <- as.matrix(tfidf)

# Calcular la suma de TF-IDF para cada t√©rmino
top_terms <- sort(colSums(tfidf_matrix), decreasing = TRUE)

# Convertir a un data frame
top_terms_df <- data.frame(term = names(top_terms), tfidf = top_terms)

```


```{r}
# Filtrar t√©rminos espec√≠ficos y aquellos que contienen "..." o "'" o "‚Äô"
top_terms_df <- top_terms_df %>%
  filter(!term %in% c("‚Åé", "‚àí", "s√≠", "std", "unstd", "ref", "swb", "pwb", "almps", "‚Äì", "*", "ùñ•","the", "and", "‚Äù", "‚Äú", "arbetslivsinstitutet", "n√∫m", "health", "n√§swall","***", "‚àó‚àó‚àó", "‚Äî")) %>%
  filter(!grepl("\\.\\.\\.", term)) %>%  # Filtrar t√©rminos que contienen "..."
  filter(!grepl("'", term)) %>%  # Filtrar t√©rminos que contienen "'"
  filter(!grepl("‚Äô", term)) %>%  # Filtrar t√©rminos que contienen "‚Äô"
  arrange(desc(tfidf)) %>%
  head(15)

# Imprimir el data frame resultante
print(top_terms_df)

# Plot de los t√©rminos principales por TF-IDF con una paleta de colores diferente
ggplot(top_terms_df, aes(x = reorder(term, tfidf), y = tfidf, fill = term)) +
  geom_bar(stat = "identity", color = "black") +
  labs(title = "Top 15 T√©rminos: Literatura en espa√±ol", x = "T√©rmino", y = "TF-IDF") +
  theme_elegante() +
   theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1), 
        legend.position = "none",
        axis.title.x = element_text(face = "bold"),  # T√≠tulo del eje x en negrita
        axis.title.y = element_text(face = "bold"))  # T√≠tulo del eje y en negrita

top_terms_df

```



```{r}
# Eliminar t√©rminos escasos para reducir el ruido y la dimensionalidad
dtm_sparse <- removeSparseTerms(dtm, 0.99)
dtm_sparse_matrix <- as.matrix(dtm_sparse)

# Determinar el n√∫mero √≥ptimo de clusters usando el m√©todo del codo
fviz_nbclust(dtm_sparse_matrix, kmeans, method = "wss")

# Aplicar k-means clustering con un n√∫mero √≥ptimo de clusters (por ejemplo, k = 3)
set.seed(1234)
num_clusters <- 5
kmeans_result <- kmeans(dtm_sparse_matrix, centers = num_clusters, nstart = 25)

# A√±adir las asignaciones de clusters a los datos originales
espa√±ol_clusters <- data.frame(text = espa√±ol_text, cluster = kmeans_result$cluster)

# Imprimir el resultado del clustering
print(espa√±ol_clusters)

# Visualizar el resultado del clustering usando PCA
pca <- prcomp(dtm_sparse_matrix, scale. = TRUE)
pca_data <- data.frame(pca$x, cluster = as.factor(kmeans_result$cluster))

ggplot(pca_data, aes(x = PC1, y = PC2, color = cluster)) +
  geom_point() +
  labs(title = "PCA de los textos", x = "Componente Principal 1", y = "Componente Principal 2") +
  theme_elegante()

# Visualizar el resultado del clustering usando t-SNE
# Eliminar duplicados antes de aplicar t-SNE
dtm_sparse_matrix_unique <- unique(dtm_sparse_matrix)

set.seed(1234)
tsne_result <- Rtsne(dtm_sparse_matrix_unique, dims = 2, perplexity = 5, verbose = TRUE, max_iter = 500)
tsne_data <- data.frame(tsne_result$Y, cluster = as.factor(kmeans_result$cluster[match(rownames(dtm_sparse_matrix_unique), rownames(dtm_sparse_matrix))]))
colnames(tsne_data) <- c("Dim1", "Dim2", "cluster")

ggplot(tsne_data, aes(x = Dim1, y = Dim2, color = cluster)) +
  geom_point() +
  labs(title = "t-SNE de los textos", x = "Dimensi√≥n 1", y = "Dimensi√≥n 2") +
  theme_elegante()
```

```{r}
library(ggwordcloud)
# Combine all cleaned text into a single data frame
cleaned_text <- data.frame(text = sapply(clean_corpus, as.character), stringsAsFactors = FALSE)

# Remove unwanted characters (".", "...", "'", "‚Äô", "s", "¬∑", "‚Äú‚Äù, etc.)
cleaned_text$text <- gsub("[\\.]{2,}", "", cleaned_text$text)  # Remove multiple dots
cleaned_text$text <- gsub("'", "", cleaned_text$text)
cleaned_text$text <- gsub("‚Äô", "", cleaned_text$text)
cleaned_text$text <- gsub("¬∑", "", cleaned_text$text)
cleaned_text$text <- gsub("and|the|of|be|j‚Äô‚Äô|‚Äú|‚Äù", "", cleaned_text$text)  # Remove specific words and characters
cleaned_text$text <- gsub("[[:punct:]]", "", cleaned_text$text)  # Remove punctuation
cleaned_text$text <- gsub("\\s+", " ", cleaned_text$text)  # Remove extra whitespace

# Tokenize the text
words <- unlist(strsplit(cleaned_text$text, " "))

# Remove empty strings and unwanted short words from the tokenized words
unwanted_words <- c("p", "o", "ob", "s", "j", "t", "si", "to", "as√≠", "in", "work", "c")
words <- words[words != "" & !words %in% unwanted_words]

# Create word frequency table
word_freq <- table(words)
word_freq_df <- as.data.frame(word_freq)
colnames(word_freq_df) <- c("word", "freq")

# Filter words with a minimum frequency
word_freq_df <- word_freq_df[word_freq_df$freq >= 100,]

# Plot word cloud using ggwordcloud
ggplot(word_freq_df, aes(label = word, size = freq, color = freq)) +
  geom_text_wordcloud_area(rm_outside = TRUE) +  # Ensure words don't overlap and stay within bounds
  scale_size_area(max_size = 30) +
  scale_color_viridis_c() +  # Set color scheme for the text
  theme_elegante() +
  labs(title = "Revisi√≥n de la literatura en espa√±ol", 
       subtitle = "M√≠nima frecuencia: 100") +
  theme(
    plot.title = element_text(size = 20, hjust = 0.5, vjust = 1, face = "bold"),  # Centered title at the top
    plot.subtitle = element_text(size = 12, hjust = 0.5, color = "darkgrey"),  # Centered subtitle below the title
    plot.margin = margin(10, 10, 10, 10)
  )

```


```{r}

# Combine all cleaned text into a single data frame
cleaned_text <- data.frame(text = sapply(clean_corpus, as.character), stringsAsFactors = FALSE)

# Remove unwanted characters (".", "...", "'", "‚Äô", "s", "¬∑", "‚Äú‚Äù, etc.)
cleaned_text$text <- gsub("[\\.]{2,}", "", cleaned_text$text)  # Remove multiple dots
cleaned_text$text <- gsub("'", "", cleaned_text$text)
cleaned_text$text <- gsub("‚Äô", "", cleaned_text$text)
cleaned_text$text <- gsub("¬∑", "", cleaned_text$text)
cleaned_text$text <- gsub("and|j|the|of|be|in|j‚Äô‚Äô|‚Äú|‚Äù", "", cleaned_text$text)  # Remove specific words and characters
cleaned_text$text <- gsub("[[:punct:]]", "", cleaned_text$text)  # Remove punctuation
cleaned_text$text <- gsub("\\s+", " ", cleaned_text$text)  # Remove extra whitespace

# Tokenize the text
words <- unlist(strsplit(cleaned_text$text, " "))

# Remove empty strings from the tokenized words
words <- words[words != ""]

# Create word frequency table
word_freq <- table(words)

# Plot word cloud of most frequent words with viridis colors
wordcloud(words = names(word_freq), freq = word_freq, min.freq = 150, 
          scale = c(5, 0.3), colors = viridis(100))


```


```{r}
# Combinar todo el texto limpiado en un √∫nico marco de datos
cleaned_text <- data.frame(text = sapply(clean_corpus, as.character), stringsAsFactors = FALSE)

# Tokenizar el texto en bigramas
bigrams <- cleaned_text %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

# Contar la frecuencia de cada bigrama
bigram_counts <- bigrams %>%
  count(bigram, sort = TRUE)

# Filtrar los bigramas no deseados
bigram_counts_filtered <- bigram_counts %>%
  filter(!bigram %in% c("p p", "https doi", "doi org", "t t", "o o", "be be", "ùêä ùêä", "journal of", "mental health", "of the", "in the"))

# Filtrar para mantener solo los 20 bigramas m√°s frecuentes
top_bigrams <- bigram_counts_filtered %>%
  top_n(20, wt = n)

# Visualizar los 20 bigramas m√°s frecuentes sin leyenda
ggplot(top_bigrams, aes(x = reorder(bigram, n), y = n, fill = reorder(bigram, n))) +
  geom_col() +
  coord_flip() +
  labs(title = "Top 20 bigramas: Literatura en espa√±ol", x = "Bigrama", y = "Frecuencia") +
  theme_elegante() +                      # Aplicar el tema elegante
  theme(axis.text.x = element_text(angle = 45, hjust = 1), 
        legend.position = "none", 
        axis.title.x = element_text(face = "bold"),   # Poner en negrita el t√≠tulo del eje x
        axis.title.y = element_text(face = "bold"),   # Poner en negrita el t√≠tulo del eje y
        plot.title = element_text(size = 12))         # Ajustar el tama√±o del t√≠tulo del gr√°fico

# Tokenizar el texto en trigramas
trigrams <- cleaned_text %>%
  unnest_tokens(trigram, text, token = "ngrams", n = 3)

# Contar la frecuencia de cada trigram
trigram_counts <- trigrams %>%
  count(trigram, sort = TRUE)

# Filtrar los trigramas no deseados
trigram_counts_filtered <- trigram_counts %>%
  filter(!trigram %in% c("https doi org", "be be be", "yes yes yes", "yr yr yr", "p p p", "age age age", "ppi age year", "t t t", "j environ res", "int j environ", "environ res public", "claimant count rate", "res public health", "publisher ltd journal", "macmillan publisher ltd", "ltd journal public", "claimant count ratw", "soc sci med", "download ilrsagepubcom howard", "howard univ undergrad", "ilrsagepubcom howard univ", "undergrad library march", 			
"univ undergrad library", "q q q", "be bubonya labour", "tennessee state university", "state university june", "ilrsagepubcom east tennessee", "download ilrsagepubcom east", "bubonya labour economics", "ilrsagepubcom east tennessee", "east tennessee state", "richard layard economics", "labor january wolizaorg", "√©poca n√∫m", "n√∫m extraordinariofebrero", "√©poca n√∫m", "of the", "rejie nueva √©poca", "nueva √©poca n√∫m", "rejie nueva √©poca", "√©poca n√∫m extraordinariofebrero", "ra√∫l porras vel√°squez", "n√©stor ra√∫l porras", "susana rodr√≠guez escanciano", "apuntes cuestiones pendientes", "of the", "apunte cuestiones pendientes", "fuente elaboraci√≥n propia", "empirico caso argentino", "toma valor si", "elaboraci√≥n propia base", "caso argentino"))

# Filtrar para mantener solo los 20 trigramas m√°s frecuentes
top_trigrams <- trigram_counts_filtered %>%
  top_n(20, wt = n)

# Visualizar los 20 trigramas m√°s frecuentes sin leyenda
ggplot(top_trigrams, aes(x = reorder(trigram, n), y = n, fill = reorder(trigram, n))) +
  geom_col() +
  coord_flip() +
  labs(title = "Top 20 trigramas: Literatura en espa√±ol", x = "Trigrama", y = "Frecuencia") +
  theme_elegante() +                      # Aplicar el tema elegante
  theme(axis.text.x = element_text(angle = 45, hjust = 1), 
        legend.position = "none", 
        axis.title.x = element_text(face = "bold"),   # Poner en negrita el t√≠tulo del eje x
        axis.title.y = element_text(face = "bold"),   # Poner en negrita el t√≠tulo del eje y
        plot.title = element_text(size = 12))         # Ajustar el tama√±o del t√≠tulo del gr√°fico
```



```{r}
# Definir una lista de palabras no deseadas
unwanted_words <- c("di", "yr", "doi", "org", "o", "p", "https", "res", 
                    "claimant count", "std", "unstd", "be", "garciag√≥mez", 
                    "int", "macmillan", "ppi", "yes", "claimant", "count", "iec", "fjmd", "t", "de", "witte", "j", "plos", "one",
                    "ùêä", "soc", "sci", "download", "ilrsagepubcom", "howard", "univ", "undergrad", "ilrsagepubcom", "httpdoiorgjournalpone", "wiley", "kk", "q", "wiley", "john", "copyright", "sarti", "zella", "ltd", "bubonya", "tennessee", "east", "ilrsagepubcom", "june", "richard", "e", "layard", "n", "joan", "proto", "eugenio", "toma", "si", "fuente", "propia", "elaboraci√≥n", "trlgss", "have", "medium", "sa", "lud", "sfconsult", "sfmalestar", "argentino", "of", "siempro", "sfinterr", "sfremedio", "in", "the", "journal", "n¬∫", "extraordinariofebrero", "public", "health", "susana", "rodr√≠guez", "ra√∫l", "n√©stor", "porras", "s", "rejie", "julio", "diciembre", "gac", "sanit", "p√°gs", "s", "ref")

# Tokenize the text into bigrams
bigrams <- cleaned_text %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

# Separate the bigrams into two columns
bigrams_separated <- bigrams %>%
  separate(bigram, into = c("word1", "word2"), sep = " ")

# Filtrar las palabras no deseadas
bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% unwanted_words & !word2 %in% unwanted_words)

# Count the frequency of each bigram
bigram_counts <- bigrams_filtered %>%
  count(word1, word2, sort = TRUE)

# Filter to keep only the top 60 bigrams for visualization
top_bigrams <- bigram_counts %>%
  top_n(60, wt = n)

# Create a graph object using igraph
bigram_graph <- graph_from_data_frame(top_bigrams)

# Plot the bigram network using ggraph
set.seed(1234)
ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), edge_width = 0.5, edge_colour = "red3",  # Cambio de color a "darkgray"
                 arrow = arrow(type = "closed", length = unit(0.2, "inches"))) +
  geom_node_point(size = 5, color = "darkolivegreen3") +
  geom_node_text(aes(label = name), repel = TRUE, size = 3) +
  theme_void() +
  labs(title = "Literatura en espa√±ol", 
       subtitle = "Top 60")

# Tokenize the text into trigrams
trigrams <- cleaned_text %>%
  unnest_tokens(trigram, text, token = "ngrams", n = 3)

# Separate the trigrams into three columns
trigrams_separated <- trigrams %>%
  separate(trigram, into = c("word1", "word2", "word3"), sep = " ")

# Filtrar las palabras no deseadas
trigrams_filtered <- trigrams_separated %>%
  filter(!word1 %in% unwanted_words & !word2 %in% unwanted_words & !word3 %in% unwanted_words)

# Count the frequency of each trigram
trigram_counts <- trigrams_filtered %>%
  count(word1, word2, word3, sort = TRUE)

# Filter to keep only the top 40 trigrams for visualization
top_trigrams <- trigram_counts %>%
  top_n(60, wt = n)

# Create a graph object using igraph
trigram_graph <- graph_from_data_frame(top_trigrams, directed = TRUE)

# Plot the trigram network using ggraph
# Plot the trigram network using ggraph with kk layout
set.seed(1234)
ggraph(trigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), edge_width = 0.5, 
                 arrow = arrow(type = "closed", length = unit(0.2, "inches")), 
                 edge_colour = "sienna") +  # Cambio de color a "darkgray"
  geom_node_point(size = 5, color = "steelblue3") +
  geom_node_text(aes(label = name), repel = TRUE, size = 3) +
  theme_void() +
  labs(title = "Literatura en espa√±ol", 
       subtitle = "Top 60")


```


```{r}
# Funci√≥n para crear matriz de co-ocurrencia
crear_matriz_coocurrencia <- function(dtm) {
  dtm_matriz <- as.matrix(dtm)
  matriz_terminos <- t(dtm_matriz) %*% dtm_matriz
  matriz_terminos[lower.tri(matriz_terminos, diag = TRUE)] <- 0
  matriz_terminos
}

# Crear la matriz de co-ocurrencia
matriz_coocurrencia <- crear_matriz_coocurrencia(dtm)

# Convertir la matriz de co-ocurrencia en un dataframe
coocurrencia_df <- as.data.frame(as.table(matriz_coocurrencia))
colnames(coocurrencia_df) <- c("termino1", "termino2", "frecuencia")

# Filtrar t√©rminos con baja co-ocurrencia y eliminar caracteres no deseados
coocurrencia_df <- coocurrencia_df %>%
  filter(frecuencia >1000)%>%
  filter(!termino1 %in% c("‚Äî", "‚Äì", "‚Äô", "‚Äò", "‚àí", "‚Åé", "F", "‚â•", "‚àó‚àó", "‚Äú", "‚Äù", "‚Äô") & 
         !termino2 %in% c("‚Äî", "‚Äì", "‚Äú", "‚Äù", "‚Äô", "‚Äò", "‚àí", "‚Åé", "F", "‚àó‚àó‚àó"))

# Crear un grafo a partir de la matriz de co-ocurrencia
grafo_coocurrencia <- graph_from_data_frame(coocurrencia_df, directed = FALSE)

# Aumentar la transparencia de las l√≠neas y ajustar el tama√±o del grafo
set.seed(175)  # Asegurar la reproducibilidad
ggraph(grafo_coocurrencia, layout = "fr") +
  geom_edge_link(aes(edge_alpha = frecuencia, edge_width = frecuencia), color = "pink3", show.legend = TRUE) +
  geom_node_point(color = "springgreen3", size = 4) +
  geom_node_text(aes(label = name), repel = TRUE, size = 3) +  # Usar repel para mejorar la distribuci√≥n de etiquetas
  theme_void() +
  labs(title = "Grafo de Co-ocurrencia de T√©rminos") +
  theme(plot.title = element_text(size = 12, face = "bold"),
        plot.subtitle = element_text(size = 10),
        plot.caption = element_text(size = 8, hjust = 0))

```

```{r}
# Funci√≥n para crear matriz de concurrencia
create_cooccurrence_matrix <- function(dtm) {
  dtm_matrix <- as.matrix(dtm)
  term_matrix <- t(dtm_matrix) %*% dtm_matrix
  term_matrix[lower.tri(term_matrix, diag = TRUE)] <- 0
  term_matrix
}

# Crear la matriz de concurrencia
cooccurrence_matrix <- create_cooccurrence_matrix(dtm)

# Convertir la matriz de concurrencia en un dataframe
cooccurrence_df <- as.data.frame(as.table(cooccurrence_matrix))
colnames(cooccurrence_df) <- c("term1", "term2", "frequency")

# Filtrar t√©rminos con concurrencia baja y eliminar caracteres no deseados
cooccurrence_df <- cooccurrence_df %>%
  filter(frequency > 1000) %>%
  filter(!term1 %in% c("‚Äì", "‚Äô", "‚Äò", "‚àí", "‚Åé", "‚Äú", "‚Äù", "ùñ•", "‚àó‚àó‚àó", "‚àó‚àó", "‚â•", "work", "and","health", "the", "employment") & 
         !term2 %in% c("‚Äì", "‚Äô", "‚Äò", "‚àí", "‚Åé", "‚Äú", "‚Äù", "ùñ•", "‚àó‚àó‚àó", "‚àó‚àó", "‚â•", "work", "and","health", "the", "employment"))

# Crear un grafo a partir de la matriz de concurrencia
cooccurrence_graph <- graph_from_data_frame(cooccurrence_df, directed = FALSE)

# Aumentar la transparencia de las l√≠neas y ajustar el tama√±o del grafo
set.seed(175)  # Asegura la reproducibilidad
ggraph(cooccurrence_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = frequency, edge_width = frequency), color = "pink3", show.legend = TRUE) +
  geom_node_point(color = "springgreen3", size = 4) +
  geom_node_text(aes(label = name), repel = TRUE, size = 3.25) +  # Usar repel para mejorar la distribuci√≥n de etiquetas
  scale_edge_width_continuous(name = "n") +
  scale_edge_alpha_continuous(name = "n") +
  theme_void() +
  labs(title = "Literatura en espa√±ol", 
       subtitle = "Frecuencia > 1000") +
  theme(plot.title = element_text(size = 12, face = "bold"),
        plot.subtitle = element_text(size = 10),
        plot.caption = element_text(size = 8, hjust = 1))

```




```{r}
library(textstem)
library(factoextra)
library(wordcloud)
library(stylo)
```
```{r}
# Crear la matriz TF-IDF
text_dtm_tfidf <- DocumentTermMatrix(clean_corpus, control = list(weighting = weightTfIdf))
tf_idf_mat <- as.matrix(text_dtm_tfidf)

# Eliminar t√©rminos con todos los valores en cero
tf_idf_mat <- tf_idf_mat[, colSums(tf_idf_mat) != 0]

# Calcular la distancia del coseno
d1 <- dist.cosine(tf_idf_mat)

# Verificar la matriz de distancias
if (any(is.na(d1)) || any(is.infinite(d1))) {
  stop("La matriz de distancias contiene valores NA o Inf.")
}

# Realizar el clustering jer√°rquico
cluster2 <- hcut(d1, k = 13, stand = TRUE, hc_method = "ward.D2", hc_func = "hclust", rect = TRUE)

# Visualizar el dendrograma
fviz_dend(cluster2, rect = TRUE, horiz = FALSE, cex = 0.5)

# Visualizar la silueta de los clusters
fviz_silhouette(cluster2)

# Visualizar los clusters
fviz_cluster(cluster2)

# Visualizar la matriz de distancias
fviz_dist(d1, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))

```
```{r}
# Verificar si hay documentos vac√≠os
empty_docs <- which(rowSums(tf_idf_mat) == 0)
if (length(empty_docs) > 0) {
  stop("Hay documentos vac√≠os en el corpus.")
}
```
```{r}
# Eliminar documentos vac√≠os del corpus
clean_corpus <- clean_corpus[which(rowSums(as.matrix(DocumentTermMatrix(clean_corpus))) != 0)]
```

```{r}
# Crear la matriz TF-IDF sin documentos vac√≠os
text_dtm_tfidf <- DocumentTermMatrix(clean_corpus, control = list(weighting = weightTfIdf))
tf_idf_mat <- as.matrix(text_dtm_tfidf)

# Eliminar t√©rminos con todos los valores en cero
tf_idf_mat <- tf_idf_mat[, colSums(tf_idf_mat) != 0]

```
```{r}
# Calcular la distancia del coseno
d1 <- dist.cosine(tf_idf_mat)

# Verificar si la matriz de distancias contiene valores NA, NaN o Inf
if (any(is.na(d1)) || any(is.infinite(d1))) {
  stop("La matriz de distancias contiene valores NA o Inf.")
}

```

```{r}
# Realizar el clustering jer√°rquico
cluster2 <- hcut(d1, k = 13, stand = TRUE, hc_method = "ward.D2", hc_func = "hclust", rect = TRUE)

# Visualizar el dendrograma
fviz_dend(cluster2, rect = TRUE, horiz = FALSE, cex = 0.5)

# Visualizar la silueta de los clusters
fviz_silhouette(cluster2)

# Visualizar los clusters
fviz_cluster(cluster2)

# Visualizar la matriz de distancias
fviz_dist(d1, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
```

```{r}

# Crear una tabla de resumen para encontrar el n√∫mero √≥ptimo de clusters
max_cut_level <- min(50, nrow(dist_mat))
clust_cuts <- data.frame(cut_level = 1:max_cut_level, avg_size = 0, avg_dist = 0)

for (i in 1:(max_cut_level - 1)) {
  clust_cuts[i, 'avg_size'] <- mean(table(cutree(tree = hclust(dist_mat), k = i)))
  dist1 <- data.frame(doc_name = rownames(dist_mat),
                      clust_cut = cutree(tree = hclust(dist_mat), k = i)) %>%
    inner_join(x = ., y = ., by = 'clust_cut') %>%
    filter(doc_name.x != doc_name.y)
  dist1$cos_dist <- NA
  for (t in 1:nrow(dist1)) {
    dist1$cos_dist[t] <- dist_mat[dist1$doc_name.x[t], dist1$doc_name.y[t]]
  }
  dist1 <- dist1 %>%
    group_by(clust_cut) %>%
    summarise(cos_dist = mean(cos_dist))
  
  clust_cuts[i, 'avg_dist'] <- mean(dist1$cos_dist)
}

# Graficar la distancia media intra-cluster para diferentes niveles de corte
library(ggplot2)
ggplot(data = clust_cuts, aes(x = cut_level, y = avg_dist)) +
  geom_line(color = 'black', size = 1) +
  labs(title = 'Intra-Cluster mean Cosine Distance by Tree Cut Level',
       x = 'Number of Final Clusters',
       y = 'Intra-Cluster Mean Cosine Distance' ) +
  geom_vline(xintercept = c(13, 15), linetype = c("twodash", "dashed"), colour = c("red","green4")) +
  geom_vline(xintercept = c(24), linetype = "longdash", colour = "blue") +
  geom_point(size = 3, aes(x = 13, y = as.numeric(clust_cuts[13, 'avg_dist']), color = "green4")) +
  geom_text(data = clust_cuts, aes(x = 4.3, y = as.numeric(clust_cuts[13, 'avg_dist']) - 0.02, label = "13 Clusters", colour = "green4", fontface = "bold")) +
  geom_point(size = 2, aes(x = 15, y = as.numeric(clust_cuts[15, 'avg_dist']), color = "red")) +
  geom_text(data = clust_cuts, aes(x = 23, y = as.numeric(clust_cuts[15, 'avg_dist']) - 0.02, label = "15 Clusters", colour = "red")) +
  scale_x_continuous(limits = c(1, max_cut_level), breaks = seq(0, max_cut_level, by = 10), labels = seq(0, max_cut_level, by = 10)))
```

```{r}
# Crear una tabla de resumen para encontrar el n√∫mero √≥ptimo de clusters
max_cut_level <- min(50, length(clean_corpus))
clust_cuts <- data.frame(cut_level = 1:max_cut_level, avg_size = 0, avg_dist = 0)

for (i in 1:(max_cut_level - 1)) {
  if (length(clean_corpus) >= 2) {
    # Crear una matriz de t√©rminos del documento (DTM) para el subconjunto de documentos
    dtm <- DocumentTermMatrix(clean_corpus[1:i])
    
    # Calcular la distancia del coseno entre documentos
    d1 <- dist(as.matrix(dtm))
    
    if (nrow(d1) >= 2) {
      # Calcular los clusters jer√°rquicos
      cluster_tree <- hclust(d1)
      
      # Obtener los clusters para el nivel actual
      clusters <- cutree(cluster_tree, k = i)
      
      # Calcular el tama√±o promedio de los clusters
      clust_cuts[i, 'avg_size'] <- mean(table(clusters))
      
      # Calcular la distancia promedio entre documentos en cada cluster
      avg_dists <- sapply(unique(clusters), function(cluster) {
        indices <- which(clusters == cluster)
        if (length(indices) > 1) {
          mean(d1[indices])
        } else {
          0
        }
      })
      
      # Calcular la distancia promedio de todos los clusters
      clust_cuts[i, 'avg_dist'] <- mean(avg_dists)
    }
  }
}

# Graficar la distancia media intra-cluster para diferentes niveles de corte
library(ggplot2)
ggplot(data = clust_cuts, aes(x = cut_level, y = avg_dist)) +
  geom_line(color = 'black', size = 1) +
  labs(title = 'Intra-Cluster mean Cosine Distance by Tree Cut Level',
       x = 'Number of Final Clusters',
       y = 'Intra-Cluster Mean Cosine Distance' ) +
  geom_vline(xintercept = c(13, 15), linetype = c("twodash", "dashed"), colour = c("red","green4")) +
  geom_vline(xintercept = c(24), linetype = "longdash", colour = "blue") +
  geom_point(size = 3, aes(x = 13, y = as.numeric(clust_cuts[13, 'avg_dist']), color = "green4")) +
  geom_text(data = clust_cuts, aes(x = 4.3, y = as.numeric(clust_cuts[13, 'avg_dist']) - 0.02, label = "13 Clusters", colour = "green4", fontface = "bold")) +
  geom_point(size = 2, aes(x = 15, y = as.numeric(clust_cuts[15, 'avg_dist']), color = "red")) +
  geom_text(data = clust_cuts, aes(x = 23, y = as.numeric(clust_cuts[15, 'avg_dist']) - 0.02, label = "15 Clusters", colour = "red")) +
  scale_x_continuous(limits = c(1, max_cut_level), breaks = seq(0, max_cut_level, by = 10), labels = seq(0, max_cut_level, by = 10))


```

```{r}
# Visualizar una nube de palabras de un cluster como ejemplo
wordcloud::wordcloud(words = names(cluster_words[[13]]), 
                     freq = cluster_words[[13]], 
                     scale = c(5, 0.5),  # Establecer el rango de escalas de tama√±o de palabra
                     max.words = 50, 
                     random.order = FALSE, 
                     colors = viridis(100),
                     main = "Top words in cluster")


```
```{r}

# Crear una funci√≥n para extraer texto de archivos PDF
extract_text_from_pdf <- function(file_path) {
  tryCatch({
    text <- pdf_text(file_path)
    text <- unlist(strsplit(text, "\n"))
    text <- text[text != ""]  # Eliminar l√≠neas vac√≠as
    return(text)
  }, error = function(e) {
    cat("Error al extraer texto del archivo:", file_path, "\n")
    return(NULL)
  })
}

# Extraer texto de todos los archivos PDF
pdf_texts <- lapply(files, extract_text_from_pdf)

# Limpiar y preprocesar el texto
clean_text <- lapply(pdf_texts, function(text) {
  text <- tolower(text)  # Convertir a min√∫sculas
  text <- removePunctuation(text)  # Eliminar signos de puntuaci√≥n
  text <- removeNumbers(text)  # Eliminar n√∫meros
  text <- removeWords(text, stopwords("spanish"))  # Eliminar palabras comunes en espa√±ol
  text <- stripWhitespace(text)  # Eliminar espacios en blanco adicionales
  return(text)
})

# Crear un corpus de texto a partir del texto limpio
corpus <- Corpus(VectorSource(clean_text))

# Crear la matriz de t√©rminos del documento (DTM)
dtm <- DocumentTermMatrix(corpus)

# Eliminar t√©rminos escasos para reducir el ruido y la dimensionalidad
dtm_sparse <- removeSparseTerms(dtm, 0.99)


# Crear la matriz de t√©rminos del documento (DTM)
dtm <- DocumentTermMatrix(corpus)

# Eliminar t√©rminos escasos para reducir el ruido y la dimensionalidad
dtm_sparse <- removeSparseTerms(dtm, 0.99)

# Convertir la matriz DTM a una matriz regular
dtm_matrix <- as.matrix(dtm_sparse)

# Determinar el n√∫mero √≥ptimo de clusters usando el m√©todo del codo
fviz_nbclust(dtm_matrix, kmeans, method = "wss")

# Aplicar k-means clustering con un n√∫mero √≥ptimo de clusters (por ejemplo, k = 3)
set.seed(123)
num_clusters <- 3
kmeans_result <- kmeans(dtm_matrix, centers = num_clusters, nstart = 25)

# A√±adir las asignaciones de clusters a los datos originales
document_clusters <- data.frame(text = clean_text, cluster = kmeans_result$cluster)

# Imprimir el resultado del clustering
print(document_clusters)

# Visualizar el resultado del clustering usando PCA
pca <- prcomp(dtm_matrix, scale. = TRUE)
pca_data <- data.frame(pca$x, cluster = as.factor(kmeans_result$cluster))

ggplot(pca_data, aes(x = PC1, y = PC2, color = cluster)) +
  geom_point() +
  labs(title = "PCA de Cl√∫steres de Documentos", x = "Componente Principal 1", y = "Componente Principal 2") +
  theme_minimal()
```

```{r}
# Cargar las librer√≠as necesarias
library(pdftools)
library(tm)
library(wordcloud)
library(RColorBrewer)
library(topicmodels)
library(textstem)
library(dplyr)
library(tidyr)
library(tidytext)
library(igraph)
library(ggraph)
library(cluster)
library(factoextra)
library(Rtsne)
library(quanteda)
library(quanteda.textplots)
library(readtext)

# Definir el tema elegante para los gr√°ficos
theme_elegante <- function(base_size = 10, base_family = "Raleway") {
  color.background = "#FFFFFF"
  color.grid.major = "#D9D9D9"
  color.axis.text = "#666666"
  color.axis.title = "#666666"
  color.title = "#666666"
  color.subtitle = "#666666"
  strip.background.color = '#9999CC'
  
  theme_bw(base_size = base_size) +
    theme(panel.background = element_rect(fill = color.background, color = color.background)) +
    theme(plot.background = element_rect(fill = color.background, color = color.background)) +
    theme(panel.border = element_rect(color = color.background)) +
    theme(panel.grid.major = element_line(color = color.grid.major, size = .55, linetype = "dotted")) +
    theme(panel.grid.minor = element_line(color = color.grid.major, size = .55, linetype = "dotted")) +
    theme(axis.ticks = element_blank()) +
    theme(legend.position = "bottom") +
    theme(legend.background = element_rect(fill = color.background)) +
    theme(legend.text = element_text(size = base_size - 3, color = color.axis.title, family = base_family)) +
    theme(strip.text.x = element_text(size = base_size, color = color.background, family = base_family)) +
    theme(strip.text.y = element_text(size = base_size, color = color.background, family = base_family)) +
    theme(strip.background = element_rect(fill = "grey70", colour = NA)) +
    theme(plot.title = element_text(color = color.title, size = 20, vjust = 1.25, family = base_family, hjust = 0.5)) +
    theme(plot.subtitle = element_text(color = color.subtitle, size = base_size + 2, family = base_family, hjust = 0.5)) +
    theme(axis.text.x = element_text(size = base_size, color = color.axis.text, family = base_family)) +
    theme(axis.text.y = element_text(size = base_size, color = color.axis.text, family = base_family)) +
    theme(text = element_text(size = base_size, color = color.axis.text, family = base_family)) +
    theme(axis.title.x = element_text(size = base_size + 2, color = color.axis.title, vjust = 0, family = base_family)) +
    theme(axis.title.y = element_text(size = base_size + 2, color = color.axis.title, vjust = 1.25, family = base_family)) +
    theme(plot.caption = element_text(size = base_size - 2, color = color.axis.title, vjust = 1.25, family = base_family)) +
    theme(legend.text = element_text(size = base_size, color = color.axis.text, family = base_family)) +
    theme(legend.title = element_text(size = base_size, color = color.axis.text, family = base_family)) +
    theme(legend.key = element_rect(colour = color.background, fill = color.background)) +
    theme(legend.key.width = unit(.75, "cm")) +
    theme(legend.key.height = unit(.75, "cm")) +
    theme(legend.spacing.x = unit(.25, 'cm')) +
    theme(legend.spacing.y = unit(.25, 'cm')) +
    theme(legend.margin = margin(t = 0, r = 0, b = 0, l = 0, unit = "cm")) +
    theme(plot.margin = unit(c(.5, .5, .5, .5), "cm"))
}

# Listar todos los archivos PDF en el directorio de trabajo
files <- list.files(pattern = "pdf$")
print(files)

# Aplicar la funci√≥n pdf_text a cada archivo PDF
textos <- lapply(files, pdf_text)

# Combinar todos los textos en un √∫nico vector de caracteres
textos_combinados <- unlist(textos)

# Crear un corpus de texto a partir del texto extra√≠do
corpus <- Corpus(VectorSource(textos_combinados))

# Limpiar los datos de texto
additional_stopwords <- c("et al", "o o", "p p", "per cent", "https doi", "doi org", "https doi org", "be be be", "t t t", "yes yes yes", "yr yr yr", "p p p ", "age age age", "o o o", "lad claimant count")
clean_corpus <- corpus %>%
  tm_map(content_transformer(tolower)) %>%
  tm_map(removePunctuation) %>%
  tm_map(removeNumbers) %>%
  tm_map(removeWords, c(stopwords("spanish"), additional_stopwords)) %>%
  tm_map(stripWhitespace) %>%
  tm_map(content_transformer(lemmatize_strings))

# Convertir el corpus limpio en tokens
tokens <- tokens(clean_corpus$content)

# Crear una matriz de t√©rminos y documentos (DFM)
dfmat <- dfm(tokens)

# Crear una nube de palabras
set.seed(100)
textplot_wordcloud(dfmat, min_count = 10, color = brewer.pal(8, "Dark2"))



# Crear una matriz de t√©rminos y documentos (DTM)
dtm <- as.matrix(dfm(tokens))

# Visualizaci√≥n con t-SNE
tsne_model <- Rtsne(dtm, dims = 2, perplexity = 5, verbose = TRUE)
tsne_data <- data.frame(x = tsne_model$Y[,1], y = tsne_model$Y[,2], label = rownames(dtm))

ggplot(tsne_data, aes(x = x, y = y, label = label)) +
  geom_text(aes(color = label)) +
  theme_minimal() +
  labs(title = "Visualizaci√≥n de palabras usando t-SNE", x = "Dimensi√≥n 1", y = "Dimensi√≥n 2")

```

```{r}
library(dendextend)

# Listar todos los archivos PDF en el directorio de trabajo
files <- list.files(pattern = "pdf$")
print(files)

# Aplicar la funci√≥n pdf_text a cada archivo PDF y combinar los textos
espa√±ol <- lapply(files, pdf_text)

# Crear un corpus de texto a partir del texto extra√≠do
corpus <- Corpus(VectorSource(lapply(seq_along(espa√±ol), function(i) paste(espa√±ol[[i]], collapse = " "))))


# Limpiar los datos de texto
additional_stopwords <- c("et al", "o o", "p p", "por ciento", "https doi", "doi org", "https doi org", "ser ser ser", "t t t", "s√≠ s√≠ s√≠", "a√±o edad edad", "o o o", "lad claimant count")
clean_corpus <- corpus %>%
  tm_map(content_transformer(tolower)) %>%
  tm_map(removePunctuation) %>%
  tm_map(removeNumbers) %>%
  tm_map(removeWords, c(stopwords("spanish"), additional_stopwords)) %>%
  tm_map(stripWhitespace) %>%
  tm_map(content_transformer(lemmatize_strings))

# Eliminar documentos vac√≠os del corpus
dtm <- DocumentTermMatrix(clean_corpus)
clean_corpus <- clean_corpus[which(rowSums(as.matrix(dtm)) != 0)]

# Crear la matriz TF-IDF sin documentos vac√≠os
text_dtm_tfidf <- DocumentTermMatrix(clean_corpus, control = list(weighting = weightTfIdf))
tf_idf_mat <- as.matrix(text_dtm_tfidf)

# Eliminar t√©rminos con todos los valores en cero
tf_idf_mat <- tf_idf_mat[, colSums(tf_idf_mat) != 0]

# Calcular la distancia del coseno
d1 <- proxy::dist(tf_idf_mat, method = "cosine")

# Verificar si la matriz de distancias contiene valores NA, NaN o Inf
if (any(is.na(d1)) || any(is.infinite(d1))) {
  stop("La matriz de distancias contiene valores NA o Inf.")
}

# Realizar el clustering jer√°rquico
cluster2 <- hclust(d1, method = "ward.D2")

# Convertir el objeto hclust en un dendrograma
dend <- as.dendrogram(cluster2)

# Asignar etiquetas a las hojas del dendrograma
labels(dend) <- files

# Visualizar el dendrograma con nombres de archivos
plot(dend, main = "Dendrograma de Clustering Jer√°rquico de PDFs", ylab = "Altura", xlab = "", cex.lab = 0.75, las = 2)

# Calcular la silhouette
sil <- silhouette(cutree(cluster2, k = 13), d1)

# Visualizar la silueta de los clusters
fviz_silhouette(sil)

# Visualizar los clusters
fviz_cluster(list(data = tf_idf_mat, cluster = cutree(cluster2, k = 13)), geom = "point", show.clust.cent = TRUE, ellipse.type = "convex", main = "Clustering de PDFs")

# Visualizar la matriz de distancias
fviz_dist(as.dist(d1), gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07")) + 
  labs(title = "Matriz de Distancias del Coseno de PDFs")
```

```{r}
library(textmineR)
```
```{r}
# Cargar las librer√≠as necesarias
library(pdftools)
library(tm)
library(wordcloud)
library(RColorBrewer)
library(topicmodels)
library(textstem)
library(dplyr)
library(tidyr)
library(tidytext)
library(igraph)
library(ggraph)
library(cluster)
library(factoextra)
library(Rtsne)
library(viridis)
library(udpipe)
library(data.table)
library(stopwords)
library(BTM)
library(textplot)

# Tema personalizado para ggplot2
theme_elegante <- function(base_size = 10, base_family = "Raleway") {
  color.background <- "#FFFFFF"
  color.grid.major <- "#D9D9D9"
  color.axis.text <- "#666666"
  color.axis.title <- "#666666"
  color.title <- "#666666"
  color.subtitle <- "#666666"
  strip.background.color <- '#9999CC'
  
  ret <- theme_bw(base_size=base_size) +
    theme(panel.background=element_rect(fill=color.background, color=color.background)) +
    theme(plot.background=element_rect(fill=color.background, color=color.background)) +
    theme(panel.border=element_rect(color=color.background)) +
    theme(panel.grid.major=element_line(color=color.grid.major,size=.55, linetype="dotted")) +
    theme(panel.grid.minor=element_line(color=color.grid.major,size=.55, linetype="dotted")) +
    theme(axis.ticks=element_blank()) +
    theme(legend.position="none") +
    theme(legend.background = element_rect(fill=color.background)) +
    theme(legend.text = element_text(size=base_size-3,color=color.axis.title, family = base_family)) +
    theme(strip.text.x = element_text(size=base_size,color=color.background, family = base_family)) +
    theme(strip.text.y = element_text(size=base_size,color=color.background, family = base_family)) +
    theme(strip.background = element_rect(fill = "grey70", colour = NA)) +
    theme(plot.title=element_text(color=color.title, size=20, vjust=1.25, family=base_family, hjust = 0.5)) +
    theme(plot.subtitle=element_text(color=color.subtitle, size=base_size+2, family = base_family, hjust = 0.5)) +
    theme(axis.text.x=element_text(size=base_size,color=color.axis.text, family = base_family)) +
    theme(axis.text.y=element_text(size=base_size,color=color.axis.text, family = base_family)) +
    theme(text=element_text(size=base_size, color=color.axis.text, family = base_family)) +
    theme(axis.title.x=element_text(size=base_size+2,color=color.axis.title, vjust=0, family = base_family)) +
    theme(axis.title.y=element_text(size=base_size+2,color=color.axis.title, vjust=1.25, family = base_family)) +
    theme(plot.caption=element_text(size=base_size-2,color=color.axis.title, vjust=1.25, family = base_family)) +
    theme(legend.text=element_text(size=base_size,color=color.axis.text, family = base_family)) +
    theme(legend.title=element_text(size=base_size,color=color.axis.text, family = base_family)) +
    theme(legend.key=element_rect(colour = color.background, fill = color.background)) +
    theme(legend.position="bottom", legend.box = "horizontal", legend.title = element_blank(), legend.key.width = unit(.75, "cm"), legend.key.height = unit(.75, "cm"), legend.spacing.x = unit(.25, 'cm'), legend.spacing.y = unit(.25, 'cm'), legend.margin = margin(t=0, r=0, b=0, l=0, unit="cm")) +
    theme(plot.margin = unit(c(.5, .5, .5, .5), "cm"))
  
  ret
}

# Listar todos los archivos PDF en el directorio de trabajo
files <- list.files(pattern = "pdf$")
print(files)

# Aplicar la funci√≥n pdf_text a cada archivo PDF y combinar los textos
espa√±ol <- lapply(files, pdf_text)

# Crear un corpus de texto a partir del texto extra√≠do
corpus <- Corpus(VectorSource(lapply(seq_along(espa√±ol), function(i) paste(espa√±ol[[i]], collapse = " "))))



# Limpiar los datos de texto
additional_stopwords <- c("et al", "o o", "p p", "por ciento", "https doi", "doi org", "https doi org", "ser ser ser", "t t t", "s√≠ s√≠ s√≠", "a√±o edad edad", "o o o", "lad claimant count")
clean_corpus <- corpus %>%
  tm_map(content_transformer(tolower)) %>%
  tm_map(removePunctuation) %>%
  tm_map(removeNumbers) %>%
  tm_map(removeWords, c(stopwords("spanish"), additional_stopwords)) %>%
  tm_map(stripWhitespace) %>%
  tm_map(content_transformer(lemmatize_strings))

# Eliminar documentos vac√≠os del corpus
dtm <- DocumentTermMatrix(clean_corpus)
clean_corpus <- clean_corpus[which(rowSums(as.matrix(dtm)) != 0)]

# Crear la matriz TF-IDF sin documentos vac√≠os
text_dtm_tfidf <- DocumentTermMatrix(clean_corpus, control = list(weighting = weightTfIdf))
tf_idf_mat <- as.matrix(text_dtm_tfidf)

# Eliminar t√©rminos con todos los valores en cero
tf_idf_mat <- tf_idf_mat[, colSums(tf_idf_mat) != 0]

# Calcular la distancia del coseno
d1 <- proxy::dist(tf_idf_mat, method = "cosine")

# Verificar si la matriz de distancias contiene valores NA, NaN o Inf
if (any(is.na(d1)) || any(is.infinite(d1))) {
  stop("La matriz de distancias contiene valores NA o Inf.")
}

# Realizar el clustering jer√°rquico
cluster2 <- hclust(d1, method = "ward.D2")

# Convertir el objeto hclust en un dendrograma
dend <- as.dendrogram(cluster2)

# Asignar etiquetas a las hojas del dendrograma
labels(dend) <- files

# Visualizar el dendrograma con nombres de archivos
plot(dend, main = "Dendrograma de Clustering Jer√°rquico de PDFs", ylab = "Altura", xlab = "", cex.lab = 0.75, las = 2)

# Calcular la silhouette
sil <- silhouette(cutree(cluster2, k = 13), d1)

# Visualizar la silueta de los clusters
fviz_silhouette(sil)

# Visualizar los clusters
fviz_cluster(list(data = tf_idf_mat, cluster = cutree(cluster2, k = 13)), geom = "point", show.clust.cent = TRUE, ellipse.type = "convex", main = "Clustering de PDFs")

# Visualizar la matriz de distancias
fviz_dist(as.dist(d1), gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07")) + 
  labs(title = "Matriz de Distancias del Coseno de PDFs")

# Uso de UDPipe para anotaci√≥n
ud_model <- udpipe_download_model(language = "spanish")
ud_model <- udpipe_load_model(ud_model$file_model)
data_tm <- data.frame(doc_id = files, text = sapply(espa√±ol, paste, collapse = " "))

anno <- udpipe(data_tm, object = ud_model, trace = 1000)
biterms <- as.data.table(anno)
biterms <- biterms[, cooccurrence(x = lemma, relevant = upos %in% c("NOUN", "ADJ", "VERB") & nchar(lemma) > 2 & !lemma %in% stopwords("es"), skipgram = 3), by = list(doc_id)]

set.seed(999)
traindata <- subset(anno, upos %in% c("NOUN", "ADJ", "VERB") & !lemma %in% stopwords("es") & nchar(lemma) > 2)
traindata <- traindata[, c("doc_id", "lemma")]

# Ajustar el modelo BTM
model <- BTM(traindata, biterms = biterms, k = 10, iter = 2000, background = FALSE, trace = 2000)

# Extraer biterms para plotear
biterms1 = terms(model, type = "biterms")$biterms

plot(model, subtitle = "Distribuci√≥n de T√≥picos en espa√±ol", biterms = biterms1, labels = paste(round(model$theta * 100, 2), "%", sep = ""), top_n = 20, topic.size = 3, label.size = 2, main.size = 2) +
  theme_void() +
  theme(legend.position = "right", legend.key.size = unit(0.5, "lines"))
```

```{r}
# Create a Document-Term Matrix (DTM)
dtm <- DocumentTermMatrix(clean_corpus)
```


```{r}
# Topic Modeling using LDA
num_topics <- 10
lda_model <- LDA(dtm, k = num_topics, control = list(seed = 1234))
lda_terms <- terms(lda_model, 10)
print(lda_terms)
```

```{r}
library(wordcloud2)
# Crear una matriz de t√©rminos y documentos (TDM)
tdm <- TermDocumentMatrix(clean_corpus)

# Convertir la matriz TDM a una matriz cl√°sica
matrix <- as.matrix(tdm)

# Crear un data frame con las frecuencias de las palabras
words <- sort(rowSums(matrix), decreasing = TRUE)
df <- data.frame(word = names(words), freq = words)

# Crear una nube de palabras
wordcloud2(slice_max(df, order_by = freq, n = 200), size = 0.4, color = "random-dark")

```
```{r}
library(wordcloud2)
# Generar la nube de palabras con mejoras est√©ticas
wordcloud2(slice_max(df, order_by = freq, n = 200), 
           size = 0.7,        # Tama√±o general de las palabras
           color = colors,    # Paleta de colores
           backgroundColor = "white",  # Fondo blanco
           fontWeight = "bold",        # Fuente en negrita
           shape = 'circle',           # Forma de la nube de palabras
           ellipticity = 0.65          # Escala de la nube
           )
```






```{r}
# Suponiendo que clean_corpus ya est√° creado
library(wordcloud2)
# Convertir el corpus a una matriz de t√©rminos de documentos (Term-Document Matrix)
tdm <- TermDocumentMatrix(clean_corpus)
matrix <- as.matrix(tdm)
words <- sort(rowSums(matrix), decreasing = TRUE)
df <- data.frame(word = names(words), freq = words)

# Definir caracteres a eliminar
chars_to_remove <- c("‚Äô", "‚Ä¶", "‚Äì", "‚Äú", "‚Äù", "‚Ä¶", "‚Äù", "‚Ä¶", "ÔÑá", "httpswwwlavanguardiacomvidaelparodelargaduracionafectaalasaludmentalsegunestudiodelacaixahtml", "‚Ä¢", "cookie", "health", "and", "the", "work")

# Filtrar el dataframe para eliminar los caracteres no deseados
df <- df[!df$word %in% chars_to_remove, ]

# Definir la paleta de colores
colors <- brewer.pal(8, "Dark2")  # Puedes elegir una paleta diferente de RColorBrewer

# Generar la nube de palabras con mejoras est√©ticas
wordcloud2(slice_max(df, order_by = freq, n = 200), 
           size = 0.7,        # Tama√±o general de las palabras
           color = colors,    # Paleta de colores
           backgroundColor = "white",  # Fondo blanco
           fontWeight = "bold",        # Fuente en negrita
           shape = 'circle',           # Forma de la nube de palabras
           ellipticity = 0.65          # Escala de la nube
)

```
```{r}
library(wordcloud2)
library(tm)
library(RColorBrewer)

# Suponiendo que clean_corpus ya est√° creado y df tambi√©n

# Definir caracteres a eliminar (si no se ha hecho ya)
chars_to_remove <- c("‚Äô", "‚Ä¶", "‚Äì", "‚Äú", "‚Äù", "‚Ä¶", "‚Äù", "‚Ä¶", "ÔÑá", "httpswwwlavanguardiacomvidaelparodelargaduracionafectaalasaludmentalsegunestudiodelacaixahtml", "‚Ä¢", "cookie", "health", "and", "the", "work")

# Filtrar el dataframe para eliminar los caracteres no deseados
df <- df[!df$word %in% chars_to_remove, ]

# Definir la paleta de colores
colors <- brewer.pal(8, "Dark2")  # Puedes elegir una paleta diferente de RColorBrewer

# Generar la nube de palabras con mejoras est√©ticas y creativas
wordcloud2(slice_max(df, order_by = freq, n = 200), 
           size = 1.2,                    # Tama√±o general de las palabras
           color = "random-dark",        # Colores aleatorios oscuros
           backgroundColor = "white",    # Fondo blanco
           shape = 'star',               # Forma de las palabras (estrella)
           rotateRatio = 0.4,            # Rotaci√≥n aleatoria de las palabras
           fontFamily = "Impact",        # Fuente de las palabras
           fontWeight = "bold",          # Fuente en negrita
           minSize = 20,                 # Tama√±o m√≠nimo de la palabra
           gridSize = 8,                 # Tama√±o de la cuadr√≠cula para ajustar la colocaci√≥n de las palabras
           ellipticity = 0.7             # Proporci√≥n de la elipse (para formas m√°s ovaladas)
)

```
```{r}
library(wordcloud2)
library(tm)
library(RColorBrewer)

# Suponiendo que clean_corpus ya est√° creado y df tambi√©n

# Definir caracteres a eliminar (si no se ha hecho ya)
chars_to_remove <- c("‚Äô", "‚Ä¶", "‚Äì", "‚Äú", "‚Äù", "‚Ä¶", "‚Äù", "‚Ä¶", "ÔÑá", "httpswwwlavanguardiacomvidaelparodelargaduracionafectaalasaludmentalsegunestudiodelacaixahtml", "‚Ä¢", "cookie", "health", "and", "the", "work")

# Filtrar el dataframe para eliminar los caracteres no deseados
df <- df[!df$word %in% chars_to_remove, ]

# Definir la paleta de colores
colors <- brewer.pal(8, "Set3")  # Paleta de colores vibrantes

# Generar la nube de palabras con mejoras est√©ticas y creativas
wordcloud2(slice_max(df, order_by = freq, n = 200), 
           size = 1.5,                    # Tama√±o general de las palabras
           color = "random-light",       # Colores aleatorios claros
           backgroundColor = "black",    # Fondo negro
           shape = 'triangle',           # Forma de las palabras (tri√°ngulo)
           rotateRatio = 0.5,            # Rotaci√≥n aleatoria de las palabras
           fontFamily = "Impact",        # Fuente de las palabras
           fontWeight = "bold",          # Fuente en negrita
           minSize = 30,                 # Tama√±o m√≠nimo de la palabra
           gridSize = 12,                # Tama√±o de la cuadr√≠cula para ajustar la colocaci√≥n de las palabras
           ellipticity = 0.8        # Proporci√≥n de la elipse (para formas m√°s ov         # Color al pasar el mouse sobre las palabras
            # Acci√≥n al hacer clic (abrir URL)
             # Par√°metro para la URL
)

```


```{r}
library(wordcloud2)
library(tm)
library(RColorBrewer)

# Suponiendo que clean_corpus ya est√° creado y df tambi√©n

# Definir caracteres a eliminar (si no se ha hecho ya)
chars_to_remove <- c("‚Äô", "‚Ä¶", "‚Äì", "‚Äú", "‚Äù", "‚Ä¶", "‚Äù", "‚Ä¶", "ÔÑá", "httpswwwlavanguardiacomvidaelparodelargaduracionafectaalasaludmentalsegunestudiodelacaixahtml", "‚Ä¢", "cookie", "health", "and", "the", "work")

# Filtrar el dataframe para eliminar los caracteres no deseados
df <- df[!df$word %in% chars_to_remove, ]

# Definir la paleta de colores
colors <- brewer.pal(8, "Spectral")  # Paleta de colores vibrantes

# Generar la nube de palabras con estilo ingenioso y llamativo
wordcloud2(slice_max(df, order_by = freq, n = 150), 
           size = 1.8,                    # Tama√±o general de las palabras
           color = "random-dark",        # Colores aleatorios oscuros
           backgroundColor = "white",    # Fondo blanco
           shape = 'ellipse',            # Forma de las palabras (elipse)
           rotateRatio = 0.4,            # Rotaci√≥n aleatoria de las palabras
           fontFamily = "Impact",        # Fuente de las palabras
           fontWeight = "bold",          # Fuente en negrita
           minSize = 25,                 # Tama√±o m√≠nimo de la palabra
           gridSize = 10,                # Tama√±o de la cuadr√≠cula para ajustar la colocaci√≥n de las palabras
           ellipticity = 0.85,           # Proporci√≥n de la elipse (para formas m√°s ovaladas)
           shuffle = TRUE               # Mezclar palabras para un efecto m√°s din√°mico
)

```



```{r}
library(wordcloud2)

# Definir la ruta de la imagen de la m√°scara
figPath <- "C:/Users/jorge/Desktop/M√ÅSTER/TFM/ESPA√ëOL/1695237285006.png"

# Suponiendo que clean_corpus ya est√° creado y df tambi√©n

# Definir caracteres a eliminar (si no se ha hecho ya)
chars_to_remove <- c("‚Äô", "‚Ä¶", "‚Äì", "‚Äú", "‚Äù", "‚Ä¶", "‚Äù", "‚Ä¶", "ÔÑá", "httpswwwlavanguardiacomvidaelparodelargaduracionafectaalasaludmentalsegunestudiodelacaixahtml", "‚Ä¢", "cookie", "health", "and", "the", "work")

# Filtrar el dataframe para eliminar los caracteres no deseados
df <- df[!df$word %in% chars_to_remove, ]

# Generar la nube de palabras con la m√°scara de la imagen
wordcloud2(slice_max(df, order_by = freq, n = 250), 
           figPath = figPath, 
           size = 1.5, 
           color = "skyblue"
)

```


```{r}
# Generar la nube de palabras con una forma predefinida (tri√°ngulo)
wordcloud2(slice_max(df, order_by = freq, n = 300), 
           size = 1.5, 
           color = "skyblue",
           shape = "triangle"
)
```


```{r}
# Definir una paleta de colores pastel personalizada
pastel_colors <- c("#FFB3BA", "#FFDFBA", "#FFFFBA", "#BAFFC9", "#BAE1FF", "#D9BAFF", "#FFBAED")

# Generar la nube de palabras con colores pastel personalizados
wordcloud2(slice_max(df, order_by = freq, n = 3000), 
           size = 1.5, 
           color = rep_len(pastel_colors, nrow(df)),
           backgroundColor = "white",
           shape = "circle",
           fontFamily = "Aptos",
           fontWeight = "bold",
           rotateRatio = 0.5, # Ajusta la rotaci√≥n de las palabras
           ellipticity = 0.65 # Ajusta la forma de la nube
)
```
```{r}
library(wordcloud2)
# Definir una paleta de colores pastel personalizada ampliada
pastel_colors <- c("#FFB3BA", "#FFDFBA", "#FFFFBA", "#BAFFC9", "#BAE1FF", "#D9BAFF", "#FFBAED", 
                   "#FAD6A5", "#C9C9FF", "#FFCCF9", "#F1CBFF", "#FFE4E1", "#E6E6FA", "#FFF0F5")

# N√∫mero de palabras m√°s frecuentes que queremos destacar con colores pastel
num_colored_words <- 100

# Generar la nube de palabras con colores personalizados
wordcloud2(
  df, 
  size = 1.5, 
  color = ifelse(rank(-df$freq) <= num_colored_words, rep_len(pastel_colors, num_colored_words), "black"),
  backgroundColor = "white",
  shape = "circle",
  fontFamily = "Aptos",
  fontWeight = "bold",
  rotateRatio = 0.5, # Ajusta la rotaci√≥n de las palabras
  ellipticity = 0.65 # Ajusta la forma de la nube
)
```
```{r}
# Definir la paleta de colores Dark2 de RColorBrewer
dark2_colors <- brewer.pal(8, "Dark2")

# N√∫mero de palabras m√°s frecuentes que queremos destacar con colores Dark2
num_colored_words <- 100
# Suponiendo que clean_corpus ya est√° creado y df tambi√©n
# Definir caracteres a eliminar (si no se ha hecho ya)
chars_to_remove <- c("‚Äô", "‚Ä¶", "‚Äì", "‚Äú", "‚Äù", "‚Ä¶", "‚Äù", "‚Ä¶", "ÔÑá", "httpswwwlavanguardiacomvidaelparodelargaduracionafectaalasaludmentalsegunestudiodelacaixahtml", "‚Ä¢", "cookie", "health", "and", "the", "work")

# Filtrar el dataframe para eliminar los caracteres no deseados
df <- df[!df$word %in% chars_to_remove, ]

# Generar la nube de palabras con colores personalizados
wordcloud2(
  df, 
  size = 1.5, 
  color = ifelse(rank(-df$freq) <= num_colored_words, 
                 rep_len(dark2_colors, nrow(df)), 
                 "black"),
  backgroundColor = "white",
  shape = "circle",
  fontFamily = "Aptos",
  fontWeight = "bold",
  rotateRatio = 0.5, # Ajusta la rotaci√≥n de las palabras
  ellipticity = 0.65 # Ajusta la forma de la nube
)
```


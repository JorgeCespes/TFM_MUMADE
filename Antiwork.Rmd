---
title: "Antiwork"
author: "Jorge C√©spedes Rico"
date: "2024-05-20"
output: html_document
---

#CARGA DE PAQUETES
```{r}
library(dplyr)
library(syuzhet)
library(tidyverse)
library(tidytext)
library(NLP)
library(tm)
library(wordcloud)
library(stopwords)
library(RColorBrewer)
library(ggplot2)
```


```{r}
options(repos=list(CRAN="http://cran.rstudio.com/"))
pk_nec = c("RedditExtractoR")
pk_inst <- pk_nec %in% installed.packages() 
if (length(pk_nec[!pk_inst])>0) install.packages(pk_nec[!pk_inst])
lapply(pk_nec, require, character.only=TRUE)
rm(pk_inst)
rm(pk_nec)
```

```{r}
## Descargar los hilos de la comunidad
threads1 <- find_thread_urls(keywords = "mental health, temporary job, low salary, precarious, stress, anxiety, quit job, burnout, great resignation", subreddit = "Antiwork",sort_by = "new", period = "all")
comments1 <- get_thread_content(threads1$url)
```
Se han obtenido en comments1$comment --> 88.938 comentarios 
```{r}
#Creacci√≥n de data frame con los comentarios 
Antiwork <- data.frame(comments1$comments)
View(Antiwork)
```

Tiene 10 columnas, √∫nicamente nos quedaremos con la columna de los comentarios y la de fecha 

```{r}
Antiwork <- select(comments1$comments, -1, -2, -4, -5, -6, -7, -8, -10)
View(Antiwork)
max(Antiwork$date)
min(Antiwork$date)
```
La fecha del √∫ltimo comentario descargado es a fecha de descarga, es decir, 16/06/2023 y la fecha m√°s antigua es el 13/04/2023. 

#LIMPIEZA DE DATOS 
##1. Eliminaci√≥n de signos y car√°cteres
```{r}
Antiwork$comment <- gsub("@","", Antiwork$comment)
Antiwork$comment <- gsub("http[^[:space:]]*", "", Antiwork$comment)
Antiwork$comment <- gsub("#", "", Antiwork$comment)
Antiwork$comment <- gsub("\\?", "", Antiwork$comment)
Antiwork$comment <- gsub("\\¬ø", "", Antiwork$comment)
Antiwork$comment <- gsub("\\!", "", Antiwork$comment)
Antiwork$comment <- gsub("\\¬°", "", Antiwork$comment)
Antiwork$comment <- gsub("\\,", "", Antiwork$comment)
Antiwork$comment <- gsub("\\|", "", Antiwork$comment)
Antiwork$comment <- gsub("‚Äú", "", Antiwork$comment)
Antiwork$comment <- gsub("‚Äù", "", Antiwork$comment)
Antiwork$comment <- gsub("‚Äò", "", Antiwork$comment)
Antiwork$comment <- gsub("‚Äô", "", Antiwork$comment)
Antiwork$comment <- gsub(":", "", Antiwork$comment)
Antiwork$comment <- gsub("\\.", "", Antiwork$comment)
Antiwork$comment <- gsub('"', '', Antiwork$comment)
Antiwork$comment <- gsub("\\(", "", Antiwork$comment)
Antiwork$comment <- gsub("\\)", "", Antiwork$comment)
Antiwork$comment <- gsub("\\-", "", Antiwork$comment)
Antiwork$comment <- gsub("\\_", "",Antiwork$comment)
Antiwork$comment <- gsub("\\=", "", Antiwork$comment)
Antiwork$comment <- gsub("\\/", "", Antiwork$comment)
Antiwork$comment <- gsub("\\¬´", "", Antiwork$comment)
Antiwork$comment <- gsub("\\¬ª", "", Antiwork$comment)
Antiwork$comment <- gsub("\\*", "", Antiwork$comment)
```

##2. Eliminaci√≥n de spam
```{r}
Antiwork <- Antiwork %>% distinct(comment, .keep_all = TRUE)
```

##3. Craci√≥n variable fecha 

```{r}
Antiwork$date <- as.Date(Antiwork$date)
```

```{r}
antes <- Antiwork %>% filter(date< "2023-06-05")
despues <- Antiwork %>% filter(date > "2023-06-05")
```


#AN√ÅLISIS DE SENTIMIENTO

```{r}
#sentimiento sobre la empresa en general 
Sentimiento_Antiwork <- get_nrc_sentiment(Antiwork$comment)
```




```{r}



theme_elegante <- function(base_size = 10,
                           base_family = "Raleway"
                           )
    {
    color.background = "#FFFFFF" # Chart Background
    color.grid.major = "#D9D9D9" # Chart Gridlines
    color.axis.text = "#666666" # 
    color.axis.title = "#666666" # 
    color.title = "#666666"
    color.subtitle = "#666666"
    strip.background.color = '#9999CC'
    
    ret <-
        theme_bw(base_size=base_size) +
        
        # Set the entire chart region to a light gray color
        theme(panel.background=element_rect(fill=color.background, color=color.background)) +
        theme(plot.background=element_rect(fill=color.background, color=color.background)) +
        theme(panel.border=element_rect(color=color.background)) +
        
        # Format the grid
        theme(panel.grid.major=element_line(color=color.grid.major,size=.55, linetype="dotted")) +
        theme(panel.grid.minor=element_line(color=color.grid.major,size=.55, linetype="dotted")) +
        theme(axis.ticks=element_blank()) +
        
        # Format the legend, but hide by default
        theme(legend.position="none") +
        theme(legend.background = element_rect(fill=color.background)) +
        theme(legend.text = element_text(size=base_size-3,color=color.axis.title, family = base_family)) +
        
        theme(strip.text.x = element_text(size=base_size,color=color.background, family = base_family)) +
        theme(strip.text.y = element_text(size=base_size,color=color.background, family = base_family)) +
        #theme(strip.background = element_rect(fill=strip.background.color, linetype="blank")) +
        theme(strip.background = element_rect(fill = "grey70", colour = NA)) +
        # theme(panel.border= element_rect(fill = NA, colour = "grey70", size = rel(1)))+
        # Set title and axis labels, and format these and tick marks
        theme(plot.title=element_text(color=color.title, 
                                      size=20, 
                                      vjust=1.25, 
                                      family=base_family, 
                                      hjust = 0.5
                                      )) +
        
        theme(plot.subtitle=element_text(color=color.subtitle, size=base_size+2, family = base_family,  hjust = 0.5))  +
        
        theme(axis.text.x=element_text(size=base_size,color=color.axis.text, family = base_family)) +
        theme(axis.text.y=element_text(size=base_size,color=color.axis.text, family = base_family)) +
        theme(text=element_text(size=base_size, color=color.axis.text, family = base_family)) +
        
        theme(axis.title.x=element_text(size=base_size+2,color=color.axis.title, vjust=0, family = base_family)) +
        theme(axis.title.y=element_text(size=base_size+2,color=color.axis.title, vjust=1.25, family = base_family)) +
        theme(plot.caption=element_text(size=base_size-2,color=color.axis.title, vjust=1.25, family = base_family)) +
        
        # Legend  
        theme(legend.text=element_text(size=base_size,color=color.axis.text, family = base_family)) +
        theme(legend.title=element_text(size=base_size,color=color.axis.text, family = base_family)) +
        theme(legend.key=element_rect(colour = color.background, fill = color.background)) +
        theme(legend.position="bottom", 
              legend.box = "horizontal", 
              legend.title = element_blank(),
              legend.key.width = unit(.75, "cm"),
              legend.key.height = unit(.75, "cm"),
              legend.spacing.x = unit(.25, 'cm'),
              legend.spacing.y = unit(.25, 'cm'),
              legend.margin = margin(t=0, r=0, b=0, l=0, unit="cm")) +

        # Plot margins
        theme(plot.margin = unit(c(.5, .5, .5, .5), "cm"))
    
    ret
}
```

```{r}
# Create a DTM for bigrams
dtm_bigrams <- DocumentTermMatrix(clean_corpus, control = list(tokenize = BigramTokenizer))

```
```{r}
# Calculate Term Frequency-Inverse Document Frequency (TF-IDF)
tfidf <- weightTfIdf(dtm)
tfidf_matrix <- as.matrix(tfidf)
```

```{r}
# Supongamos que `tfidf_matrix` es tu matriz TF-IDF

# Calcular la suma de TF-IDF para cada t√©rmino
top_terms <- sort(colSums(tfidf_matrix), decreasing = TRUE)

# Convertir a un data frame
top_terms_df <- data.frame(term = names(top_terms), tfidf = top_terms)

# Filtrar los t√©rminos espec√≠ficos y aquellos que contienen "..." o "'" o "‚Äô"
top_terms_df <- top_terms_df %>%
  filter(!term %in% c("httpswwwquoracomunansweredhowdoiprotectemployeesandbusinessesincovid", 
                      "httpswwwquoracomwhataresomementalhealthtipsduringcovid")) %>%
  filter(!grepl("\\.\\.\\.", term)) %>%  # Filtrar t√©rminos que contienen "..."
  filter(!grepl("'", term)) %>%  # Filtrar t√©rminos que contienen "'"
  filter(!grepl("‚Äô", term)) %>%  # Filtrar t√©rminos que contienen "‚Äô"
  arrange(desc(tfidf)) %>%
  head(18)

# Imprimir el data frame resultante
print(top_terms_df)

```

```{r}
# Find the top terms by TF-IDF
top_terms <- sort(colSums(tfidf_matrix), decreasing = TRUE)
print(head(top_terms, 15))

# Convertir los top_terms en un data frame
top_terms_df <- data.frame(term = names(top_terms), tfidf = top_terms)
# Filtrar los t√©rminos espec√≠ficos y aquellos que contienen "..." o "'" o "‚Äô"
top_terms_df <- top_terms_df %>%
  filter(!term %in% c("‚Åé", "‚àí", "‚Ä¶", "yes", "std", "unstd","‚Äô", "swb", "pwb", "almps", "‚Äì", "*","httpswwwquoracomwhatsthebestwaytoincreaseproductivity", "httpswwwquoracomunansweredhowdoiprotectemployeesandbusinessesincovid","httpswwwquoracomwhataresomementalhealthtipsduringcovid", "ùñ•", "***", "‚àó‚àó‚àó")) %>%
  filter(!grepl("\\.\\.\\.", term)) %>%  # Filtrar t√©rminos que contienen "..."
  filter(!grepl("'", term)) %>%  # Filtrar t√©rminos que contienen "'"
  filter(!grepl("‚Äô", term)) %>%  # Filtrar t√©rminos que contienen "‚Äô"
  arrange(desc(tfidf)) %>%
  head(15)

# Ordenar los t√©rminos por TF-IDF
top_terms_df <- top_terms_df %>% arrange(desc(tfidf))

# Seleccionar los 10 primeros t√©rminos
top_terms_df <- head(top_terms_df, 15)

# Cargar la biblioteca ggplot2
library(ggplot2)

# Graficar los principales t√©rminos por TF-IDF con una paleta de colores diferente
ggplot(top_terms_df, aes(x = reorder(term, tfidf), y = tfidf, fill = term)) +
  geom_bar(stat = "identity", color = "black")  +  # Usar la paleta de colores viridis
  labs(title = "Top 15 t√©rminos  por TF-IDF", x = "T√©rmino", y = "TF-IDF") +
  theme_elegante() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1), 
        legend.position = "none",
        axis.title.x = element_text(face = "bold"),  # T√≠tulo del eje x en negrita
        axis.title.y = element_text(face = "bold"))  # T√≠tulo del eje y en negrita
top_terms_df
```


```{r}
# Crear un corpus de texto
corpus <- Corpus(VectorSource(Antiwork$comment))

# Definir stopwords adicionales
additional_stopwords <- c("upvote", "upvotes", "view", "views", "y", "dfollow", "answer", "answers")

# Limpiar el corpus de texto
clean_corpus <- corpus %>%
  tm_map(content_transformer(tolower)) %>%
  tm_map(removePunctuation) %>%
  tm_map(removeNumbers) %>%
  tm_map(removeWords, c(stopwords("english"), additional_stopwords)) %>%
  tm_map(stripWhitespace) 

# Crear una Document-Term Matrix (DTM) para bigramas
BigramTokenizer <- function(x) {
  unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
}
dtm_bigrams <- DocumentTermMatrix(clean_corpus, control = list(tokenize = BigramTokenizer))

# Calcular el TF-IDF
tfidf <- weightTfIdf(dtm_bigrams)
tfidf_matrix <- as.matrix(tfidf)

# Calcular la suma de TF-IDF para cada t√©rmino
top_terms <- sort(colSums(tfidf_matrix), decreasing = TRUE)

# Convertir a un data frame
top_terms_df <- data.frame(term = names(top_terms), tfidf = top_terms)

# Filtrar los t√©rminos espec√≠ficos y aquellos que contienen "..." o "'" o "‚Äô"
top_terms_df <- top_terms_df %>%
  filter(!term %in% c("httpswwwquoracomunansweredhowdoiprotectemployeesandbusinessesincovid", 
                      "httpswwwquoracomwhataresomementalhealthtipsduringcovid")) %>%
  filter(!grepl("\\.\\.\\.", term)) %>%  # Filtrar t√©rminos que contienen "..."
  filter(!grepl("'", term)) %>%  # Filtrar t√©rminos que contienen "'"
  filter(!grepl("‚Äô", term)) %>%  # Filtrar t√©rminos que contienen "‚Äô"
  arrange(desc(tfidf)) %>%
  head(15)

# Imprimir el data frame resultante
print(top_terms_df)

# Graficar los principales t√©rminos por TF-IDF con una paleta de colores diferente
ggplot(top_terms_df, aes(x = reorder(term, tfidf), y = tfidf, fill = term)) +
  geom_bar(stat = "identity", color = "black")  +  # Usar la paleta de colores viridis
  labs(title = "Top 15 t√©rminos  por TF-IDF", x = "T√©rmino", y = "TF-IDF") +
  theme_elegante() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1), 
        legend.position = "none",
        axis.title.x = element_text(face = "bold"),  # T√≠tulo del eje x en negrita
        axis.title.y = element_text(face = "bold"))  # T√≠tulo del eje y en negrita
```

```{r}

library(ggwordcloud)
library(viridis)
# Combine all cleaned text into a single data frame
cleaned_text <- data.frame(text = sapply(clean_corpus, as.character), stringsAsFactors = FALSE)

# Remove unwanted characters (".", "...", "'", "‚Äô", "s", "¬∑", "‚Äú‚Äù, etc.)
cleaned_text$text <- gsub("[\\.]{2,}", "", cleaned_text$text)  # Remove multiple dots
cleaned_text$text <- gsub("'", "", cleaned_text$text)
cleaned_text$text <- gsub("‚Äô", "", cleaned_text$text)
cleaned_text$text <- gsub("¬∑", "", cleaned_text$text)
cleaned_text$text <- gsub("and|the|of|j‚Äô‚Äô|‚Äú|‚Äù", "", cleaned_text$text)  # Remove specific words and characters
cleaned_text$text <- gsub("[[:punct:]]", "", cleaned_text$text)  # Remove punctuation
cleaned_text$text <- gsub("\\s+", " ", cleaned_text$text)  # Remove extra whitespace

# Tokenize the text
words <- unlist(strsplit(cleaned_text$text, " "))

# Remove empty strings and unwanted short words from the tokenized words
unwanted_words <- c("p", "o", "ob", "s", "j", "t", "b", "de", "c", "e", "	ùêä", "quora", "don", "k", "re", "etc", "youre", "m", "s", "also")
words <- words[words != "" & !words %in% unwanted_words]

# Create word frequency table
word_freq <- table(words)
word_freq_df <- as.data.frame(word_freq)
colnames(word_freq_df) <- c("word", "freq")

# Filter words with a minimum frequency
word_freq_df <- word_freq_df[word_freq_df$freq >= 150,]

# Plot word cloud using ggwordcloud
ggplot(word_freq_df, aes(label = word, size = freq, color = freq)) +
  geom_text_wordcloud_area(rm_outside = TRUE) +  # Ensure words don't overlap and stay within bounds
  scale_size_area(max_size = 30) +
  scale_color_viridis_c() +  # Set color scheme for the text
  theme_elegante() +
  labs(title = "Precariousness and mental health", 
       subtitle = "M√≠nima frecuencia: 150") +
  theme(
    plot.title = element_text(size = 20, hjust = 0.5, vjust = 1, face = "bold"),  # Centered title at the top
    plot.subtitle = element_text(size = 12, hjust = 0.5, color = "darkgrey"),  # Centered subtitle below the title
    plot.margin = margin(10, 10, 10, 10)
  )

```
```{r}
# Combine all cleaned text into a single data frame
cleaned_text <- data.frame(text = sapply(clean_corpus, as.character), stringsAsFactors = FALSE)

# Tokenize the text into bigrams
bigrams <- cleaned_text %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  filter(!is.na(bigram))  # Filtrar valores NA

# Count the frequency of each bigram
bigram_counts <- bigrams %>%
  count(bigram, sort = TRUE)

# Filter to remove unwanted bigrams
unwanted_bigrams <- c("don t", "originally answer", "follow former", "can t", "health quora", "s mental health", "don t know", "try quora add", "quora add question", "arent allow write", "allow write question", "NA", "didn t", "isn t", "don t need", "don t get", "work don t", "don t work","can t refuse")
filtered_bigrams <- bigram_counts %>%
  filter(!bigram %in% unwanted_bigrams)

# Filter to keep only the top 20 bigrams
top_bigrams <- filtered_bigrams %>%
  top_n(20, wt = n)

# Visualize the top 20 bigrams without legend
ggplot(top_bigrams, aes(x = reorder(bigram, n), y = n, fill = reorder(bigram, n))) +
  geom_col() +
  coord_flip() +
  labs(title = "Top 20 bigramas", x = "Bigrama", y = "Frecuencia") +
  theme_elegante() +                      
  theme(axis.text.x = element_text(angle = 45, hjust = 1), 
        axis.title.x = element_text(face = "bold"),  
        axis.title.y = element_text(face = "bold"),  
        legend.position = "none")

# Tokenize the text into trigrams
trigrams <- cleaned_text %>%
  unnest_tokens(trigram, text, token = "ngrams", n = 3) %>%
  filter(!is.na(trigram))  # Filtrar valores NA

# Count the frequency of each trigram
trigram_counts <- trigrams %>%
  count(trigram, sort = TRUE)

# Filter to remove unwanted trigrams
unwanted_trigrams <- c("k share", "s mental health", "don t know", "write arent allow", "try quora add", "quora add question", "arent allow write", "allow write question", "don t", "originally answer", "follow former", "can t", "health quora", "s mental health", "don t know", "try quora add", "quora add question", "arent allow write", "allow write question", "NA", "didn t", "isn t", "don t need", "don t get", "work don t", "don t work","can t refuse")
filtered_trigrams <- trigram_counts %>%
  filter(!trigram %in% unwanted_trigrams)

# Filter to keep only the top 20 trigrams
top_trigrams <- filtered_trigrams %>%
  top_n(20, wt = n)

# Visualize the top 20 trigrams without legend
ggplot(top_trigrams, aes(x = reorder(trigram, n), y = n, fill = reorder(trigram, n))) +
  geom_col() +
  coord_flip() +
  labs(title = "Top 20 trigramas", x = "Trigrama", y = "Frecuencia") +
  theme_elegante() +                      
  theme(axis.text.x = element_text(angle = 45, hjust = 1), 
        axis.title.x = element_text(face = "bold"),  
        axis.title.y = element_text(face = "bold"),  
        legend.position = "none")
```

```{r}
# Define a list of unwanted words
unwanted_words <- c("di", "yr", "doi", "org", "o", "p", "https", "res", 
                    "claimant count", "std", "unstd", "be", "garciag√≥mez", 
                    "int", "macmillan", "ppi", "yes", "claimant", "count", "iec", "fjmd", "t", "pa", "creed", "univ", "connecticut", "garc√≠ag√≥mez", "sjpsagepubcom", "tue", "sep", "ii", "download", "iza", "e", "ects", "jstor", "ltd", "ùêä", "q", "de", "witte", "copyright", "john", "wiley", "httpsdoiorgjournalpone", "sarti", "zella", "s", "j", "epidemiol", "jung", "dw", "kwak", "kk", "vol", "pp", "plos", "bmc", "quora", "api", "scraper", "im", "album", "yupoo", "k", "cicd", "ci", "tool", "idenative", "pipeline", "ad", "free", "mo", "k", "arent", "allow", "request", "sponsor")

# Combine all cleaned text into a single data frame
cleaned_text <- data.frame(text = sapply(clean_corpus, as.character), stringsAsFactors = FALSE)

# Tokenize the text into bigrams
bigrams <- cleaned_text %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

# Separate the bigrams into two columns
bigrams_separated <- bigrams %>%
  separate(bigram, into = c("word1", "word2"), sep = " ")

# Filter out unwanted words and NA values
bigrams_filtered <- bigrams_separated %>%
  filter(!is.na(word1) & !is.na(word2) & !word1 %in% unwanted_words & !word2 %in% unwanted_words)

# Count the frequency of each bigram
bigram_counts <- bigrams_filtered %>%
  count(word1, word2, sort = TRUE)

# Filter to keep only the top 60 bigrams for visualization
top_bigrams <- bigram_counts %>%
  top_n(60, wt = n)

# Create a graph object using igraph
bigram_graph <- graph_from_data_frame(top_bigrams)

# Plot the bigram network using ggraph
set.seed(1234)
ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), edge_width = 0.6, edge_colour = "red3",  # Change color to "darkgray"
                 arrow = arrow(type = "closed", length = unit(0.185, "inches"))) +
  geom_node_point(size = 5, color = "darkolivegreen3") +
  geom_node_text(aes(label = name), repel = TRUE, size = 3.25) +
  theme_void() +
  labs(title = "Red de bigramas: Precariousness and mental health", 
       subtitle = "Top 60")

# Tokenize the text into trigrams
trigrams <- cleaned_text %>%
  unnest_tokens(trigram, text, token = "ngrams", n = 3)

# Separate the trigrams into three columns
trigrams_separated <- trigrams %>%
  separate(trigram, into = c("word1", "word2", "word3"), sep = " ")

# Filter out unwanted words and NA values
trigrams_filtered <- trigrams_separated %>%
  filter(!is.na(word1) & !is.na(word2) & !is.na(word3) & !word1 %in% unwanted_words & !word2 %in% unwanted_words & !word3 %in% unwanted_words)

# Count the frequency of each trigram
trigram_counts <- trigrams_filtered %>%
  count(word1, word2, word3, sort = TRUE)

# Filter to keep only the top 60 trigrams for visualization
top_trigrams <- trigram_counts %>%
  top_n(60, wt = n)

# Create a graph object using igraph
trigram_graph <- graph_from_data_frame(top_trigrams, directed = TRUE)

# Plot the trigram network using ggraph with kk layout
set.seed(1234)
ggraph(trigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), edge_width = 0.6, 
                 arrow = arrow(type = "closed", length = unit(0.185, "inches")), 
                 edge_colour = "sienna") +  # Change color to "darkgray"
  geom_node_point(size = 5, color = "steelblue3") +
  geom_node_text(aes(label = name), repel = TRUE, size = 3.25) +
  theme_void() +
  labs(title = "Red de trigramas: Precariousness and mental health", 
       subtitle = "Top 60")


```

```{r}
# Create a Document-Term Matrix (DTM)
dtm <- DocumentTermMatrix(clean_corpus)
```


```{r}
# Funci√≥n para crear matriz de concurrencia
create_cooccurrence_matrix <- function(dtm) {
  dtm_matrix <- as.matrix(dtm)
  term_matrix <- t(dtm_matrix) %*% dtm_matrix
  term_matrix[lower.tri(term_matrix, diag = TRUE)] <- 0
  term_matrix
}

# Crear la matriz de concurrencia
cooccurrence_matrix <- create_cooccurrence_matrix(dtm)

# Convertir la matriz de concurrencia en un dataframe
cooccurrence_df <- as.data.frame(as.table(cooccurrence_matrix))
colnames(cooccurrence_df) <- c("term1", "term2", "frequency")

# Filtrar t√©rminos con concurrencia baja y eliminar caracteres no deseados
cooccurrence_df <- cooccurrence_df %>%
  filter(frequency > 300) %>%
  filter(!term1 %in% c("‚Äì", "‚Äô", "‚Äò", "‚àí", "‚Ä¶", "‚Åé", "‚Äú", "‚Äù", "ùñ•", "‚àó‚àó‚àó", "‚àó‚àó", "‚â•") & 
         !term2 %in% c("‚Äì", "‚Äô", "‚Äò", "‚àí","‚Ä¶", "‚Åé", "‚Äú", "‚Äù", "ùñ•", "‚àó‚àó‚àó", "‚àó‚àó", "‚â•"))

# Crear un grafo a partir de la matriz de concurrencia
cooccurrence_graph <- graph_from_data_frame(cooccurrence_df, directed = FALSE)

# Aumentar la transparencia de las l√≠neas y ajustar el tama√±o del grafo
set.seed(175)  # Asegura la reproducibilidad
ggraph(cooccurrence_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = frequency, edge_width = frequency), color = "pink3", show.legend = TRUE) +
  geom_node_point(color = "springgreen3", size = 4) +
  geom_node_text(aes(label = name), repel = TRUE, size = 3.25) +  # Usar repel para mejorar la distribuci√≥n de etiquetas
  scale_edge_width_continuous(name = "n") +
  scale_edge_alpha_continuous(name = "n") +
  theme_void() +
  labs(title = "Grafo de concurrencia de t√©rminos", 
       subtitle = "Frecuencia > 300") +
  theme(plot.title = element_text(size = 12, face = "bold"),
        plot.subtitle = element_text(size = 10),
        plot.caption = element_text(size = 8, hjust = 1))

```



#NUBE DE PALABRAS 
```{r}
#fijaci√≥n de semilla 
set.seed(13)
```

##Nube de palabras completa
```{r}
Antiwork$comment %>% VectorSource() %>% VCorpus() %>% tm_map(removeWords,
stopwords("english")) %>%
wordcloud(scale=c(3, 0.4), min.freq = 100, max.words=100, color = brewer.pal(8, "Dark2"))
```



```{r}
# Sumarizar las emociones
emotion_counts <- colSums(Sentimiento_Antiwork[, 1:10])

# Crear un dataframe para facilitar la visualizaci√≥n
emotion_df <- data.frame(emotion = names(emotion_counts), count = emotion_counts)

# Crear un gr√°fico de barras para las emociones con colores
ggplot(emotion_df, aes(x = reorder(emotion, -count), y = count, fill = emotion)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  labs(title = "Sentimientos: Precariousness and mental health", subtitle = "Reddit", x = "Sentimiento", y = "Total") +
  theme_elegante() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1), 
        axis.title.x = element_text(face = "bold"),
        axis.title.y = element_text(face = "bold")) +
  scale_x_discrete(labels = c("positive" = "positivo", "negative" = "negativo", "trust" = "confianza",
                              "sadness" = "tristeza", "fear" = "miedo", "anger" = "enfado",
                              "anticipation" = "anticipaci√≥n", "disgust" = "disgusto",
                              "surprise" = "sorpresa", "joy" = "alegr√≠a"))
```


```{r}
# Sentiment Analysis

sentiment_totals<- colSums(Sentimiento_Antiwork)
sentiment_totals_df <- data.frame(Sentiment = names(sentiment_totals), Total = sentiment_totals)

# Calcular los porcentajes de cada sentimiento
sentiment_totals_df$Porcentaje <- (sentiment_totals_df$Total / sum(sentiment_totals_df$Total)) * 100

# Graficar los sentimientos con porcentajes
ggplot(sentiment_totals_df, aes(x = reorder(Sentiment, -Total), y = Total, fill = Sentiment)) +
  geom_bar(stat = "identity") +
  labs(title = "Sentimientos:Precariousness and mental health",subtitle = "Reddit", x = "Sentimiento", y = "Total") +
  theme_elegante() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = "none",
        axis.title.x = element_text(face = "bold"),
        axis.title.y = element_text(face = "bold")) +
  scale_x_discrete(labels = c("positive" = "positivo", "negative" = "negativo", "trust" = "confianza",
                              "sadness" = "tristeza", "fear" = "miedo", "anger" = "enfado",
                              "anticipation" = "anticipaci√≥n", "disgust" = "disgusto",
                              "surprise" = "sorpresa", "joy" = "alegr√≠a")) +
  geom_text(aes(label = sprintf("%.1f%%", Porcentaje)), vjust = -0.5, size = 3)
```


```{r}
# Sumarizar las polaridades
polarity_counts <- colSums(Sentimiento_Antiwork[, 9:10])

# Crear un dataframe para facilitar la visualizaci√≥n
polarity_df <- data.frame(polarity = names(polarity_counts), count = polarity_counts)

# Crear un gr√°fico de barras para las polaridades
ggplot(polarity_df, aes(x = reorder(polarity, -count), y = count)) +
  geom_bar(stat = "identity", fill = "lightgreen") +
  labs(title = "Distribuci√≥n de Polaridades en los Comentarios de mental",
       x = "Polaridad", y = "Frecuencia") +
  theme_minimal()
```

```{r}
# Supongamos que tienes una columna "date" en el dataframe mental
# Convertir las fechas a formato Date
Antiwork$date <- as.Date(Antiwork$date)

# A√±adir las emociones al dataframe original
Antiwork <- cbind(Antiwork, Sentimiento_Antiwork)

# Calcular la media de cada emoci√≥n por fecha
emotion_trends <- Antiwork%>%
  group_by(date) %>%
  summarise(across(anger:positive, mean))

# Convertir a formato largo para ggplot2
emotion_trends_long <- emotion_trends %>%
  gather(key = "emotion", value = "average_score", -date)

# Crear un gr√°fico de l√≠neas para las emociones a lo largo del tiempo
ggplot(emotion_trends_long, aes(x = date, y = average_score, color = emotion)) +
  geom_line() +
  labs(title = "Tendencias de Emociones en los Comentarios de mental a lo largo del Tiempo",
       x = "Fecha", y = "Puntuaci√≥n Promedio de Emoci√≥n") +
  theme_minimal()
```

```{r}

# Gr√°fico de √°rea para las tendencias de emociones a lo largo del tiempo
ggplot(emotion_trends_long, aes(x = date, y = average_score, fill = emotion)) +
  geom_area() +
  labs(title = "Tendencias de Emociones en los Comentarios de mental a lo largo del Tiempo",
       x = "Fecha", y = "Puntuaci√≥n Promedio de Emoci√≥n", fill = "Emoci√≥n") +
  theme_minimal()
```



```{r}

library(tidytext)
library(reshape2)

# Convertir el corpus a un dataframe
comments_df <- data.frame(text = Antiwork$comment)

# Tokenizar los comentarios
comments_tokens <- comments_df %>%
  unnest_tokens(word, text)

# Unir los comentarios con el sentimiento de las palabras
comments_sentiment <- comments_tokens %>%
  inner_join(get_sentiments("bing"))

# Contar las palabras por sentimiento
word_sentiment_count <- comments_sentiment %>%
  count(word, sentiment, sort = TRUE)

# Convertir los datos a un formato adecuado para comparison.cloud
word_sentiment_matrix <- word_sentiment_count %>%
  acast(word ~ sentiment, value.var = "n", fill = 0)

# Crear la nube de palabras
comparison.cloud(word_sentiment_matrix,
                  colors = c("#F8766D", "#00BFC4"),
                  max.words = 100)
```



```{r}

sentimientos <- get_sentiment(Antiwork$comment)
summary(sentimientos)
```

```{r}

# Aseg√∫rate de que la columna date est√© en formato POSIXct
Antiwork$date <- as.POSIXct(Antiwork$date)

# Crear el data frame sentimientosdf y ordenar las fechas
sentimientosdf <- data.frame(sentimientos)
sentimientosdf$date <- sort(Antiwork$date)

# Crear el gr√°fico
ggplot(sentimientosdf, aes(x = date, y = sentimientos)) +
  geom_line(linewidth = 1.0, colour = "grey") +
  geom_smooth(method = "loess", se = FALSE, colour = "red", linewidth = 1.2) + 
  theme_bw() +
  scale_x_datetime(date_breaks = "1 month") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.3)) +
  labs(title = "Trayectoria emocional de los comentarios", x = "Fecha", y = "Grado emocional")
```

```{r}
porcent_val <- get_percentage_values(sentimientos, bins = 50)
plot(porcent_val, type = "l", col = "blue", 
     main = "Comentarios empleando medias basadas en %", 
     xlab = "Evoluci√≥n", ylab = "Grado emocional") 
abline(h = 0, lty = 2, col = "grey")
```
```{r}
simple_plot(sentimientos)
```
```{r}
emociones <- get_nrc_sentiment(Antiwork$comment) 
ggplot(emociones, aes(x = Antiwork$date)) +
  geom_smooth(aes(y = positive, colour = "Positivo"), method = "loess", size = 1.2, se = FALSE) +
  geom_smooth(aes(y = negative, colour = "Negativo"), method = "loess", size = 1.2, se = FALSE) +
  scale_colour_manual(values = c("Positivo" = "#62ca35", "Negativo" = "#dc143c")) +
  theme_bw() +
  scale_x_datetime(date_breaks = "1 month") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.3)) +
  labs(title = "Sentimientos", x = "Fecha", y = "Grado Emocional")
```

```{r}
emociones2 <- emociones
colnames(emociones2) <- c("Ira", "Anticipacion", "Disgusto", "Miedo", "Alegria", "Tristeza",
                          "Sorpresa", "Confianza", "Negativo", "Positivo")
emociones2$fecha <- Antiwork$date

# Transformar los datos a formato largo
emociones2_long <- gather(emociones2, key = "emocion", value = "sentimiento", -fecha)

# Filtrar las emociones que no sean "Negativo" y "Positivo"
emociones2_long <- subset(emociones2_long, emocion != "Negativo" & emocion != "Positivo")

# Convertir 'emocion' a factor
emociones2_long$emocion <- as.factor(emociones2_long$emocion)

ggplot(emociones2_long, aes(x = fecha, y = sentimiento, color = emocion)) +
  geom_smooth(method = "loess", size = 1.2, se = FALSE) + 
  theme_elegante() +
  scale_color_manual(values = brewer.pal(8, "Dark2"), name = "Emoci√≥n") +
  scale_x_datetime(date_breaks = "1 year") +  # Cambiado a intervalos anuales
  theme(axis.text.x = element_text(angle = 90, vjust = 0.3)) +
  labs(title = "Trayectoria emocional en los comentarios", x = "Fecha", y = "Grado emocional")
```
```{r}
# Cargar las bibliotecas necesarias
library(dplyr)
library(ggplot2)
library(udpipe)
library(textrank)
library(tidytext)
library(lubridate)

# Procesamiento del texto con UDPipe
ud_model <- udpipe_download_model(language = "english")
udpipe_model <- udpipe_load_model(ud_model$file_model)

# Anotaci√≥n del texto
anot <- udpipe_annotate(udpipe_model, x = Antiwork$comment)
anot <- as.data.frame(anot)

# Filtrar sustantivos
terminos_n <- subset(anot, upos == "NOUN")

# Calcular la frecuencia de los lemas de sustantivos
terminos_n <- txt_freq(terminos_n$lemma)

# Convertir los t√©rminos a factores con niveles ordenados
terminos_n$key <- factor(terminos_n$key, levels = rev(terminos_n$key))

# Graficar los sustantivos m√°s frecuentes
ggplot(head(terminos_n, 20), aes(x = key, y = freq)) +
  geom_bar(stat = "identity", fill = "cadetblue") +
  coord_flip() +
  labs(title = "Sustantivos m√°s frecuentes", x = "Frecuencia", y = "Sustantivos") +
  theme_minimal()

```
```{r}
# Filtrar nombres propios
terminos_np <- subset(anot, upos == "PROPN")

# Calcular la frecuencia de los lemas de nombres propios
terminos_np <- txt_freq(terminos_np$lemma)

# Convertir los t√©rminos a factores con niveles ordenados
terminos_np$key <- factor(terminos_np$key, levels = rev(terminos_np$key))

# Graficar los nombres propios m√°s frecuentes
grafico_nombres_propios <- ggplot(head(terminos_np, 20), aes(x = key, y = freq)) +
  geom_bar(stat = "identity", fill = "cadetblue") +
  coord_flip() +
  labs(title = "Nombres propios m√°s frecuentes", x = "Frecuencia", y = "Nombres propios") +
  theme_minimal()
grafico_nombres_propios
```

```{r}
# Filtrar adjetivos
terminos_a <- subset(anot, upos == "ADJ")

# Calcular la frecuencia de los lemas de adjetivos
terminos_a <- txt_freq(terminos_a$lemma)

# Convertir los t√©rminos a factores con niveles ordenados
terminos_a$key <- factor(terminos_a$key, levels = rev(terminos_a$key))

# Graficar los adjetivos m√°s frecuentes
grafico_adjetivos <- ggplot(head(terminos_a, 20), aes(x = key, y = freq)) +
  geom_bar(stat = "identity", fill = "cadetblue") +
  coord_flip() +
  labs(title = "Adjetivos m√°s frecuentes", x = "Frecuencia", y = "Adjetivos") +
  theme_minimal()
grafico_adjetivos
```

```{r}
terminos_v <- subset(anot, upos == "VERB")
terminos_v <- txt_freq(terminos_v$lemma)
terminos_v$key <- factor(terminos_v$key, levels = rev(terminos_v$key))
# Graficar los verbos m√°s frecuentes
grafico_verbos <- ggplot(head(terminos_v, 20), aes(x = key, y = freq)) +
  geom_bar(stat = "identity", fill = "cadetblue") +
  coord_flip() +
  labs(title = "Verbos m√°s frecuentes", x = "Frecuencia", y = "Verbos") +
  theme_minimal()
grafico_verbos
```

```{r}
library(lattice)
kwrake <- keywords_rake(x = anot, term = "lemma", group = "doc_id", # Agrupamos por documento (reddits)
                    relevant = anot$upos %in% c("NOUN", "ADJ","PROPN")) # S√≥lo sustantivos, adjetivos y
                                                                        #  nombres propios
kwrake$key <- factor(kwrake$keyword, levels = rev(kwrake$keyword))

# Graficamos palabras clave que aparezcan al menos 5 veces
barchart(key ~ rake, data = head(subset(kwrake, freq > 4), 20), col = "cadetblue",
         main = list("Identificar palabras clave mediante RAKE", cex="1"), xlab = "RAKE")
```

```{r}
kwpmi <- keywords_collocation(x = anot, term = "lemma", group = "doc_id")
kwpmi$key <- factor(kwpmi$keyword, levels = rev(kwpmi$keyword))

barchart(key ~ pmi, data = head(subset(kwpmi, freq > 9), 20), col = "cadetblue",
         main = list("Identificar palabras clave mediante Colocaci√≥n PMI", cex="1"),
         xlab = "PMI (Pointwise Mutual Information)")
```

```{r}
anot$phrase_tag <- as_phrasemachine(anot$upos, type = "upos")

# Cambiar etiqueta a "pronombres" y "n√∫meros"
anot$phrase_tag[anot$upos=="PRON"] <- "O"
anot$phrase_tag[anot$upos=="NUM"] <- "O"

# Obtenemos las frases nominales simples (expresi√≥n regular)
kwphrases <- keywords_phrases(x = anot$phrase_tag, term = tolower(anot$token),
                              pattern = "(A|N)*N(P+D*(A|N)*N)*",
                              is_regex = TRUE, detailed = FALSE)

# Filtramos aquellas que contienen m√°s de una palabra y aparecen m√°s de 3 veces
kwphrases <- subset(kwphrases, ngram > 1 & freq > 3)
kwphrases$key <- factor(kwphrases$keyword, levels = rev(kwphrases$keyword))

barchart(key ~ freq, data = head(kwphrases, 20), col = "cadetblue",
         main = list("Palabras Clave - Frases nominales simples", cex="1"), xlab = "Frecuencia")
```

```{r}
keywords <- textrank_keywords(anot$lemma, relevant = anot$upos %in% c("NOUN", "ADJ","PROPN"),
                              ngram_max = 8, sep = " ") # S√≥lo sustantivos, adjetivos y nombres propios

# Filtramos aquellas que contienen m√°s de una palabra y aparecen m√°s de 3 veces
keywords <- subset(keywords$keywords, ngram > 1 & freq > 15) 
keywords$keyword <- factor(keywords$keyword, levels = rev(keywords$keyword))

# Nube de palabras clave m√°s frecuentes
wordcloud(words=keywords$keyword, freq=keywords$freq, random.order=F, colors=brewer.pal(8,"Dark2"))
```


```{r}
library(formattable)
terminos <- subset(anot, upos %in% c("ADJ", "NOUN", "PROPN") & 
                     !lemma %in% c("birth control"))

# Qu√© palabras coocurren por reddits
cooc <- cooccurrence(terminos, group = "doc_id", term = "lemma")
head(data.frame(cooc)) %>% formattable()    
```



```{r}
wordnetwork <- graph_from_data_frame(head(cooc, 50))
ggraph(wordnetwork, layout = "fr") +
  geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = "pink") +
  geom_node_text(aes(label = name), col = "darkgreen", size = 3.5) +
  theme_graph(base_family = "sans") + theme(legend.position = "none") +
  labs(title = "Coocurrencias reddits/oraci√≥n", 
       subtitle = "Sustantivos, Adjetivos y Nombres Propios") +
  theme(plot.title = element_text(size = 14), plot.margin = margin(0,1,0,1))
```



```{r}
# Graficamos coocurrencias con skipgram = 1
wordnetwork <- graph_from_data_frame(head(cooc, 50))
ggraph(wordnetwork, layout = "fr") +
  geom_edge_link(aes(width = cooc, edge_alpha = cooc),edge_colour = "pink") +
  geom_node_text(aes(label = name), col = "darkgreen", size = 3.5) +
  theme_graph(base_family = "sans") +
  labs(title = "Qu√© palabras est√°n pr√≥ximas", subtitle = "Sustantivos, Adjetivos y Nombres Propios") +
  theme(plot.title = element_text(size = 14), plot.margin = margin(0,1,0,1))
```


```{r}
# Identificador √∫nico de cada oraci√≥n del corpus
anot$id <- unique_identifier(anot, fields = c("doc_id", "sentence_id"))

# Seleccionamos s√≥lo sustantivos, adjetivos y nombres propios
dtf <- subset(anot, upos %in% c("NOUN", "ADJ", "PROPN"))

# Creamos matriz documento-t√©rmino
dtf <- document_term_frequencies(dtf, document = "id", term = "lemma")
dtm <- document_term_matrix(dtf)

# Eliminamos t√©rminos que aparezcan menos de 10 veces
dtm <- dtm_remove_lowfreq(dtm, minfreq = 10)

# Eliminamos algunas palabras que no aportan nada al an√°lisis
dtm <- dtm_remove_terms(dtm, terms = c("birth control"))
```

```{r}
termcorr <- dtm_cor(dtm)
dim(termcorr)
```
```{r}
library(formattable)
library(dplyr)

# Crear un ejemplo de termcorr
set.seed(123)
termcorr <- matrix(runif(100, min = -1, max = 1), ncol = 10)
colnames(termcorr) <- paste0("Term", 1:10)
rownames(termcorr) <- paste0("Term", 1:10)

# Redondear y convertir a data frame
df <- as.data.frame(round(termcorr[1:10, 1:5], 2))

# Verificar si los datos est√°n correctamente cargados
print(df)

# Aplicar formattable con estilo
formatted_df <- formattable(df, list(
  area(col = 1:5) ~ color_tile("white", "blue"),
  area(row = 1:10, col = 1:5) ~ formatter(
    "span", style = x ~ style(display = "block", padding = "0 4px"))
))

# Mostrar el resultado
formatted_df

```

# WORD EMBEDDINGS 

```{r}
terminos <- subset(anot, upos %in% c("NOUN", "ADJ", "PROPN"), select = c("doc_id", "lemma"))
terminos <- split(terminos$lemma, terminos$doc_id)
terminos <- lapply(terminos, paste, collapse = " ")
docs <- do.call(rbind.data.frame, c(terminos, stringsAsFactors=FALSE))
colnames(docs) <- "texto"
docs$id <- as.numeric(str_sub(names(terminos),start = 4))
docs <- docs[order(docs$id), c("id", "texto")]
row.names(docs) <- NULL
rm(terminos)
```

```{r}
library(text2vec)
tokens <- space_tokenizer(docs$texto)
```

```{r}
# Crear iterador
it <- itoken(tokens, progressbar = FALSE)

# Crear el vocabulario
vocab <- create_vocabulary(it)
vocab <- vocab[order(vocab$term_count, decreasing = T),]
vocab$doc_porc <- vocab$doc_count/length(docs$texto)  # Porcentaje

# Filtramos los t√©rminos del vocabulario en aquellos que aparecen al menos 7 veces y que no aparezcan 
# en m√°s del 50% de los documentos (reddits)
vocab <- prune_vocabulary(vocab, term_count_min = 7, doc_proportion_max = 0.5)
```

```{r}
# Definir la funci√≥n de vectorizaci√≥n
vectorizer <- vocab_vectorizer(vocab)

# Construir la matriz de coocurrencia de t√©rminos
tcm <- create_tcm(it, vectorizer, skip_grams_window = 5)
```

```{r}
glove <- GlobalVectors$new(rank = 20, x_max = 10)
```

```{r}
wv_main <- glove$fit_transform(tcm, n_iter = 50, convergence_tol = 0.005)
```

```{r}
wv_context <- glove$components
dim(wv_context)
```



```{r}
word_vectors <- wv_main + t(wv_context)
```

```{r}
# vector("exposici√≥n")
wv <- word_vectors["resignation", ,drop = FALSE]
# Palabras relacionadas con "exposici√≥n"
cos_sim <- sim2(x = word_vectors, y = wv, method = "cosine", norm = "l2")
head(sort(cos_sim[,1], decreasing = TRUE), 5)
```
```{r}
# vector("anunciaci√≥n")
wv <- word_vectors["stress", ,drop = FALSE]
# Palabras relacionadas con "anunciaci√≥n"
cos_sim <- sim2(x = word_vectors, y = wv, method = "cosine", norm = "l2")
head(sort(cos_sim[,1], decreasing = TRUE), 5)
```
```{r}
# vector("mujer")
wv <- word_vectors["job", ,drop = FALSE]
# Palabras relacionadas con "mujer"
cos_sim <- sim2(x = word_vectors, y = wv, method = "cosine", norm = "l2")
head(sort(cos_sim[,1], decreasing = TRUE), 5)
```
```{r}
library(Rtsne)
library(ggrepel)
tsne <- Rtsne(word_vectors, dims = 2, perplexity = 20,max_iter = 2000)
tsne_plot <- data.frame(x = tsne$Y[,1], y = tsne$Y[,2], palabra = rownames(word_vectors))
ggplot(tsne_plot, aes(x,y)) +
  geom_text_repel(aes(label=palabra), size = 2.8, box.padding = 0.1) +
  labs(title = "Modelo GloVe (t-SNE)", subtitle = "Precariousness and mental health", x = "X", y = "Y") +
  theme_elegante()
```

```{r}

library(tm)
library(wordcloud)
library(RColorBrewer)
library(textstem)
# Convertir los datos de emociones en una sola cadena de texto
cloud_emotions_data <- c(
  paste(Antiwork$comment[Sentimiento_Antiwork$sadness > 0], collapse = " "),
  paste(Antiwork$comment[Sentimiento_Antiwork$joy > 0], collapse = " "),
  paste(Antiwork$comment[Sentimiento_Antiwork$anger > 0], collapse = " "),
  paste(Antiwork$comment[Sentimiento_Antiwork$fear > 0], collapse = " "),
  paste(Antiwork$comment[Sentimiento_Antiwork$trust > 0], collapse = " "),
  paste(Antiwork$comment[Sentimiento_Antiwork$anticipation > 0], collapse = " "),
  paste(Antiwork$comment[Sentimiento_Antiwork$disgust > 0], collapse = " "),
  paste(Antiwork$comment[Sentimiento_Antiwork$surprise > 0], collapse = " ")
)

# Crear un corpus de texto
cloud_corpus <- Corpus(VectorSource(cloud_emotions_data))

# Limpiar los datos de texto
cloud_corpus <- cloud_corpus %>%
  tm_map(content_transformer(tolower)) %>%
  tm_map(removePunctuation) %>%
  tm_map(removeNumbers) %>%
  tm_map(removeWords, c(stopwords("english"))) %>%
  tm_map(stripWhitespace) %>%
  tm_map(content_transformer(lemmatize_strings))

# Crear una matriz de t√©rminos por documento
cloud_tdm <- TermDocumentMatrix(cloud_corpus)
cloud_tdm <- as.matrix(cloud_tdm)

# Verificar cu√°ntas emociones tienen palabras asociadas
num_emotions <- ncol(cloud_tdm)
emotion_labels <- c('tristeza', 'alegr√≠a', 'enfado', 'miedo', 'confianza', 'anticipaci√≥n', 'disgusto', 'sorpresa')
emotion_labels <- emotion_labels[1:num_emotions]

# Asignar nombres de columnas a la matriz de t√©rminos
colnames(cloud_tdm) <- emotion_labels

# Generar la nube de palabras
set.seed(757)  # Se puede establecer cualquier entero
comparison.cloud(cloud_tdm, random.order = FALSE,
                 colors = brewer.pal(8, "Dark2"),
                 title.size = 1.2, max.words = 1000, scale = c(4, 0.5), rot.per = 0.4)

```

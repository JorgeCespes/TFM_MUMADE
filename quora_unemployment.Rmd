---
title: "quora_unemployment"
author: "Jorge C√©spedes Rico"
date: "2024-05-24"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# Load the necessary libraries
library(pdftools)
library(tm)
library(wordcloud)
library(RColorBrewer)
library(syuzhet)
library(ggplot2)
library(topicmodels)
library(textstem)
library(dplyr)
library(tidyr)
library(tidytext)
library(lubridate)
library(igraph)
library(ggraph)
library(cluster)
library(factoextra)
library(Rtsne)
```

```{r}

theme_elegante <- function(base_size = 10,
                           base_family = "Raleway"
                           )
    {
    color.background = "#FFFFFF" # Chart Background
    color.grid.major = "#D9D9D9" # Chart Gridlines
    color.axis.text = "#666666" # 
    color.axis.title = "#666666" # 
    color.title = "#666666"
    color.subtitle = "#666666"
    strip.background.color = '#9999CC'
    
    ret <-
        theme_bw(base_size=base_size) +
        
        # Set the entire chart region to a light gray color
        theme(panel.background=element_rect(fill=color.background, color=color.background)) +
        theme(plot.background=element_rect(fill=color.background, color=color.background)) +
        theme(panel.border=element_rect(color=color.background)) +
        
        # Format the grid
        theme(panel.grid.major=element_line(color=color.grid.major,size=.55, linetype="dotted")) +
        theme(panel.grid.minor=element_line(color=color.grid.major,size=.55, linetype="dotted")) +
        theme(axis.ticks=element_blank()) +
        
        # Format the legend, but hide by default
        theme(legend.position="none") +
        theme(legend.background = element_rect(fill=color.background)) +
        theme(legend.text = element_text(size=base_size-3,color=color.axis.title, family = base_family)) +
        
        theme(strip.text.x = element_text(size=base_size,color=color.background, family = base_family)) +
        theme(strip.text.y = element_text(size=base_size,color=color.background, family = base_family)) +
        #theme(strip.background = element_rect(fill=strip.background.color, linetype="blank")) +
        theme(strip.background = element_rect(fill = "grey70", colour = NA)) +
        # theme(panel.border= element_rect(fill = NA, colour = "grey70", size = rel(1)))+
        # Set title and axis labels, and format these and tick marks
        theme(plot.title=element_text(color=color.title, 
                                      size=20, 
                                      vjust=1.25, 
                                      family=base_family, 
                                      hjust = 0.5
                                      )) +
        
        theme(plot.subtitle=element_text(color=color.subtitle, size=base_size+2, family = base_family,  hjust = 0.5))  +
        
        theme(axis.text.x=element_text(size=base_size,color=color.axis.text, family = base_family)) +
        theme(axis.text.y=element_text(size=base_size,color=color.axis.text, family = base_family)) +
        theme(text=element_text(size=base_size, color=color.axis.text, family = base_family)) +
        
        theme(axis.title.x=element_text(size=base_size+2,color=color.axis.title, vjust=0, family = base_family)) +
        theme(axis.title.y=element_text(size=base_size+2,color=color.axis.title, vjust=1.25, family = base_family)) +
        theme(plot.caption=element_text(size=base_size-2,color=color.axis.title, vjust=1.25, family = base_family)) +
        
        # Legend  
        theme(legend.text=element_text(size=base_size,color=color.axis.text, family = base_family)) +
        theme(legend.title=element_text(size=base_size,color=color.axis.text, family = base_family)) +
        theme(legend.key=element_rect(colour = color.background, fill = color.background)) +
        theme(legend.position="bottom", 
              legend.box = "horizontal", 
              legend.title = element_blank(),
              legend.key.width = unit(.75, "cm"),
              legend.key.height = unit(.75, "cm"),
              legend.spacing.x = unit(.25, 'cm'),
              legend.spacing.y = unit(.25, 'cm'),
              legend.margin = margin(t=0, r=0, b=0, l=0, unit="cm")) +

        # Plot margins
        theme(plot.margin = unit(c(.5, .5, .5, .5), "cm"))
    
    ret
}
```



```{r}
# List all PDF files in the working directory
files <- list.files(pattern = "pdf$")
```

```{r}
# Apply the pdf_text function to each PDF file
quora <- lapply(files, pdf_text)
```

```{r}
# Combine all texts into a single character vector
quora_text <- unlist(quora)
```

```{r}
# Create a text corpus from the extracted text
corpus <- Corpus(VectorSource(quora_text))

```

```{r}
# Definir nuevas stopwords
new_stopwords <- c("upvote", "upvotes", "view", "views", "y", "dfollow", "answer", "answers")

# Agregar las nuevas stopwords a la lista existente
custom_stopwords <- c(stopwords("english"), new_stopwords)

# Clean the text data
clean_corpus <- corpus %>%
  tm_map(content_transformer(tolower)) %>%
  tm_map(removePunctuation) %>%
  tm_map(removeNumbers) %>%
  tm_map(removeWords, custom_stopwords) %>%
  tm_map(stripWhitespace) %>%
  tm_map(content_transformer(lemmatize_strings))

```

```{r}
# Create a Document-Term Matrix (DTM)
dtm <- DocumentTermMatrix(clean_corpus)

# Inspect the DTM
inspect(dtm[1:10, 1:10])
```

```{r}
# N-grams Analysis
# Create a function to tokenize into bigrams
BigramTokenizer <- function(x) {
  unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
}
```

```{r}
# Create a DTM for bigrams
dtm_bigrams <- DocumentTermMatrix(clean_corpus, control = list(tokenize = BigramTokenizer))

```

```{r}
# Inspect the DTM for bigrams
inspect(dtm_bigrams[1:10, 1:10])

```

```{r}
# Calculate Term Frequency-Inverse Document Frequency (TF-IDF)
tfidf <- weightTfIdf(dtm)
tfidf_matrix <- as.matrix(tfidf)

# Find the top terms by TF-IDF
top_terms <- sort(colSums(tfidf_matrix), decreasing = TRUE)
print(head(top_terms, 15))

# Convertir los top_terms en un data frame
top_terms_df <- data.frame(term = names(top_terms), tfidf = top_terms)
# Filtrar los t√©rminos espec√≠ficos y aquellos que contienen "..." o "'" o "‚Äô"
top_terms_df <- top_terms_df %>%
  filter(!term %in% c("‚Åé", "‚àí", "‚Ä¶", "yes", "std", "unstd","‚Äô", "swb", "pwb", "almps", "‚Äì", "*", "ùñ•", "***", "‚àó‚àó‚àó")) %>%
  filter(!grepl("\\.\\.\\.", term)) %>%  # Filtrar t√©rminos que contienen "..."
  filter(!grepl("'", term)) %>%  # Filtrar t√©rminos que contienen "'"
  filter(!grepl("‚Äô", term)) %>%  # Filtrar t√©rminos que contienen "‚Äô"
  arrange(desc(tfidf)) %>%
  head(15)

# Ordenar los t√©rminos por TF-IDF
top_terms_df <- top_terms_df %>% arrange(desc(tfidf))

# Seleccionar los 10 primeros t√©rminos
top_terms_df <- head(top_terms_df, 15)

# Cargar la biblioteca ggplot2
library(ggplot2)

# Graficar los principales t√©rminos por TF-IDF con una paleta de colores diferente
ggplot(top_terms_df, aes(x = reorder(term, tfidf), y = tfidf, fill = term)) +
  geom_bar(stat = "identity", color = "black")  +  # Usar la paleta de colores viridis
  labs(title = "Top 15 t√©rminos  por TF-IDF", x = "T√©rmino", y = "TF-IDF") +
  theme_elegante() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1), 
        legend.position = "none",
        axis.title.x = element_text(face = "bold"),  # T√≠tulo del eje x en negrita
        axis.title.y = element_text(face = "bold"))  # T√≠tulo del eje y en negrita
top_terms_df
```

```{r}
# Topic Modeling using LDA
num_topics <- 5
lda_model <- LDA(dtm, k = num_topics, control = list(seed = 1234))

# Extract topics and terms
lda_terms <- terms(lda_model, 10)
print(lda_terms)

```

```{r}
# Sentiment Analysis
# Get the sentiment scores for each document
sentiments <- get_nrc_sentiment(quora_text)
```

```{r}
# Sum the sentiment scores for all documents
sentiment_totals <- colSums(sentiments)
print(sentiment_totals)

# Convertir sentiment_totals en un data frame
sentiment_totals_df <- data.frame(Sentimiento = names(sentiment_totals), Total = sentiment_totals)



ggplot(sentiment_totals_df, aes(x = reorder(Sentimiento, -Total), y = Total, fill = Sentimiento)) +
  geom_bar(stat = "identity") +
  scale_fill_viridis_d() +  # Escala de colores
  labs(title = "Totales de Sentimiento por Emoci√≥n",
       x = "Emoci√≥n",
       y = "Total",
       fill = "Emoci√≥n") +
  theme_classic() +  # Tema minimalista
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotar etiquetas del eje x
```


```{r}

# Realizar an√°lisis de sentimientos
sentimientos <- get_nrc_sentiment(quora_text)

# Calcular el total de sentimientos
total_sentimientos <- colSums(sentimientos)

# Crear un marco de datos con los totales de sentimientos
total_sentimientos_df <- data.frame(Sentimiento = names(total_sentimientos), Total = total_sentimientos)

```

```{r}
ggplot(total_sentimientos_df, aes(x = reorder(Sentimiento, -Total), y = Total, fill = Sentimiento)) +
  geom_bar(stat = "identity") +
  labs(title = "Sentimientos: Unemployment and mental health", subtitle = "Quora", x = "Sentimiento", y = "Total") +
  theme_elegante() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = "none",
        axis.title.x = element_text(face = "bold"),
        axis.title.y = element_text(face = "bold")) +
  scale_x_discrete(labels = c("positive" = "positivo", "negative" = "negativo", "trust" = "confianza",
                              "sadness" = "tristeza", "fear" = "miedo", "anger" = "enfado",
                              "anticipation" = "anticipaci√≥n", "disgust" = "disgusto",
                              "surprise" = "sorpresa", "joy" = "alegr√≠a"))

```




```{r}
# Plot the sentiment scores
barplot(sentiment_totals, las = 2, col = rainbow(10), 
        main = "Sentiment Analysis", ylab = "Count")
```

```{r}
# Sentiment Analysis using different lexicons
# Syuzhet
syuzhet_vector <- get_sentiment(quora_text, method = "syuzhet")
# Bing
bing_vector <- get_sentiment(quora_text, method = "bing")
# Afinn
afinn_vector <- get_sentiment(quora_text, method = "afinn")

```

```{r}
sentiment_df <- data.frame(
  syuzhet = syuzhet_vector,
  bing = bing_vector,
  afinn = afinn_vector,
  doc_id = 1:length(syuzhet_vector)
)

# Reshape the data frame for ggplot2
sentiment_melt <- sentiment_df %>%
  pivot_longer(cols = -doc_id, names_to = "method", values_to = "score")
```

```{r}
# Plot sentiment scores over documents
ggplot(sentiment_melt, aes(x = doc_id, y = score, color = method)) +
  geom_line() +
  labs(title = "Sentiment Scores Over Documents",
       x = "Document ID", y = "Sentiment Score") +
  theme_minimal()
```

```{r}
# Visualizing Sentiment Across Different Methods
par(mfrow = c(3, 1))
plot(syuzhet_vector, type = "l", main = "Syuzhet Sentiment Trajectory", 
     xlab = "Document ID", ylab = "Sentiment Score")
plot(bing_vector, type = "l", main = "Bing Sentiment Trajectory", 
     xlab = "Document ID", ylab = "Sentiment Score")
plot(afinn_vector, type = "l", main = "Afinn Sentiment Trajectory", 
     xlab = "Document ID", ylab = "Sentiment Score")
```


```{r}
library(ggwordcloud)
library(viridis)
# Combine all cleaned text into a single data frame
cleaned_text <- data.frame(text = sapply(clean_corpus, as.character), stringsAsFactors = FALSE)

# Remove unwanted characters (".", "...", "'", "‚Äô", "s", "¬∑", "‚Äú‚Äù, etc.)
cleaned_text$text <- gsub("[\\.]{2,}", "", cleaned_text$text)  # Remove multiple dots
cleaned_text$text <- gsub("'", "", cleaned_text$text)
cleaned_text$text <- gsub("‚Äô", "", cleaned_text$text)
cleaned_text$text <- gsub("¬∑", "", cleaned_text$text)
cleaned_text$text <- gsub("and|the|of|j‚Äô‚Äô|‚Äú|‚Äù", "", cleaned_text$text)  # Remove specific words and characters
cleaned_text$text <- gsub("[[:punct:]]", "", cleaned_text$text)  # Remove punctuation
cleaned_text$text <- gsub("\\s+", " ", cleaned_text$text)  # Remove extra whitespace

# Tokenize the text
words <- unlist(strsplit(cleaned_text$text, " "))

# Remove empty strings and unwanted short words from the tokenized words
unwanted_words <- c("p", "o", "ob", "s", "j", "t", "b", "de", "c", "e", "	ùêä", "quora", "don" )
words <- words[words != "" & !words %in% unwanted_words]

# Create word frequency table
word_freq <- table(words)
word_freq_df <- as.data.frame(word_freq)
colnames(word_freq_df) <- c("word", "freq")

# Filter words with a minimum frequency
word_freq_df <- word_freq_df[word_freq_df$freq >= 250,]

# Plot word cloud using ggwordcloud
ggplot(word_freq_df, aes(label = word, size = freq, color = freq)) +
  geom_text_wordcloud_area(rm_outside = TRUE) +  # Ensure words don't overlap and stay within bounds
  scale_size_area(max_size = 30) +
  scale_color_viridis_c() +  # Set color scheme for the text
  theme_elegante() +
  labs(title = "Unemployment and mental health", 
       subtitle = "M√≠nima frecuencia: 250") +
  theme(
    plot.title = element_text(size = 20, hjust = 0.5, vjust = 1, face = "bold"),  # Centered title at the top
    plot.subtitle = element_text(size = 12, hjust = 0.5, color = "darkgrey"),  # Centered subtitle below the title
    plot.margin = margin(10, 10, 10, 10)
  )

```


```{r}
# Cargar bibliotecas necesarias
library(pdftools)
library(tm)
library(wordcloud)
library(RColorBrewer)
library(syuzhet)
library(ggplot2)
library(topicmodels)
library(textstem)
library(dplyr)
library(tidyr)
library(tidytext)
library(lubridate)
library(cluster)
library(factoextra)
library(Rtsne)

# Listar todos los archivos PDF en el directorio de trabajo
files <- list.files(pattern = "pdf$")

# Aplicar la funci√≥n pdf_text a cada archivo PDF
quora <- lapply(files, pdf_text)

# Combinar todos los textos en un solo vector de caracteres
quora_text <- unlist(quora)

# Crear un corpus de texto a partir del texto extra√≠do
corpus <- Corpus(VectorSource(quora_text))

# Definir nuevas stopwords
new_stopwords <- c("upvote", "upvotes", "view", "views", "y", "dfollow", "answer", "answers")

# Agregar las nuevas stopwords a la lista existente
custom_stopwords <- c(stopwords("english"), new_stopwords)

# Limpiar los datos de texto
clean_corpus <- corpus %>%
  tm_map(content_transformer(tolower)) %>%
  tm_map(removePunctuation) %>%
  tm_map(removeNumbers) %>%
  tm_map(removeWords, custom_stopwords) %>%
  tm_map(stripWhitespace) %>%
  tm_map(content_transformer(lemmatize_strings))

# Crear una Document-Term Matrix (DTM)
dtm <- DocumentTermMatrix(clean_corpus)

# Convertir la DTM a una matriz
dtm_matrix <- as.matrix(dtm)

# Normalizar la matriz DTM usando TF-IDF
tfidf <- weightTfIdf(dtm)
tfidf_matrix <- as.matrix(tfidf)

# Eliminar t√©rminos escasos para reducir el ruido y la dimensionalidad
dtm_sparse <- removeSparseTerms(dtm, 0.99)
dtm_sparse_matrix <- as.matrix(dtm_sparse)

# Determinar el n√∫mero √≥ptimo de clusters usando el m√©todo del codo
fviz_nbclust(dtm_sparse_matrix, kmeans, method = "wss")

# Aplicar k-means clustering con un n√∫mero √≥ptimo de clusters (por ejemplo, k = 3)
set.seed(1234)
num_clusters <- 3
kmeans_result <- kmeans(dtm_sparse_matrix, centers = num_clusters, nstart = 25)

# Verificar la cantidad de documentos y clusters
print(length(quora_text))  # Deber√≠a ser 26
print(length(kmeans_result$cluster))  # Deber√≠a ser 26

# A√±adir las asignaciones de clusters a los datos originales
quora_clusters <- data.frame(text = quora_text, cluster = kmeans_result$cluster)

# Imprimir el resultado del clustering
print(quora_clusters)

# Visualizar el resultado del clustering usando PCA
pca <- prcomp(dtm_sparse_matrix, scale. = TRUE)
pca_data <- data.frame(pca$x, cluster = as.factor(kmeans_result$cluster))

ggplot(pca_data, aes(x = PC1, y = PC2, color = cluster)) +
  geom_point() +
  labs(title = "PCA of Document Clusters", x = "Principal Component 1", y = "Principal Component 2") +
  theme_minimal()

# Visualizar el resultado del clustering usando t-SNE
set.seed(1234)
tsne_result <- Rtsne(dtm_sparse_matrix, dims = 2, perplexity = 5, verbose = TRUE, max_iter = 500)
tsne_data <- data.frame(tsne_result$Y, cluster = as.factor(kmeans_result$cluster))
colnames(tsne_data) <- c("Dim1", "Dim2", "cluster")

ggplot(tsne_data, aes(x = Dim1, y = Dim2, color = cluster)) +
  geom_point() +
  labs(title = "t-SNE of Document Clusters", x = "Dimension 1", y = "Dimension 2") +
  theme_minimal()

```

```{r}
# Load necessary libraries
library(pdftools)
library(tm)
library(dplyr)
library(tidytext)
library(ggplot2)

# List all PDF files in the working directory
files <- list.files(pattern = "pdf$")

# Apply the pdf_text function to each PDF file
quora <- lapply(files, pdf_text)

# Combine all texts into a single character vector
quora_text <- unlist(quora)

# Create a text corpus from the extracted text
corpus <- Corpus(VectorSource(quora_text))

# Clean the text data
clean_corpus <- corpus %>%
  tm_map(content_transformer(tolower)) %>%
  tm_map(removePunctuation) %>%
  tm_map(removeNumbers) %>%
  tm_map(removeWords, c(stopwords("english"), new_stopwords)) %>%  # Remove additional stopwords
  tm_map(stripWhitespace)

# Combine all cleaned text into a single data frame
cleaned_text <- data.frame(text = sapply(clean_corpus, as.character), stringsAsFactors = FALSE)

# Tokenize the text into bigrams
bigrams <- cleaned_text %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

# Count the frequency of each bigram
bigram_counts <- bigrams %>%
  count(bigram, sort = TRUE)

# Filter to keep only the top 20 bigrams
top_bigrams <- bigram_counts %>%
  top_n(20, wt = n)

# Visualize the top 20 bigrams without legend
ggplot(top_bigrams, aes(x = reorder(bigram, n), y = n, fill = reorder(bigram, n))) +
  geom_col() +
  coord_flip() +
  labs(title = "Top 20 bigramas", x = "Bigrama", y = "Frecuencia") +
  theme_elegante() +                      # Apply a classic theme
  theme(axis.text.x = element_text(angle = 45, hjust = 1), 
        axis.title.x = element_text(face = "bold"),  # T√≠tulo del eje x en negrita
        axis.title.y = element_text(face = "bold"),  # T√≠tulo del eje y en negrita
        legend.position = "none")  # Rotate x-axis labels and remove legend

# Tokenize the text into trigrams
trigrams <- cleaned_text %>%
  unnest_tokens(trigram, text, token = "ngrams", n = 3)

# Count the frequency of each trigram
trigram_counts <- trigrams %>%
  count(trigram, sort = TRUE)

# Filter to keep only the top 20 trigrams
top_trigrams <- trigram_counts %>%
  top_n(20, wt = n)

# Visualize the top 20 trigrams without legend
ggplot(top_trigrams, aes(x = reorder(trigram, n), y = n, fill = reorder(trigram, n))) +
  geom_col() +
  coord_flip() +
  labs(title = "Top 20 trigramas", x = "Trigram", y = "Frequency") +
  theme_elegante() +                      # Apply a classic theme
  theme(axis.text.x = element_text(angle = 45, hjust = 1), 
        axis.title.x = element_text(face = "bold"),  # T√≠tulo del eje x en negrita
        axis.title.y = element_text(face = "bold"),  # T√≠tulo del eje y en negrita
        legend.position = "none")  # Rotate x-axis labels and remove legend

```



```{r}
# Load necessary libraries
library(pdftools)
library(tm)
library(dplyr)
library(tidytext)
library(ggplot2)
library(tidyr)
library(igraph)
library(ggraph)

# List all PDF files in the working directory
files <- list.files(pattern = "pdf$")

# Apply the pdf_text function to each PDF file
quora <- lapply(files, pdf_text)

# Combine all texts into a single character vector
quora_text <- unlist(quora)

# Create a text corpus from the extracted text
corpus <- Corpus(VectorSource(quora_text))

# Define additional stopwords
additional_stopwords <- c("upvotes", "view", "views", "y", "related", "follow", "upvote", "answers")

# Clean the text data and remove specific words
clean_corpus <- corpus %>%
  tm_map(content_transformer(tolower)) %>%
  tm_map(removePunctuation) %>%
  tm_map(removeNumbers) %>%
  tm_map(removeWords, c(stopwords("english"), additional_stopwords)) %>%
  tm_map(stripWhitespace)

# Combine all cleaned text into a single data frame
cleaned_text <- data.frame(text = sapply(clean_corpus, as.character), stringsAsFactors = FALSE)

# Tokenize the text into bigrams
bigrams <- cleaned_text %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

# Separate the bigrams into two columns
bigrams_separated <- bigrams %>%
  separate(bigram, into = c("word1", "word2"), sep = " ")

# Count the frequency of each bigram
bigram_counts <- bigrams_separated %>%
  count(word1, word2, sort = TRUE)

# Filter to keep only the top 20 bigrams for visualization
top_bigrams <- bigram_counts %>%
  top_n(40, wt = n)

# Create a graph object using igraph
bigram_graph <- graph_from_data_frame(top_bigrams)

# Plot the bigram network using ggraph
set.seed(1234)
ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "lightpink") +
  geom_node_point(size = 5, color = "lightgreen") +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void() +
  labs(title = "Bigram Network of Text Corpus")

# For trigrams (optional)
# Tokenize the text into trigrams
trigrams <- cleaned_text %>%
  unnest_tokens(trigram, text, token = "ngrams", n=3)

# Separate the trigrams into three columns
trigrams_separated <- trigrams %>%
  separate(trigram, into = c("word1", "word2", "word3"), sep = " ")

# Count the frequency of each trigram
trigram_counts <- trigrams_separated %>%
  count(word1, word2, word3, sort = TRUE)

# Filter to keep only the top 20 trigrams for visualization
top_trigrams <- trigram_counts %>%
  top_n(40, wt = n)

# Create a graph object using igraph
trigram_graph <- graph_from_data_frame(top_trigrams, directed = TRUE)

# Plot the trigram network using ggraph
set.seed(1234)
ggraph(trigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), arrow = arrow(type = "closed", length = unit(0.15, "inches")), edge_colour = "yellow") +
  geom_node_point(size = 5, color = "coral") +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void() +
  labs(title = "Trigram Network of Text Corpus")


```


```{r}
# Define a list of unwanted words
unwanted_words <- c("di", "yr", "doi", "org", "o", "p", "https", "res", 
                    "claimant count", "std", "unstd", "be", "garciag√≥mez", 
                    "int", "macmillan", "ppi", "yes", "claimant", "count", "iec", "fjmd", "t", "pa", "creed", "univ", "connecticut", "garc√≠ag√≥mez", "sjpsagepubcom", "tue", "sep", "ii", "download", "iza", "e", "ects", "jstor", "ltd", "ùêä", "q", "de", "witte", "copyright", "john", "wiley", "httpsdoiorgjournalpone", "sarti", "zella", "s", "j", "epidemiol", "jung", "dw", "kwak", "kk", "vol", "pp", "plos", "bmc")

# Tokenize the text into bigrams
bigrams <- cleaned_text %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

# Separate the bigrams into two columns
bigrams_separated <- bigrams %>%
  separate(bigram, into = c("word1", "word2"), sep = " ")

# Filter out unwanted words
bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% unwanted_words & !word2 %in% unwanted_words)

# Count the frequency of each bigram
bigram_counts <- bigrams_filtered %>%
  count(word1, word2, sort = TRUE)

# Filter to keep only the top 60 bigrams for visualization
top_bigrams <- bigram_counts %>%
  top_n(60, wt = n)

# Create a graph object using igraph
bigram_graph <- graph_from_data_frame(top_bigrams)

# Plot the bigram network using ggraph
set.seed(1234)
ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), edge_width = 0.6, edge_colour = "red3",  # Change color to "darkgray"
                 arrow = arrow(type = "closed", length = unit(0.185, "inches"))) +
  geom_node_point(size = 5, color = "darkolivegreen3") +
  geom_node_text(aes(label = name), repel = TRUE, size = 3.25) +
  theme_void() +
  labs(title = "Red de bigramas: Unemployment and mental health", 
       subtitle = "Top 60")

# Tokenize the text into trigrams
trigrams <- cleaned_text %>%
  unnest_tokens(trigram, text, token = "ngrams", n = 3)

# Separate the trigrams into three columns
trigrams_separated <- trigrams %>%
  separate(trigram, into = c("word1", "word2", "word3"), sep = " ")

# Filter out unwanted words
trigrams_filtered <- trigrams_separated %>%
  filter(!word1 %in% unwanted_words & !word2 %in% unwanted_words & !word3 %in% unwanted_words)

# Count the frequency of each trigram
trigram_counts <- trigrams_filtered %>%
  count(word1, word2, word3, sort = TRUE)

# Filter to keep only the top 60 trigrams for visualization
top_trigrams <- trigram_counts %>%
  top_n(60, wt = n)

# Create a graph object using igraph
trigram_graph <- graph_from_data_frame(top_trigrams, directed = TRUE)

# Plot the trigram network using ggraph with kk layout
set.seed(1234)
ggraph(trigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), edge_width = 0.6, 
                 arrow = arrow(type = "closed", length = unit(0.185, "inches")), 
                 edge_colour = "sienna") +  # Change color to "darkgray"
  geom_node_point(size = 5, color = "steelblue3") +
  geom_node_text(aes(label = name), repel = TRUE, size = 3.25) +
  theme_void() +
  labs(title = "Red de trigramas: Unemployment and mental health", 
       subtitle = "Top 60")



```


```{r}
# Load necessary libraries
library(pdftools)
library(tm)
library(wordcloud)
library(RColorBrewer)

# List all PDF files in the working directory
files <- list.files(pattern = "pdf$")

# Apply the pdf_text function to each PDF file
quora <- lapply(files, pdf_text)

# Combine all texts into a single character vector
quora_text <- unlist(quora)

# Create a text corpus from the extracted text
corpus <- Corpus(VectorSource(quora_text))

# Clean the text data
clean_corpus <- corpus %>%
  tm_map(content_transformer(tolower)) %>%
  tm_map(removePunctuation) %>%
  tm_map(removeNumbers) %>%
  tm_map(removeWords, stopwords("english")) %>%
  tm_map(stripWhitespace)

# Combine all cleaned text into a single data frame
cleaned_text <- data.frame(text = sapply(clean_corpus, as.character), stringsAsFactors = FALSE)

# Tokenize the text
words <- unlist(strsplit(cleaned_text$text, " "))

# Create word frequency table
word_freq <- table(words)

# Plot word cloud of most frequent words
wordcloud(words = names(word_freq), freq = word_freq, min.freq = 180, 
          scale=c(5, 0.5), colors=brewer.pal(8, "Dark2"))


```


```{r}
library(tidytext)
library(pdftools)
library(tm)
library(dplyr)
library(reshape2)
library(wordcloud)

# List all PDF files in the working directory
pdf_files <- list.files(pattern = "pdf$")

# Crear una funci√≥n para extraer texto de PDFs
extract_text_from_pdf <- function(pdf_file) {
  text <- pdf_text(pdf_file)
  return(text)
}

# Aplicar la funci√≥n a cada PDF para extraer texto
pdf_texts <- lapply(pdf_files, extract_text_from_pdf)

# Convertir el texto de los PDFs a un solo vector de texto
all_text <- unlist(pdf_texts)

# Convertir el corpus a un dataframe
comments_df <- data.frame(text = all_text)

# Tokenizar los comentarios
comments_tokens <- comments_df %>%
  unnest_tokens(word, text)

# Unir los comentarios con el sentimiento de las palabras
comments_sentiment <- comments_tokens %>%
  inner_join(get_sentiments("bing"))

# Contar las palabras por sentimiento
word_sentiment_count <- comments_sentiment %>%
  count(word, sentiment, sort = TRUE)

# Convertir los datos a un formato adecuado para comparison.cloud
word_sentiment_matrix <- word_sentiment_count %>%
  acast(word ~ sentiment, value.var = "n", fill = 0)

# Crear la nube de palabras
comparison.cloud(word_sentiment_matrix,
                  colors = c("#F8766D", "#00BFC4"),
                  max.words = 100)

```
```{r}
library(dplyr)
library(tidyr)
library(tidytext)
library(wordcloud)
library(reshape2)
library(RColorBrewer)

# Convertir el corpus a un dataframe
comments_df <- data.frame(text = all_text)

# Tokenizar los comentarios
comments_tokens <- comments_df %>%
  unnest_tokens(word, text)

# Obtener los sentimientos de las palabras utilizando el diccionario "nrc"
sentimientos <- get_sentiments("nrc")

# Unir los comentarios con los sentimientos de las palabras
comments_sentiment <- comments_tokens %>%
  inner_join(sentimientos, by = "word")

# Traducir los nombres de los sentimientos al espa√±ol
comments_sentiment$sentiment <- recode(comments_sentiment$sentiment,
                                       "disgust" = "disgusto",
                                       "anticipation" = "anticipaci√≥n",
                                       "anger" = "enfado",
                                       "trust" = "confianza",
                                       "surprise" = "sorpresa",
                                       "sadness" = "tristeza",
                                       "positive" = "positivo",
                                       "negative" = "negativo",
                                       "joy" = "alegr√≠a",
                                       "fear" = "miedo")

# Filtrar los sentimientos "positivo" y "negativo"
comments_sentiment <- comments_sentiment %>%
  filter(!sentiment %in% c("positivo", "negativo"))

# Contar las palabras por sentimiento
word_sentiment_count <- comments_sentiment %>%
  count(word, sentiment, sort = TRUE)

# Convertir los datos a un formato adecuado para comparison.cloud
word_sentiment_matrix <- acast(word_sentiment_count, word ~ sentiment, value.var = "n", fill = 0)

# Escalar los valores para cada sentimiento para mejorar la visualizaci√≥n
word_sentiment_matrix_scaled <- word_sentiment_matrix / rowSums(word_sentiment_matrix) * 100

# Ajustar el tama√±o m√≠nimo y m√°ximo de las palabras para mostrar m√°s palabras
min_freq <- 1
max_freq <- 200

# Filtrar palabras con frecuencia m√≠nima y m√°xima
filtered_words <- rowSums(word_sentiment_matrix_scaled) >= min_freq & rowSums(word_sentiment_matrix_scaled) <= max_freq
word_sentiment_matrix_scaled_filtered <- word_sentiment_matrix_scaled[filtered_words, ]

# Crear la nube de palabras con diferentes colores para cada sentimiento y ajustes en el tama√±o
comparison.cloud(word_sentiment_matrix_scaled_filtered,
                 colors = brewer.pal(ncol(word_sentiment_matrix_scaled_filtered), "Dark2"),  # Escala de colores Dark2
                 max.words = 150,  # Aumentar el n√∫mero m√°ximo de palabras
                 scale = c(2.5, 0.2),  # Ajustar el tama√±o de las palabras
                 random.order = FALSE,  # Desactivar el orden aleatorio
                 rot.per = 0.1)  # Rotaci√≥n porcentaje para mayor legibilidad

```




```{r}
# Funci√≥n para crear matriz de coocurrencia
create_cooccurrence_matrix <- function(dtm) {
  dtm_matrix <- as.matrix(dtm)
  term_matrix <- t(dtm_matrix) %*% dtm_matrix
  term_matrix[lower.tri(term_matrix, diag = TRUE)] <- 0
  term_matrix
}

# Crear la matriz de coocurrencia
cooccurrence_matrix <- create_cooccurrence_matrix(dtm)

# Convertir la matriz de coocurrencia en un dataframe
cooccurrence_df <- as.data.frame(as.table(cooccurrence_matrix))
colnames(cooccurrence_df) <- c("term1", "term2", "frequency")

# Filtrar t√©rminos con coocurrencia baja y eliminar caracteres no deseados
cooccurrence_df <- cooccurrence_df %>%
  filter(frequency > 1500) %>%
  filter(!term1 %in% c("‚Äì", "‚Äô", "‚Äò", "‚àí", "‚Åé", "‚Äú", "‚Äù", "ùñ•", "‚àó‚àó‚àó", "‚àó‚àó", "‚â•") & 
         !term2 %in% c("‚Äì", "‚Äô", "‚Äò", "‚àí", "‚Åé", "‚Äú", "‚Äù", "ùñ•", "‚àó‚àó‚àó", "‚àó‚àó", "‚â•"))

# Crear un grafo a partir de la matriz de coocurrencia
cooccurrence_graph <- graph_from_data_frame(cooccurrence_df, directed = FALSE)

# Aumentar la transparencia de las l√≠neas y ajustar el tama√±o del grafo
set.seed(175)  # Asegura la reproducibilidad
ggraph(cooccurrence_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = frequency, edge_width = frequency), color = "pink3", show.legend = TRUE) +
  geom_node_point(color = "springgreen3", size = 4) +
  geom_node_text(aes(label = name), repel = TRUE, size = 3.25) +  # Usar repel para mejorar la distribuci√≥n de etiquetas
  scale_edge_width_continuous(name = "n") +
  scale_edge_alpha_continuous(name = "n") +
  theme_void() +
  labs(title = "Grafo de concurrencia de t√©rminos", 
       subtitle = "Frecuencia > 1500") +
  theme(plot.title = element_text(size = 12, face = "bold"),
        plot.subtitle = element_text(size = 10),
        plot.caption = element_text(size = 8, hjust = 1))

```



```{r}
library(pdftools)
library(tm)
library(dplyr)
library(tidyr)
library(ggplot2)
library(tokenizers)
library(syuzhet)

# Listar todos los archivos PDF en el directorio de trabajo
pdf_files <- list.files(pattern = "pdf$")

# Funci√≥n para extraer texto de los PDFs
extract_text_from_pdf <- function(pdf_file) {
  text <- pdf_text(pdf_file)
  return(text)
}

# Aplicar la funci√≥n a cada archivo PDF para extraer texto
pdf_texts <- lapply(pdf_files, extract_text_from_pdf)

# Convertir el texto de los PDFs en un solo vector de texto
all_text <- unlist(pdf_texts)

# Preprocesar el texto
clean_text <- all_text %>%
  tolower() %>%
  removePunctuation() %>%
  removeNumbers() %>%
  removeWords(stopwords("en")) %>%
  removeWords(c("upvotes", "view", "views", "y", "related", "follow", "upvote", "answers")) %>%
  stripWhitespace()

# Realizar an√°lisis de sentimientos usando syuzhet
sentiments <- get_nrc_sentiment(clean_text)

# Sumarizar las emociones
emotion_counts <- colSums(sentiments[, 1:8])

# Crear un dataframe para facilitar la visualizaci√≥n
emotion_df <- data.frame(emotion = names(emotion_counts), count = emotion_counts)

# Crear un gr√°fico de barras para las emociones
ggplot(emotion_df, aes(x = reorder(emotion, -count), y = count)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  labs(title = "Distribuci√≥n de Emociones en los Textos de PDFs",
       x = "Emoci√≥n", y = "Frecuencia") +
  theme_minimal()

# Sumarizar las polaridades
polarity_counts <- colSums(sentiments[, 9:10])

# Crear un dataframe para facilitar la visualizaci√≥n
polarity_df <- data.frame(polarity = names(polarity_counts), count = polarity_counts)

# Crear un gr√°fico de barras para las polaridades
ggplot(polarity_df, aes(x = reorder(polarity, -count), y = count)) +
  geom_bar(stat = "identity", fill = "lightgreen") +
  labs(title = "Distribuci√≥n de Polaridades en los Textos de PDFs",
       x = "Polaridad", y = "Frecuencia") +
  theme_minimal()

```


```{r}
# Cargar las bibliotecas necesarias
library(tidytext)
library(igraph)
library(ggraph)
library(pdftools)
library(tm)

# Listar todos los archivos PDF en el directorio de trabajo
pdf_files <- list.files(pattern = "pdf$")

# Funci√≥n para extraer texto de los PDFs
extract_text_from_pdf <- function(pdf_file) {
  text <- pdf_text(pdf_file)
  return(text)
}

# Aplicar la funci√≥n a cada archivo PDF para extraer texto
pdf_texts <- lapply(pdf_files, extract_text_from_pdf)

# Convertir el texto de los PDFs en un solo vector de texto
all_text <- unlist(pdf_texts)

# Preprocesar el texto
clean_text <- all_text %>%
  tolower() %>%
  removePunctuation() %>%
  removeNumbers() %>%
  removeWords(stopwords("en")) %>%
  stripWhitespace()

# Convertir el texto limpio en un data frame para tidytext
cleaned_text_df <- data.frame(text = clean_text, stringsAsFactors = FALSE)

# Tokenizar el texto en bigramas
bigramas <- cleaned_text_df %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

# Separar los bigramas en dos columnas
bigram_separated <- bigramas %>%
  separate(bigram, into = c("word1", "word2"), sep = " ")

# Contar la frecuencia de los bigramas
bigram_freq <- bigram_separated %>%
  count(word1, word2, sort = TRUE) %>%
  filter(n >= 60) # Frecuencia m√≠nima de bigramas

# Crear el gr√°fico de red
set.seed(175)
graph <- graph_from_data_frame(bigram_freq, directed = FALSE)

# Visualizar el gr√°fico de red
ggraph(graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "darkgrey") +
  geom_node_point(color = "skyblue", size = 5) +
  geom_node_text(aes(label = name), repel = TRUE, size = 5, color = "black") +
  theme_void() +
  labs(title = "Red de Bigramas en los Textos de PDFs",
       subtitle = "Bigramas con una frecuencia mayor o igual a 40",
       caption = "Fuente: Textos de los PDFs adjuntados") +
  theme(plot.title = element_text(size = 14, face = "bold"),
        plot.subtitle = element_text(size = 10),
        plot.caption = element_text(size = 8))
```

---
title: "quora_total"
author: "Jorge C√©spedes Rico"
date: "2024-06-03"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, warning = FALSE}
# Load the necessary libraries
library(pdftools)
library(tm)
library(wordcloud)
library(RColorBrewer)
library(syuzhet)
library(ggplot2)
library(topicmodels)
library(textstem)
library(dplyr)
library(tidyr)
library(tidytext)
library(lubridate)
library(igraph)
library(ggraph)
library(cluster)
library(factoextra)
library(Rtsne)
```

```{r}
# List all PDF files in the working directory
files <- list.files(pattern = "pdf$")
files
```

```{r}


theme_elegante <- function(base_size = 10,
                           base_family = "Raleway"
                           )
    {
    color.background = "#FFFFFF" # Chart Background
    color.grid.major = "#D9D9D9" # Chart Gridlines
    color.axis.text = "#666666" # 
    color.axis.title = "#666666" # 
    color.title = "#666666"
    color.subtitle = "#666666"
    strip.background.color = '#9999CC'
    
    ret <-
        theme_bw(base_size=base_size) +
        
        # Set the entire chart region to a light gray color
        theme(panel.background=element_rect(fill=color.background, color=color.background)) +
        theme(plot.background=element_rect(fill=color.background, color=color.background)) +
        theme(panel.border=element_rect(color=color.background)) +
        
        # Format the grid
        theme(panel.grid.major=element_line(color=color.grid.major,size=.55, linetype="dotted")) +
        theme(panel.grid.minor=element_line(color=color.grid.major,size=.55, linetype="dotted")) +
        theme(axis.ticks=element_blank()) +
        
        # Format the legend, but hide by default
        theme(legend.position="none") +
        theme(legend.background = element_rect(fill=color.background)) +
        theme(legend.text = element_text(size=base_size-3,color=color.axis.title, family = base_family)) +
        
        theme(strip.text.x = element_text(size=base_size,color=color.background, family = base_family)) +
        theme(strip.text.y = element_text(size=base_size,color=color.background, family = base_family)) +
        #theme(strip.background = element_rect(fill=strip.background.color, linetype="blank")) +
        theme(strip.background = element_rect(fill = "grey70", colour = NA)) +
        # theme(panel.border= element_rect(fill = NA, colour = "grey70", size = rel(1)))+
        # Set title and axis labels, and format these and tick marks
        theme(plot.title=element_text(color=color.title, 
                                      size=20, 
                                      vjust=1.25, 
                                      family=base_family, 
                                      hjust = 0.5
                                      )) +
        
        theme(plot.subtitle=element_text(color=color.subtitle, size=base_size+2, family = base_family,  hjust = 0.5))  +
        
        theme(axis.text.x=element_text(size=base_size,color=color.axis.text, family = base_family)) +
        theme(axis.text.y=element_text(size=base_size,color=color.axis.text, family = base_family)) +
        theme(text=element_text(size=base_size, color=color.axis.text, family = base_family)) +
        
        theme(axis.title.x=element_text(size=base_size+2,color=color.axis.title, vjust=0, family = base_family)) +
        theme(axis.title.y=element_text(size=base_size+2,color=color.axis.title, vjust=1.25, family = base_family)) +
        theme(plot.caption=element_text(size=base_size-2,color=color.axis.title, vjust=1.25, family = base_family)) +
        
        # Legend  
        theme(legend.text=element_text(size=base_size,color=color.axis.text, family = base_family)) +
        theme(legend.title=element_text(size=base_size,color=color.axis.text, family = base_family)) +
        theme(legend.key=element_rect(colour = color.background, fill = color.background)) +
        theme(legend.position="bottom", 
              legend.box = "horizontal", 
              legend.title = element_blank(),
              legend.key.width = unit(.75, "cm"),
              legend.key.height = unit(.75, "cm"),
              legend.spacing.x = unit(.25, 'cm'),
              legend.spacing.y = unit(.25, 'cm'),
              legend.margin = margin(t=0, r=0, b=0, l=0, unit="cm")) +

        # Plot margins
        theme(plot.margin = unit(c(.5, .5, .5, .5), "cm"))
    
    ret
}
```


```{r}
# Apply the pdf_text function to each PDF file
quora <- lapply(files, pdf_text)
```

```{r}
# Combine all texts into a single character vector
quora_text <- unlist(quora)
```

```{r}
# Create a text corpus from the extracted text
corpus <- Corpus(VectorSource(quora_text))
```

```{r}
# Define additional stopwords
additional_stopwords <- c("upvote", "upvotes", "view", "views", "y", "dfollow", "answer", "answers", "‚Äô", "‚Ä¶", "‚Äì", "‚Äú", "‚Äù", "‚Ä¶")
```

```{r}
# Clean the text data
clean_corpus <- corpus %>%
  tm_map(content_transformer(tolower)) %>%
  tm_map(removePunctuation) %>%
  tm_map(removeNumbers) %>%
  tm_map(removeWords, c(stopwords("english"), additional_stopwords)) %>%
  tm_map(stripWhitespace) %>%
  tm_map(content_transformer(lemmatize_strings))

```


```{r}
# Create a Document-Term Matrix (DTM)
dtm <- DocumentTermMatrix(clean_corpus)
```

```{r}
BigramTokenizer <- function(x) {
  unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
}

# Create a DTM for bigrams
dtm_bigrams <- DocumentTermMatrix(clean_corpus, control = list(tokenize = BigramTokenizer))

```
```{r}
# Calculate Term Frequency-Inverse Document Frequency (TF-IDF)
tfidf <- weightTfIdf(dtm)
tfidf_matrix <- as.matrix(tfidf)
```

```{r}
# Supongamos que `tfidf_matrix` es tu matriz TF-IDF

# Calcular la suma de TF-IDF para cada t√©rmino
top_terms <- sort(colSums(tfidf_matrix), decreasing = TRUE)

# Convertir a un data frame
top_terms_df <- data.frame(term = names(top_terms), tfidf = top_terms)

# Filtrar los t√©rminos espec√≠ficos y aquellos que contienen "..." o "'" o "‚Äô"
top_terms_df <- top_terms_df %>%
  filter(!term %in% c("httpswwwquoracomunansweredhowdoiprotectemployeesandbusinessesincovid", 
                      "httpswwwquoracomwhataresomementalhealthtipsduringcovid")) %>%
  filter(!grepl("\\.\\.\\.", term)) %>%  # Filtrar t√©rminos que contienen "..."
  filter(!grepl("'", term)) %>%  # Filtrar t√©rminos que contienen "'"
  filter(!grepl("‚Äô", term)) %>%  # Filtrar t√©rminos que contienen "‚Äô"
  arrange(desc(tfidf)) %>%
  head(18)

# Imprimir el data frame resultante
print(top_terms_df)

```

```{r}
# Find the top terms by TF-IDF
top_terms <- sort(colSums(tfidf_matrix), decreasing = TRUE)
print(head(top_terms, 15))

# Convertir los top_terms en un data frame
top_terms_df <- data.frame(term = names(top_terms), tfidf = top_terms)
# Filtrar los t√©rminos espec√≠ficos y aquellos que contienen "..." o "'" o "‚Äô"
top_terms_df <- top_terms_df %>%
  filter(!term %in% c("‚Åé", "‚àí", "‚Ä¶", "yes", "std", "unstd","‚Äô", "swb", "pwb", "almps", "‚Äì", "*","httpswwwquoracomwhatsthebestwaytoincreaseproductivity", "httpswwwquoracomunansweredhowdoiprotectemployeesandbusinessesincovid","httpswwwquoracomwhataresomementalhealthtipsduringcovid", "ùñ•", "***", "‚àó‚àó‚àó")) %>%
  filter(!grepl("\\.\\.\\.", term)) %>%  # Filtrar t√©rminos que contienen "..."
  filter(!grepl("'", term)) %>%  # Filtrar t√©rminos que contienen "'"
  filter(!grepl("‚Äô", term)) %>%  # Filtrar t√©rminos que contienen "‚Äô"
  arrange(desc(tfidf)) %>%
  head(15)

# Ordenar los t√©rminos por TF-IDF
top_terms_df <- top_terms_df %>% arrange(desc(tfidf))

# Seleccionar los 10 primeros t√©rminos
top_terms_df <- head(top_terms_df, 15)

# Cargar la biblioteca ggplot2
library(ggplot2)

# Graficar los principales t√©rminos por TF-IDF con una paleta de colores diferente
ggplot(top_terms_df, aes(x = reorder(term, tfidf), y = tfidf, fill = term)) +
  geom_bar(stat = "identity", color = "black")  +  # Usar la paleta de colores viridis
  labs(title = "Top 15 t√©rminos: hilos de Quora", x = "T√©rmino", y = "TF-IDF") +
  theme_elegante() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1), 
        legend.position = "none",
        axis.title.x = element_text(face = "bold"),  # T√≠tulo del eje x en negrita
        axis.title.y = element_text(face = "bold"))  # T√≠tulo del eje y en negrita
top_terms_df
```


```{r}

library(ggwordcloud)
library(viridis)
# Combine all cleaned text into a single data frame
cleaned_text <- data.frame(text = sapply(clean_corpus, as.character), stringsAsFactors = FALSE)

# Remove unwanted characters (".", "...", "'", "‚Äô", "s", "¬∑", "‚Äú‚Äù, etc.)
cleaned_text$text <- gsub("[\\.]{2,}", "", cleaned_text$text)  # Remove multiple dots
cleaned_text$text <- gsub("'", "", cleaned_text$text)
cleaned_text$text <- gsub("‚Äô", "", cleaned_text$text)
cleaned_text$text <- gsub("¬∑", "", cleaned_text$text)
cleaned_text$text <- gsub("and|the|of|j‚Äô‚Äô|‚Äú|‚Äù", "", cleaned_text$text)  # Remove specific words and characters
cleaned_text$text <- gsub("[[:punct:]]", "", cleaned_text$text)  # Remove punctuation
cleaned_text$text <- gsub("\\s+", " ", cleaned_text$text)  # Remove extra whitespace

# Tokenize the text
words <- unlist(strsplit(cleaned_text$text, " "))

# Remove empty strings and unwanted short words from the tokenized words
unwanted_words <- c("p", "o", "ob", "s", "j", "t", "b", "de", "c", "e", "	ùêä", "quora", "don", "k", "re", "etc")
words <- words[words != "" & !words %in% unwanted_words]

# Create word frequency table
word_freq <- table(words)
word_freq_df <- as.data.frame(word_freq)
colnames(word_freq_df) <- c("word", "freq")

# Filter words with a minimum frequency
word_freq_df <- word_freq_df[word_freq_df$freq >= 400,]

# Plot word cloud using ggwordcloud
ggplot(word_freq_df, aes(label = word, size = freq, color = freq)) +
  geom_text_wordcloud_area(rm_outside = TRUE) +  # Ensure words don't overlap and stay within bounds
  scale_size_area(max_size = 30) +
  scale_color_viridis_c() +  # Set color scheme for the text
  theme_elegante() +
  labs(title = "Hilos de Quora", 
       subtitle = "M√≠nima frecuencia: 400") +
  theme(
    plot.title = element_text(size = 20, hjust = 0.5, vjust = 1, face = "bold"),  # Centered title at the top
    plot.subtitle = element_text(size = 12, hjust = 0.5, color = "darkgrey"),  # Centered subtitle below the title
    plot.margin = margin(10, 10, 10, 10)
  )

```
```{r}

# Combine all cleaned text into a single data frame
cleaned_text <- data.frame(text = sapply(clean_corpus, as.character), stringsAsFactors = FALSE)

# Tokenize the text into bigrams
bigrams <- cleaned_text %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

# Count the frequency of each bigram
bigram_counts <- bigrams %>%
  count(bigram, sort = TRUE)

# Filter to remove unwanted bigrams
unwanted_bigrams <- c("don t", "originally answer", "follow former", "can t", "health quora", "s mental health", "don t know", "try quora add", "quora add question", "arent allow write", "allow write question")
filtered_bigrams <- bigram_counts %>%
  filter(!bigram %in% unwanted_bigrams)

# Filter to keep only the top 20 bigrams
top_bigrams <- filtered_bigrams %>%
  top_n(20, wt = n)

# Visualize the top 20 bigrams without legend
ggplot(top_bigrams, aes(x = reorder(bigram, n), y = n, fill = reorder(bigram, n))) +
  geom_col() +
  coord_flip() +
  labs(title = "Top 20 bigramas: Quora", x = "Bigrama", y = "Frecuencia") +
  theme_elegante() +                      
  theme(axis.text.x = element_text(angle = 45, hjust = 1), 
        axis.title.x = element_text(face = "bold"),  
        axis.title.y = element_text(face = "bold"),  
        legend.position = "none")

# Tokenize the text into trigrams
trigrams <- cleaned_text %>%
  unnest_tokens(trigram, text, token = "ngrams", n = 3)

# Count the frequency of each trigram
trigram_counts <- trigrams %>%
  count(trigram, sort = TRUE)

# Filter to remove unwanted trigrams
unwanted_trigrams <- c("k share", "s mental health", "don t know", "write arent allow", "try quora add", "quora add question", "arent allow write", "allow write question")
filtered_trigrams <- trigram_counts %>%
  filter(!trigram %in% unwanted_trigrams)

# Filter to keep only the top 20 trigrams
top_trigrams <- filtered_trigrams %>%
  top_n(20, wt = n)

# Visualize the top 20 trigrams without legend
ggplot(top_trigrams, aes(x = reorder(trigram, n), y = n, fill = reorder(trigram, n))) +
  geom_col() +
  coord_flip() +
  labs(title = "Top 20 trigramas: Quora", x = "Trigrama", y = "Frecuencia") +
  theme_elegante() +                      
  theme(axis.text.x = element_text(angle = 45, hjust = 1), 
        axis.title.x = element_text(face = "bold"),  
        axis.title.y = element_text(face = "bold"),  
        legend.position = "none")
```


```{r}
# Define a list of unwanted words
unwanted_words <- c("di", "yr", "doi", "org", "o", "p", "https", "res", 
                    "claimant count", "std", "unstd", "be", "garciag√≥mez", 
                    "int", "macmillan", "ppi", "yes", "claimant", "count", "iec", "fjmd", "t", "pa", "creed", "univ", "connecticut", "garc√≠ag√≥mez", "sjpsagepubcom", "tue", "sep", "ii", "download", "iza", "e", "ects", "jstor", "ltd", "ùêä", "q", "de", "witte", "copyright", "john", "wiley", "httpsdoiorgjournalpone", "sarti", "zella", "s", "j", "epidemiol", "jung", "dw", "kwak", "kk", "vol", "pp", "plos", "bmc", "quora", "api", "scraper", "im", "album", "yupoo", "k", "cicd", "ci", "tool", "idenative", "pipeline", "ad", "free", "mo", "k", "arent", "allow", "request", "sponsor")

# Tokenize the text into bigrams
bigrams <- cleaned_text %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

# Separate the bigrams into two columns
bigrams_separated <- bigrams %>%
  separate(bigram, into = c("word1", "word2"), sep = " ")

# Filter out unwanted words
bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% unwanted_words & !word2 %in% unwanted_words)

# Count the frequency of each bigram
bigram_counts <- bigrams_filtered %>%
  count(word1, word2, sort = TRUE)

# Filter to keep only the top 60 bigrams for visualization
top_bigrams <- bigram_counts %>%
  top_n(60, wt = n)

# Create a graph object using igraph
bigram_graph <- graph_from_data_frame(top_bigrams)

# Plot the bigram network using ggraph
set.seed(1234)
ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), edge_width = 0.6, edge_colour = "red3",  # Change color to "darkgray"
                 arrow = arrow(type = "closed", length = unit(0.185, "inches"))) +
  geom_node_point(size = 5, color = "darkolivegreen3") +
  geom_node_text(aes(label = name), repel = TRUE, size = 3.25) +
  theme_void() +
  labs(title = "Quora", 
       subtitle = "Top 60")

# Tokenize the text into trigrams
trigrams <- cleaned_text %>%
  unnest_tokens(trigram, text, token = "ngrams", n = 3)

# Separate the trigrams into three columns
trigrams_separated <- trigrams %>%
  separate(trigram, into = c("word1", "word2", "word3"), sep = " ")

# Filter out unwanted words
trigrams_filtered <- trigrams_separated %>%
  filter(!word1 %in% unwanted_words & !word2 %in% unwanted_words & !word3 %in% unwanted_words)

# Count the frequency of each trigram
trigram_counts <- trigrams_filtered %>%
  count(word1, word2, word3, sort = TRUE)

# Filter to keep only the top 60 trigrams for visualization
top_trigrams <- trigram_counts %>%
  top_n(60, wt = n)

# Create a graph object using igraph
trigram_graph <- graph_from_data_frame(top_trigrams, directed = TRUE)

# Plot the trigram network using ggraph with kk layout
set.seed(1234)
ggraph(trigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), edge_width = 0.6, 
                 arrow = arrow(type = "closed", length = unit(0.185, "inches")), 
                 edge_colour = "sienna") +  # Change color to "darkgray"
  geom_node_point(size = 5, color = "steelblue3") +
  geom_node_text(aes(label = name), repel = TRUE, size = 3.25) +
  theme_void() +
  labs(title = "Quora", 
       subtitle = "Top 60")


```


```{r}
# Funci√≥n para crear matriz de concurrencia
create_cooccurrence_matrix <- function(dtm) {
  dtm_matrix <- as.matrix(dtm)
  term_matrix <- t(dtm_matrix) %*% dtm_matrix
  term_matrix[lower.tri(term_matrix, diag = TRUE)] <- 0
  term_matrix
}

# Crear la matriz de concurrencia
cooccurrence_matrix <- create_cooccurrence_matrix(dtm)

# Convertir la matriz de concurrencia en un dataframe
cooccurrence_df <- as.data.frame(as.table(cooccurrence_matrix))
colnames(cooccurrence_df) <- c("term1", "term2", "frequency")

# Filtrar t√©rminos con concurrencia baja y eliminar caracteres no deseados
cooccurrence_df <- cooccurrence_df %>%
  filter(frequency > 7000) %>%
  filter(!term1 %in% c("‚Äì", "‚Äô", "‚Äò", "‚àí", "‚Ä¶", "‚Åé", "‚Äú", "‚Äù", "ùñ•", "‚àó‚àó‚àó", "‚àó‚àó", "‚â•") & 
         !term2 %in% c("‚Äì", "‚Äô", "‚Äò", "‚àí","‚Ä¶", "‚Åé", "‚Äú", "‚Äù", "ùñ•", "‚àó‚àó‚àó", "‚àó‚àó", "‚â•"))

# Crear un grafo a partir de la matriz de concurrencia
cooccurrence_graph <- graph_from_data_frame(cooccurrence_df, directed = FALSE)

# Aumentar la transparencia de las l√≠neas y ajustar el tama√±o del grafo
set.seed(175)  # Asegura la reproducibilidad
ggraph(cooccurrence_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = frequency, edge_width = frequency), color = "pink3", show.legend = TRUE) +
  geom_node_point(color = "springgreen3", size = 4) +
  geom_node_text(aes(label = name), repel = TRUE, size = 3.25) +  # Usar repel para mejorar la distribuci√≥n de etiquetas
  scale_edge_width_continuous(name = "n") +
  scale_edge_alpha_continuous(name = "n") +
  theme_void() +
  labs(title = "Quora", 
       subtitle = "Frecuencia > 7000") +
  theme(plot.title = element_text(size = 12, face = "bold"),
        plot.subtitle = element_text(size = 10),
        plot.caption = element_text(size = 8, hjust = 1))

```


```{r}
# Sentiment Analysis
sentiments <- get_nrc_sentiment(quora_text)
sentiment_totals <- colSums(sentiments)
sentiment_totals_df <- data.frame(Sentiment = names(sentiment_totals), Total = sentiment_totals)

```

```{r}
ggplot(sentiment_totals_df, aes(x = reorder(Sentiment, -Total), y = Total, fill = Sentiment)) +
  geom_bar(stat = "identity") +
  labs(title = "Sentimientos: Quora", x = "Sentimiento", y = "Total") +
  theme_elegante() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = "none",
        axis.title.x = element_text(face = "bold"),
        axis.title.y = element_text(face = "bold")) +
  scale_x_discrete(labels = c("positive" = "positivo", "negative" = "negativo", "trust" = "confianza",
                              "sadness" = "tristeza", "fear" = "miedo", "anger" = "enfado",
                              "anticipation" = "anticipaci√≥n", "disgust" = "disgusto",
                              "surprise" = "sorpresa", "joy" = "alegr√≠a"))
```

```{r}
# Sentiment Analysis
sentiments_quora <- get_nrc_sentiment(quora_text)
sentiment_totals_quora <- colSums(sentiments_quora)
sentiment_totals_df_quora <- data.frame(Sentiment = names(sentiment_totals_quora), Total = sentiment_totals_quora)

# Calcular los porcentajes de cada sentimiento
sentiment_totals_df_quora$Porcentaje <- (sentiment_totals_df_quora$Total / sum(sentiment_totals_df_quora$Total)) * 100

# Graficar los sentimientos con porcentajes
ggplot(sentiment_totals_df_quora, aes(x = reorder(Sentiment, -Total), y = Total, fill = Sentiment)) +
  geom_bar(stat = "identity") +
  labs(title = "Sentimientos: Quora", x = "Sentimiento", y = "Total") +
  theme_elegante() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = "none",
        axis.title.x = element_text(face = "bold"),
        axis.title.y = element_text(face = "bold")) +
  scale_x_discrete(labels = c("positive" = "positivo", "negative" = "negativo", "trust" = "confianza",
                              "sadness" = "tristeza", "fear" = "miedo", "anger" = "enfado",
                              "anticipation" = "anticipaci√≥n", "disgust" = "disgusto",
                              "surprise" = "sorpresa", "joy" = "alegr√≠a")) +
  geom_text(aes(label = sprintf("%.1f%%", Porcentaje)), vjust = -0.5, size = 3)
```


```{r}
library(tm)
library(wordcloud)
library(RColorBrewer)
library(textstem)
# Convertir los datos de emociones en una sola cadena de texto
cloud_emotions_data <- c(
  paste(quora_text[sentiments$sadness > 0], collapse = " "),
  paste(quora_text[sentiments$joy > 0], collapse = " "),
  paste(quora_text[sentiments$anger > 0], collapse = " "),
  paste(quora_text[sentiments$fear > 0], collapse = " "),
  paste(quora_text[sentiments$trust > 0], collapse = " "),
  paste(quora_text[sentiments$anticipation > 0], collapse = " "),
  paste(quora_text[sentiments$disgust > 0], collapse = " "),
  paste(quora_text[sentiments$surprise > 0], collapse = " ")
)

# Crear un corpus de texto
cloud_corpus <- Corpus(VectorSource(cloud_emotions_data))

# Limpiar los datos de texto
cloud_corpus <- cloud_corpus %>%
  tm_map(content_transformer(tolower)) %>%
  tm_map(removePunctuation) %>%
  tm_map(removeNumbers) %>%
  tm_map(removeWords, c(stopwords("english"))) %>%
  tm_map(stripWhitespace) %>%
  tm_map(content_transformer(lemmatize_strings))

# Crear una matriz de t√©rminos por documento
cloud_tdm <- TermDocumentMatrix(cloud_corpus)
cloud_tdm <- as.matrix(cloud_tdm)

# Verificar cu√°ntas emociones tienen palabras asociadas
num_emotions <- ncol(cloud_tdm)
emotion_labels <- c('tristeza', 'alegr√≠a', 'enfado', 'miedo', 'confianza', 'anticipaci√≥n', 'disgusto', 'sorpresa')
emotion_labels <- emotion_labels[1:num_emotions]

# Asignar nombres de columnas a la matriz de t√©rminos
colnames(cloud_tdm) <- emotion_labels

# Generar la nube de palabras
set.seed(757)  # Se puede establecer cualquier entero
comparison.cloud(cloud_tdm, random.order = FALSE,
                 colors = brewer.pal(8, "Dark2"),
                 title.size = 1.2, max.words = 1000, scale = c(4, 0.5), rot.per = 0.4)

```



```{r}
library(reshape2)
# Convertir el corpus a un dataframe
comments_df <- data.frame(text = cleaned_text)

# Tokenizar los comentarios
comments_tokens <- comments_df %>%
  unnest_tokens(word, text)

# Unir los comentarios con el sentimiento de las palabras
comments_sentiment <- comments_tokens %>%
  inner_join(get_sentiments("bing"))

# Contar las palabras por sentimiento
word_sentiment_count <- comments_sentiment %>%
  count(word, sentiment, sort = TRUE)

# Convertir los datos a un formato adecuado para comparison.cloud
word_sentiment_matrix <- word_sentiment_count %>%
  acast(word ~ sentiment, value.var = "n", fill = 0)

# Crear la nube de palabras
comparison.cloud(word_sentiment_matrix,
                  colors = c("#F8766D", "#00BFC4"),
                  max.words = 100) 

```


```{r}
# Eliminar t√©rminos escasos para reducir el ruido y la dimensionalidad
dtm_sparse <- removeSparseTerms(dtm, 0.99)
dtm_sparse_matrix <- as.matrix(dtm_sparse)

# Determinar el n√∫mero √≥ptimo de clusters usando el m√©todo del codo
fviz_nbclust(dtm_sparse_matrix, kmeans, method = "wss")

# Aplicar k-means clustering con un n√∫mero √≥ptimo de clusters (por ejemplo, k = 3)
set.seed(1234)
num_clusters <- 3
kmeans_result <- kmeans(dtm_sparse_matrix, centers = num_clusters, nstart = 25)

# Verificar la cantidad de documentos y clusters
print(length(quora_text))  # Verifica la cantidad de documentos
print(length(kmeans_result$cluster))  # Verifica la cantidad de clusters

# A√±adir las asignaciones de clusters a los datos originales
quora_clusters <- data.frame(text = quora_text, cluster = kmeans_result$cluster)

# Imprimir el resultado del clustering
print(quora_clusters)

# Visualizar el resultado del clustering usando PCA
pca <- prcomp(dtm_sparse_matrix, scale. = TRUE)
pca_data <- data.frame(pca$x, cluster = as.factor(kmeans_result$cluster))

ggplot(pca_data, aes(x = PC1, y = PC2, color = cluster)) +
  geom_point() +
  labs(title = "PCA of Document Clusters", x = "Principal Component 1", y = "Principal Component 2") +
  theme_minimal()

# Visualizar el resultado del clustering usando t-SNE
# Eliminar duplicados antes de aplicar t-SNE
dtm_sparse_matrix_unique <- unique(dtm_sparse_matrix)

set.seed(1234)
tsne_result <- Rtsne(dtm_sparse_matrix_unique, dims = 2, perplexity = 5, verbose = TRUE, max_iter = 500)
tsne_data <- data.frame(tsne_result$Y, cluster = as.factor(kmeans_result$cluster[match(rownames(dtm_sparse_matrix_unique), rownames(dtm_sparse_matrix))]))
colnames(tsne_data) <- c("Dim1", "Dim2", "cluster")

ggplot(tsne_data, aes(x = Dim1, y = Dim2, color = cluster)) +
  geom_point() +
  labs(title = "t-SNE of Document Clusters", x = "Dimension 1", y = "Dimension 2") +
  theme_minimal()


```


```{r}
clean_corpus <- clean_corpus[which(rowSums(as.matrix(dtm)) != 0)]

# Crear la matriz TF-IDF sin documentos vac√≠os
text_dtm_tfidf <- DocumentTermMatrix(clean_corpus, control = list(weighting = weightTfIdf))
tf_idf_mat <- as.matrix(text_dtm_tfidf)

# Eliminar t√©rminos con todos los valores en cero
tf_idf_mat <- tf_idf_mat[, colSums(tf_idf_mat) != 0]

# Calcular la distancia del coseno
d1 <- proxy::dist(tf_idf_mat, method = "cosine")

# Verificar si la matriz de distancias contiene valores NA, NaN o Inf
if (any(is.na(d1)) || any(is.infinite(d1))) {
  stop("La matriz de distancias contiene valores NA o Inf.")
}

library(dendextend)

# Realizar el clustering jer√°rquico
cluster2 <- hclust(d1, method = "ward.D2")

# Convertir el objeto hclust en un dendrograma
dend <- as.dendrogram(cluster2)

# Asignar etiquetas a las hojas del dendrograma
labels(dend) <- files

# Colorear los clusters
dend <- dend %>%
  color_branches(k = 13) %>%
  set("labels_colors", k = 13) %>%
  set("labels_cex", 0.7) %>%
  set("branches_lwd", 2)

# Visualizar el dendrograma con nombres de archivos
plot(dend, main = "Dendrograma de Clustering Jer√°rquico de PDFs", ylab = "Altura", xlab = "", cex.lab = 0.75, las = 2)


# Crear la matriz DTM inicial
dtm <- DocumentTermMatrix(clean_corpus)

# Eliminar documentos vac√≠os del DTM y del corpus
non_empty_docs <- which(rowSums(as.matrix(dtm)) != 0)
dtm <- dtm[non_empty_docs, ]
clean_corpus <- clean_corpus[non_empty_docs]
files <- files[non_empty_docs]

# Crear la matriz TF-IDF
text_dtm_tfidf <- DocumentTermMatrix(clean_corpus, control = list(weighting = weightTfIdf))
tf_idf_mat <- as.matrix(text_dtm_tfidf)

# Eliminar t√©rminos con todos los valores en cero
tf_idf_mat <- tf_idf_mat[, colSums(tf_idf_mat) != 0]

# Calcular la distancia del coseno
d1 <- proxy::dist(tf_idf_mat, method = "cosine")

# Realizar el clustering jer√°rquico
cluster2 <- hclust(d1, method = "ward.D2")

# Convertir el objeto hclust en un dendrograma
dend <- as.dendrogram(cluster2)

# Asignar etiquetas a las hojas del dendrograma
labels(dend) <- files

# Asignar clusters a cada archivo
clusters <- cutree(cluster2, k = 13)
file_clusters <- data.frame(File = files, Cluster = clusters)

# Mostrar la tabla
print(file_clusters)

# Calcular la silhouette
sil <- silhouette(cutree(cluster2, k = 13), d1)

# Visualizar la silueta de los clusters
fviz_silhouette(sil)

# Visualizar los clusters
fviz_cluster(list(data = tf_idf_mat, cluster = cutree(cluster2, k = 13)), geom = "point", show.clust.cent = TRUE, ellipse.type = "convex", main = "Clustering de PDFs")

# Visualizar la matriz de distancias
fviz_dist(as.dist(d1), gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07")) + 
  labs(title = "Matriz de Distancias del Coseno de PDFs")

# Uso de UDPipe para anotaci√≥n
ud_model <- udpipe_download_model(language = "english")
ud_model <- udpipe_load_model(ud_model$file_model)
data_tm <- data.frame(doc_id = files, text = sapply(ingles, paste, collapse = " "))

anno <- udpipe(data_tm, object = ud_model, trace = 1000)
biterms <- as.data.table(anno)
biterms <- biterms[, cooccurrence(x = lemma, relevant = upos %in% c("NOUN", "ADJ", "VERB") & nchar(lemma) > 2 & !lemma %in% stopwords("es"), skipgram = 3), by = list(doc_id)]

set.seed(999)
traindata <- subset(anno, upos %in% c("NOUN", "ADJ", "VERB") & !lemma %in% stopwords("es") & nchar(lemma) > 2)
traindata <- traindata[, c("doc_id", "lemma")]

# Ajustar el modelo BTM
model <- BTM(traindata, biterms = biterms, k = 10, iter = 2000, background = FALSE, trace = 2000)

# Extraer biterms para plotear
biterms1 = terms(model, type = "biterms")$biterms

plot(model, subtitle = "Distribuci√≥n de T√≥picos en ingl√©s", biterms = biterms1, labels = paste(round(model$theta * 100, 2), "%", sep = ""), top_n = 20, topic.size = 3, label.size = 2, main.size = 2) +
  theme_void() +
  theme(legend.position = "right", legend.key.size = unit(0.5, "lines"))
```

# WORD EMBEDDINGS 
```{r}
# Cargar las bibliotecas necesarias
library(dplyr)
library(ggplot2)
library(udpipe)
library(textrank)
library(tidytext)
library(lubridate)

# Procesamiento del texto con UDPipe
ud_model <- udpipe_download_model(language = "english")
udpipe_model <- udpipe_load_model(ud_model$file_model)

# Anotaci√≥n del texto
anot <- udpipe_annotate(udpipe_model, x = clean_corpus$content)
anot <- as.data.frame(anot)

# Filtrar sustantivos
terminos_n <- subset(anot, upos == "NOUN")

# Calcular la frecuencia de los lemas de sustantivos
terminos_n <- txt_freq(terminos_n$lemma)

# Convertir los t√©rminos a factores con niveles ordenados
terminos_n$key <- factor(terminos_n$key, levels = rev(terminos_n$key))

# Graficar los sustantivos m√°s frecuentes
ggplot(head(terminos_n, 20), aes(x = key, y = freq)) +
  geom_bar(stat = "identity", fill = "cadetblue") +
  coord_flip() +
  labs(title = "Sustantivos m√°s frecuentes", x = "Frecuencia", y = "Sustantivos") +
  theme_minimal()

```


```{r}
# Filtrar nombres propios
terminos_np <- subset(anot, upos == "PROPN")

# Calcular la frecuencia de los lemas de nombres propios
terminos_np <- txt_freq(terminos_np$lemma)

# Convertir los t√©rminos a factores con niveles ordenados
terminos_np$key <- factor(terminos_np$key, levels = rev(terminos_np$key))

# Graficar los nombres propios m√°s frecuentes
grafico_nombres_propios <- ggplot(head(terminos_np, 20), aes(x = key, y = freq)) +
  geom_bar(stat = "identity", fill = "coral") +
  coord_flip() +
  labs(title = "Nombres propios m√°s frecuentes", x = "Frecuencia", y = "Nombres propios") +
  theme_minimal()
grafico_nombres_propios
```

```{r}
# Filtrar adjetivos
terminos_a <- subset(anot, upos == "ADJ")

# Calcular la frecuencia de los lemas de adjetivos
terminos_a <- txt_freq(terminos_a$lemma)

# Convertir los t√©rminos a factores con niveles ordenados
terminos_a$key <- factor(terminos_a$key, levels = rev(terminos_a$key))

# Graficar los adjetivos m√°s frecuentes
grafico_adjetivos <- ggplot(head(terminos_a, 20), aes(x = key, y = freq)) +
  geom_bar(stat = "identity", fill = "#C8A2C8") +
  coord_flip() +
  labs(title = "Adjetivos m√°s frecuentes", x = "Frecuencia", y = "Adjetivos") +
  theme_minimal()
grafico_adjetivos
```

```{r}
terminos_v <- subset(anot, upos == "VERB")
terminos_v <- txt_freq(terminos_v$lemma)
terminos_v$key <- factor(terminos_v$key, levels = rev(terminos_v$key))
# Graficar los verbos m√°s frecuentes
grafico_verbos <- ggplot(head(terminos_v, 20), aes(x = key, y = freq)) +
  geom_bar(stat = "identity", fill = "#DC143C") +
  coord_flip() +
  labs(title = "Verbos m√°s frecuentes", x = "Frecuencia", y = "Verbos") +
  theme_minimal()
grafico_verbos
```

```{r}
library(lattice)
kwrake <- keywords_rake(x = anot, term = "lemma", group = "doc_id", # Agrupamos por documento (tweet)
                    relevant = anot$upos %in% c("NOUN", "ADJ","PROPN")) # S√≥lo sustantivos, adjetivos y
                                                                        #  nombres propios
kwrake$key <- factor(kwrake$keyword, levels = rev(kwrake$keyword))

# Graficamos palabras clave que aparezcan al menos 5 veces
barchart(key ~ rake, data = head(subset(kwrake, freq > 4), 20), col = "cadetblue",
         main = list("Identificar palabras clave mediante RAKE", cex="1"), xlab = "RAKE")
```

```{r}
kwpmi <- keywords_collocation(x = anot, term = "lemma", group = "doc_id")
kwpmi$key <- factor(kwpmi$keyword, levels = rev(kwpmi$keyword))

barchart(key ~ pmi, data = head(subset(kwpmi, freq > 9), 20), col = "cadetblue",
         main = list("Identificar palabras clave mediante Colocaci√≥n PMI", cex="1"),
         xlab = "PMI (Pointwise Mutual Information)")
```

```{r}
anot$phrase_tag <- as_phrasemachine(anot$upos, type = "upos")

# Cambiar etiqueta a "pronombres" y "n√∫meros"
anot$phrase_tag[anot$upos=="PRON"] <- "O"
anot$phrase_tag[anot$upos=="NUM"] <- "O"

# Obtenemos las frases nominales simples (expresi√≥n regular)
kwphrases <- keywords_phrases(x = anot$phrase_tag, term = tolower(anot$token),
                              pattern = "(A|N)*N(P+D*(A|N)*N)*",
                              is_regex = TRUE, detailed = FALSE)

# Filtramos aquellas que contienen m√°s de una palabra y aparecen m√°s de 3 veces
kwphrases <- subset(kwphrases, ngram > 1 & freq > 3)
kwphrases$key <- factor(kwphrases$keyword, levels = rev(kwphrases$keyword))

barchart(key ~ freq, data = head(kwphrases, 20), col = "cadetblue",
         main = list("Palabras Clave - Frases nominales simples", cex="1"), xlab = "Frecuencia")
```

```{r}
keywords <- textrank_keywords(anot$lemma, relevant = anot$upos %in% c("NOUN", "ADJ","PROPN"),
                              ngram_max = 8, sep = " ") # S√≥lo sustantivos, adjetivos y nombres propios

# Filtramos aquellas que contienen m√°s de una palabra y aparecen m√°s de 3 veces
keywords <- subset(keywords$keywords, ngram > 1 & freq > 3) 
keywords$keyword <- factor(keywords$keyword, levels = rev(keywords$keyword))

# Nube de palabras clave m√°s frecuentes
wordcloud(words=keywords$keyword, freq=keywords$freq, random.order=F, colors=brewer.pal(8,"Dark2"))
```


```{r}
library(tidytext)

# Filtrar los t√©rminos por categor√≠as gramaticales espec√≠ficas
terminos <- subset(anot, upos %in% c("ADJ", "NOUN", "PROPN") & !lemma %in% c("don't"))

# Qu√© palabras coocurren por tweet
cooc <- cooccurrence(terminos, group = "doc_id", term = "lemma")

# Imprimir las primeras filas de los resultados
head(data.frame(cooc))
   
```



```{r}
wordnetwork <- graph_from_data_frame(head(cooc, 50))
ggraph(wordnetwork, layout = "fr") +
  geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = "pink") +
  geom_node_text(aes(label = name), col = "darkgreen", size = 3.5) +
  theme_graph(base_family = "sans") + theme(legend.position = "none") +
  labs(title = "Coocurrencias tweet/oraci√≥n", 
       subtitle = "Sustantivos, Adjetivos y Nombres Propios") +
  theme(plot.title = element_text(size = 14), plot.margin = margin(0,1,0,1))
```



```{r}
# Graficamos coocurrencias con skipgram = 1
wordnetwork <- graph_from_data_frame(head(cooc, 50))
ggraph(wordnetwork, layout = "fr") +
  geom_edge_link(aes(width = cooc, edge_alpha = cooc),edge_colour = "pink") +
  geom_node_text(aes(label = name), col = "darkgreen", size = 3.5) +
  theme_graph(base_family = "sans") +
  labs(title = "Qu√© palabras est√°n pr√≥ximas", subtitle = "Sustantivos, Adjetivos y Nombres Propios") +
  theme(plot.title = element_text(size = 14), plot.margin = margin(0,1,0,1))
```


```{r}
# Identificador √∫nico de cada oraci√≥n del corpus
anot$id <- unique_identifier(anot, fields = c("doc_id", "sentence_id"))

# Seleccionamos s√≥lo sustantivos, adjetivos y nombres propios
dtf <- subset(anot, upos %in% c("NOUN", "ADJ", "PROPN"))

# Creamos matriz documento-t√©rmino
dtf <- document_term_frequencies(dtf, document = "id", term = "lemma")
dtm <- document_term_matrix(dtf)

# Eliminamos t√©rminos que aparezcan menos de 10 veces
dtm <- dtm_remove_lowfreq(dtm, minfreq = 10)

# Eliminamos algunas palabras que no aportan nada al an√°lisis
dtm <- dtm_remove_terms(dtm, terms = c("don't"))
```

```{r}
termcorr <- dtm_cor(dtm)
dim(termcorr)
```
```{r}
library(formattable)
library(dplyr)

# Crear un ejemplo de termcorr
set.seed(123)
termcorr <- matrix(runif(100, min = -1, max = 1), ncol = 10)
colnames(termcorr) <- paste0("Term", 1:10)
rownames(termcorr) <- paste0("Term", 1:10)

# Redondear y convertir a data frame
df <- as.data.frame(round(termcorr[1:10, 1:5], 2))

# Verificar si los datos est√°n correctamente cargados
print(df)

# Aplicar formattable con estilo
formatted_df <- formattable(df, list(
  area(col = 1:5) ~ color_tile("white", "blue"),
  area(row = 1:10, col = 1:5) ~ formatter(
    "span", style = x ~ style(display = "block", padding = "0 4px"))
))

# Mostrar el resultado
formatted_df

```
```{r}
terminos <- subset(anot, upos %in% c("NOUN", "ADJ", "PROPN"), select = c("doc_id", "lemma"))
terminos <- split(terminos$lemma, terminos$doc_id)
terminos <- lapply(terminos, paste, collapse = " ")
docs <- do.call(rbind.data.frame, c(terminos, stringsAsFactors=FALSE))
colnames(docs) <- "texto"
docs$id <- as.numeric(str_sub(names(terminos),start = 4))
docs <- docs[order(docs$id), c("id", "texto")]
row.names(docs) <- NULL
rm(terminos)
```

```{r}
library(text2vec)
tokens <- space_tokenizer(docs$texto)
```

```{r}
# Crear iterador
it <- itoken(tokens, progressbar = FALSE)

# Crear el vocabulario
vocab <- create_vocabulary(it)
vocab <- vocab[order(vocab$term_count, decreasing = T),]
vocab$doc_porc <- vocab$doc_count/length(docs$texto)  # Porcentaje

# Filtramos los t√©rminos del vocabulario en aquellos que aparecen al menos 7 veces y que no aparezcan 
# en m√°s del 50% de los documentos (tweets)
vocab <- prune_vocabulary(vocab, term_count_min = 7, doc_proportion_max = 0.5)
```

```{r}
# Definir la funci√≥n de vectorizaci√≥n
vectorizer <- vocab_vectorizer(vocab)

# Construir la matriz de coocurrencia de t√©rminos
tcm <- create_tcm(it, vectorizer, skip_grams_window = 5)
```

```{r}
glove <- GlobalVectors$new(rank = 20, x_max = 10)
```

```{r}
wv_main <- glove$fit_transform(tcm, n_iter = 50, convergence_tol = 0.005)
```

```{r}
wv_context <- glove$components
dim(wv_context)
```
```{r}
word_vectors <- wv_main + t(wv_context)
```

```{r}
# vector("exposici√≥n")
wv <- word_vectors["ask", ,drop = FALSE]
# Palabras relacionadas con "exposici√≥n"
cos_sim <- sim2(x = word_vectors, y = wv, method = "cosine", norm = "l2")
head(sort(cos_sim[,1], decreasing = TRUE), 5)
```

```{r}
# vector("anunciaci√≥n")
wv <- word_vectors["nice", ,drop = FALSE]
# Palabras relacionadas con "anunciaci√≥n"
cos_sim <- sim2(x = word_vectors, y = wv, method = "cosine", norm = "l2")
head(sort(cos_sim[,1], decreasing = TRUE), 5)
```

```{r}
# vector("mujer")
wv <- word_vectors["money", ,drop = FALSE]
# Palabras relacionadas con "mujer"
cos_sim <- sim2(x = word_vectors, y = wv, method = "cosine", norm = "l2")
head(sort(cos_sim[,1], decreasing = TRUE), 5)
```

```{r}
library(Rtsne)
library(ggrepel)
library(ggplot2)

# Ejecutar t-SNE en los vectores de palabras
tsne <- Rtsne(word_vectors, dims = 2, perplexity = 20, max_iter = 2000)

# Crear un data frame para el gr√°fico
tsne_plot <- data.frame(x = tsne$Y[,1], y = tsne$Y[,2], palabra = rownames(word_vectors))

# Seleccionar un subconjunto aleatorio de palabras
set.seed(123)  # Para reproducibilidad
n_palabras <- 50  # N√∫mero de palabras a mostrar
palabras_mostrar <- sample(tsne_plot$palabra, n_palabras)

# Filtrar el data frame para incluir solo las palabras seleccionadas
tsne_plot_subset <- tsne_plot[tsne_plot$palabra %in% palabras_mostrar, ]

# Crear el gr√°fico t-SNE con un subconjunto de palabras
ggplot(tsne_plot_subset, aes(x, y)) +
  geom_point(size = 3, aes(color = palabra)) +  # A√±adir puntos con colores basados en las palabras
  geom_text_repel(aes(label = palabra), size = 3.5, box.padding = 0.3, max.overlaps = 10) +
  scale_color_viridis_d() +  # Utilizar una paleta de colores adecuada
  labs(title = "Modelo GloVe (t-SNE)", x = "Dimensi√≥n 1", y = "Dimensi√≥n 2") +
  theme_elegante() +
  theme(
    legend.position = "none",  # Quitar la leyenda si no es necesaria
    plot.title = element_text(hjust = 0.5),  # Centrar el t√≠tulo
    plot.background = element_rect(fill = "white"),  # Fondo blanco
    panel.grid.major = element_line(color = "gray85"),  # L√≠neas de la cuadr√≠cula
    panel.grid.minor = element_blank(),  # Ocultar l√≠neas de cuadr√≠cula menores
    panel.border = element_blank()  # Ocultar borde del panel
  )



```


```{r}
library(wordcloud2)
# Convertir el corpus a una matriz de t√©rminos de documentos (Term-Document Matrix)
tdm <- TermDocumentMatrix(clean_corpus)
matrix <- as.matrix (tdm)
words <- sort(rowSums(matrix), decreasing = TRUE)
df <- data.frame(word = names(words), freq = words)
wordcloud2(slice_max(df, order_by = freq, n = 200), size = 0.4, color = "random-dark")

```
```{r}
# Generar la nube de palabras con mejoras est√©ticas
library(RColorBrewer)
wordcloud2(slice_max(df, order_by = freq, n = 200), 
           size = 0.7,        # Tama√±o general de las palabras
           color = colors,    # Paleta de colores
           backgroundColor = "white",  # Fondo blanco
           fontWeight = "bold",        # Fuente en negrita
           shape = 'circle',           # Forma de la nube de palabras
           ellipticity = 0.65          # Escala de la nube
           )
```

```{r}
# Convertir el corpus a una matriz de t√©rminos de documentos (Term-Document Matrix)
tdm <- TermDocumentMatrix(clean_corpus)
matrix <- as.matrix(tdm)
words <- sort(rowSums(matrix), decreasing = TRUE)
df <- data.frame(word = names(words), freq = words)

# Definir la paleta de colores
colors <- brewer.pal(8, "Dark2")  # Puedes elegir una paleta diferente de RColorBrewer

# Generar la nube de palabras con mejoras est√©ticas
wordcloud2(slice_max(df, order_by = freq, n = 200), 
           size = 0.7,        # Tama√±o general de las palabras
           color = colors,    # Paleta de colores
           backgroundColor = "white",  # Fondo blanco
           fontWeight = "bold",        # Fuente en negrita
           shape = 'circle',           # Forma de la nube de palabras
           ellipticity = 0.65          # Escala de la nube
)
df
```
```{r}
# Suponiendo que clean_corpus ya est√° creado

# Convertir el corpus a una matriz de t√©rminos de documentos (Term-Document Matrix)
tdm <- TermDocumentMatrix(clean_corpus)
matrix <- as.matrix(tdm)
words <- sort(rowSums(matrix), decreasing = TRUE)
df <- data.frame(word = names(words), freq = words)

# Definir caracteres a eliminar
chars_to_remove <- c("‚Äô", "‚Ä¶", "‚Äì", "‚Äú", "‚Äù", "‚Ä¶")

# Filtrar el dataframe para eliminar los caracteres no deseados
df <- df[!df$word %in% chars_to_remove, ]

# Definir la paleta de colores
colors <- brewer.pal(8, "Dark2")  # Puedes elegir una paleta diferente de RColorBrewer

# Generar la nube de palabras con mejoras est√©ticas
wordcloud2(slice_max(df, order_by = freq, n = 200), 
           size = 0.7,        # Tama√±o general de las palabras
           color = colors,    # Paleta de colores
           backgroundColor = "white",  # Fondo blanco
           fontWeight = "bold",        # Fuente en negrita
           shape = 'circle',           # Forma de la nube de palabras
           ellipticity = 0.65          # Escala de la nube
)

```


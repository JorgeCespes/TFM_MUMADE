---
title: "desempleo_chatgpt"
author: "Jorge C√©spedes Rico"
date: "2024-05-21"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


#CARGA DE PAQUETES
```{r}
library(dplyr)
library(syuzhet)
library(tidyverse)
library(tidytext)
library(NLP)
library(tm)
library(wordcloud)
library(stopwords)
library(RColorBrewer)
library(ggplot2)
```


```{r}
options(repos=list(CRAN="http://cran.rstudio.com/"))
pk_nec = c("RedditExtractoR")
pk_inst <- pk_nec %in% installed.packages() 
if (length(pk_nec[!pk_inst])>0) install.packages(pk_nec[!pk_inst])
lapply(pk_nec, require, character.only=TRUE)
rm(pk_inst)
rm(pk_nec)
```

```{r}
## Descargar los hilos de la comunidad
threads1 <- find_thread_urls(keywords = "unemployment, job loss, being unemployed, laid off, unemployment benefits, mental health, depression, anxiety, stress, emotional well-being, unemployment and COVID-19", subreddit = "unemployment, jobs, recruitinghell, mentalhealth, depression, anxiety, COVID-19, coronavirus",sort_by = "new", period = "all")
comments1 <- get_thread_content(threads1$url)
```

```{r}
#Creacci√≥n de data frame con los comentarios 
desempleo <- data.frame(comments1$comments)
View(desempleo)
```

Tiene 10 columnas, √∫nicamente nos quedaremos con la columna de los comentarios y la de fecha 

```{r}
desempleo <- select(comments1$comments, -1, -2, -4, -5, -6, -7, -8, -10)
View(desempleo)
max(desempleo$date)
min(desempleo$date)
```
La fecha del √∫ltimo comentario descargado es a fecha de descarga, es decir, 16/06/2023 y la fecha m√°s antigua es el 13/04/2023. 

#LIMPIEZA DE DATOS 
##1. Eliminaci√≥n de signos y car√°cteres
```{r}
desempleo$comment <- gsub("@","", desempleo$comment)
desempleo$comment <- gsub("http[^[:space:]]*", "", desempleo$comment)
desempleo$comment <- gsub("#", "", desempleo$comment)
desempleo$comment <- gsub("\\?", "", desempleo$comment)
desempleo$comment <- gsub("\\¬ø", "", desempleo$comment)
desempleo$comment <- gsub("\\!", "", desempleo$comment)
desempleo$comment <- gsub("\\¬°", "", desempleo$comment)
desempleo$comment <- gsub("\\,", "", desempleo$comment)
desempleo$comment <- gsub("\\|", "", desempleo$comment)
desempleo$comment <- gsub("‚Äú", "", desempleo$comment)
desempleo$comment <- gsub("‚Äù", "", desempleo$comment)
desempleo$comment <- gsub("‚Äò", "", desempleo$comment)
desempleo$comment <- gsub("‚Äô", "", desempleo$comment)
desempleo$comment <- gsub(":", "", desempleo$comment)
desempleo$comment <- gsub("\\.", "", desempleo$comment)
desempleo$comment <- gsub('"', '', desempleo$comment)
desempleo$comment <- gsub("\\(", "", desempleo$comment)
desempleo$comment <- gsub("\\)", "", desempleo$comment)
desempleo$comment <- gsub("\\-", "", desempleo$comment)
desempleo$comment <- gsub("\\_", "",desempleo$comment)
desempleo$comment <- gsub("\\=", "", desempleo$comment)
desempleo$comment <- gsub("\\/", "", desempleo$comment)
desempleo$comment <- gsub("\\¬´", "", desempleo$comment)
desempleo$comment <- gsub("\\¬ª", "", desempleo$comment)
desempleo$comment <- gsub("\\*", "", desempleo$comment)
```

##2. Eliminaci√≥n de spam
```{r}
desempleo <- desempleo %>% distinct(comment, .keep_all = TRUE)
```

##3. Craci√≥n variable fecha 

```{r}
desempleo$date <- as.Date(desempleo$date)
```

```{r}
antes <- desempleo %>% filter(date< "2023-06-05")
despues <- desempleo %>% filter(date > "2023-06-05")
```


#AN√ÅLISIS DE SENTIMIENTO

```{r}
#sentimiento sobre la empresa en general 
Sentimiento_desempleo <- get_nrc_sentiment(desempleo$comment)
```

```{r}
#antes del lanzamiento de las desempleo Vision Pro
Sentimiento_antes_lanz <- get_nrc_sentiment(antes$comment)
```

```{r}
#despu√©s del lanzamiento de las desempleo Vision Pro 
Sentimiento_despues_lanz <- get_nrc_sentiment(despues$comment)
```

```{r}



theme_elegante <- function(base_size = 10,
                           base_family = "Raleway"
                           )
    {
    color.background = "#FFFFFF" # Chart Background
    color.grid.major = "#D9D9D9" # Chart Gridlines
    color.axis.text = "#666666" # 
    color.axis.title = "#666666" # 
    color.title = "#666666"
    color.subtitle = "#666666"
    strip.background.color = '#9999CC'
    
    ret <-
        theme_bw(base_size=base_size) +
        
        # Set the entire chart region to a light gray color
        theme(panel.background=element_rect(fill=color.background, color=color.background)) +
        theme(plot.background=element_rect(fill=color.background, color=color.background)) +
        theme(panel.border=element_rect(color=color.background)) +
        
        # Format the grid
        theme(panel.grid.major=element_line(color=color.grid.major,size=.55, linetype="dotted")) +
        theme(panel.grid.minor=element_line(color=color.grid.major,size=.55, linetype="dotted")) +
        theme(axis.ticks=element_blank()) +
        
        # Format the legend, but hide by default
        theme(legend.position="none") +
        theme(legend.background = element_rect(fill=color.background)) +
        theme(legend.text = element_text(size=base_size-3,color=color.axis.title, family = base_family)) +
        
        theme(strip.text.x = element_text(size=base_size,color=color.background, family = base_family)) +
        theme(strip.text.y = element_text(size=base_size,color=color.background, family = base_family)) +
        #theme(strip.background = element_rect(fill=strip.background.color, linetype="blank")) +
        theme(strip.background = element_rect(fill = "grey70", colour = NA)) +
        # theme(panel.border= element_rect(fill = NA, colour = "grey70", size = rel(1)))+
        # Set title and axis labels, and format these and tick marks
        theme(plot.title=element_text(color=color.title, 
                                      size=20, 
                                      vjust=1.25, 
                                      family=base_family, 
                                      hjust = 0.5
                                      )) +
        
        theme(plot.subtitle=element_text(color=color.subtitle, size=base_size+2, family = base_family,  hjust = 0.5))  +
        
        theme(axis.text.x=element_text(size=base_size,color=color.axis.text, family = base_family)) +
        theme(axis.text.y=element_text(size=base_size,color=color.axis.text, family = base_family)) +
        theme(text=element_text(size=base_size, color=color.axis.text, family = base_family)) +
        
        theme(axis.title.x=element_text(size=base_size+2,color=color.axis.title, vjust=0, family = base_family)) +
        theme(axis.title.y=element_text(size=base_size+2,color=color.axis.title, vjust=1.25, family = base_family)) +
        theme(plot.caption=element_text(size=base_size-2,color=color.axis.title, vjust=1.25, family = base_family)) +
        
        # Legend  
        theme(legend.text=element_text(size=base_size,color=color.axis.text, family = base_family)) +
        theme(legend.title=element_text(size=base_size,color=color.axis.text, family = base_family)) +
        theme(legend.key=element_rect(colour = color.background, fill = color.background)) +
        theme(legend.position="bottom", 
              legend.box = "horizontal", 
              legend.title = element_blank(),
              legend.key.width = unit(.75, "cm"),
              legend.key.height = unit(.75, "cm"),
              legend.spacing.x = unit(.25, 'cm'),
              legend.spacing.y = unit(.25, 'cm'),
              legend.margin = margin(t=0, r=0, b=0, l=0, unit="cm")) +

        # Plot margins
        theme(plot.margin = unit(c(.5, .5, .5, .5), "cm"))
    
    ret
}
```

```{r}
# Crear un corpus de texto
corpus <- Corpus(VectorSource(unemployment_2$comment))

# Definir stopwords adicionales
additional_stopwords <- c("upvote", "upvotes", "view", "views", "y", "dfollow", "answer", "answers")

# Limpiar el corpus de texto
clean_corpus <- corpus %>%
  tm_map(content_transformer(tolower)) %>%
  tm_map(removePunctuation) %>%
  tm_map(removeNumbers) %>%
  tm_map(removeWords, c(stopwords("english"), additional_stopwords)) %>%
  tm_map(stripWhitespace) %>%
  tm_map(content_transformer(lemmatize_strings))

# Crear una Document-Term Matrix (DTM) para bigramas
BigramTokenizer <- function(x) {
  unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
}
dtm_bigrams <- DocumentTermMatrix(clean_corpus, control = list(tokenize = BigramTokenizer))

# Calcular el TF-IDF
tfidf <- weightTfIdf(dtm_bigrams)
tfidf_matrix <- as.matrix(tfidf)

# Calcular la suma de TF-IDF para cada t√©rmino
top_terms <- sort(colSums(tfidf_matrix), decreasing = TRUE)

# Convertir a un data frame
top_terms_df <- data.frame(term = names(top_terms), tfidf = top_terms)

# Filtrar los t√©rminos espec√≠ficos y aquellos que contienen "..." o "'" o "‚Äô"
top_terms_df <- top_terms_df %>%
  filter(!term %in% c("httpswwwquoracomunansweredhowdoiprotectemployeesandbusinessesincovid", 
                      "httpswwwquoracomwhataresomementalhealthtipsduringcovid")) %>%
  filter(!grepl("\\.\\.\\.", term)) %>%  # Filtrar t√©rminos que contienen "..."
  filter(!grepl("'", term)) %>%  # Filtrar t√©rminos que contienen "'"
  filter(!grepl("‚Äô", term)) %>%  # Filtrar t√©rminos que contienen "‚Äô"
  arrange(desc(tfidf)) %>%
  head(15)

# Imprimir el data frame resultante
print(top_terms_df)

# Graficar los principales t√©rminos por TF-IDF con una paleta de colores diferente
ggplot(top_terms_df, aes(x = reorder(term, tfidf), y = tfidf, fill = term)) +
  geom_bar(stat = "identity", color = "black")  +  # Usar la paleta de colores viridis
  labs(title = "Top 15 t√©rminos  por TF-IDF", x = "T√©rmino", y = "TF-IDF") +
  theme_elegante() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1), 
        legend.position = "none",
        axis.title.x = element_text(face = "bold"),  # T√≠tulo del eje x en negrita
        axis.title.y = element_text(face = "bold"))  # T√≠tulo del eje y en negrita
```

```{r}

library(ggwordcloud)
library(viridis)
# Combine all cleaned text into a single data frame
cleaned_text <- data.frame(text = sapply(clean_corpus, as.character), stringsAsFactors = FALSE)

# Remove unwanted characters (".", "...", "'", "‚Äô", "s", "¬∑", "‚Äú‚Äù, etc.)
cleaned_text$text <- gsub("[\\.]{2,}", "", cleaned_text$text)  # Remove multiple dots
cleaned_text$text <- gsub("'", "", cleaned_text$text)
cleaned_text$text <- gsub("‚Äô", "", cleaned_text$text)
cleaned_text$text <- gsub("¬∑", "", cleaned_text$text)
cleaned_text$text <- gsub("and|the|of|j‚Äô‚Äô|‚Äú|‚Äù", "", cleaned_text$text)  # Remove specific words and characters
cleaned_text$text <- gsub("[[:punct:]]", "", cleaned_text$text)  # Remove punctuation
cleaned_text$text <- gsub("\\s+", " ", cleaned_text$text)  # Remove extra whitespace

# Tokenize the text
words <- unlist(strsplit(cleaned_text$text, " "))

# Remove empty strings and unwanted short words from the tokenized words
unwanted_words <- c("p", "o", "ob", "s", "j", "t", "b", "de", "c", "e", "	ùêä", "quora", "don", "k", "re", "etc", "youre", "m", "s", "also")
words <- words[words != "" & !words %in% unwanted_words]

# Create word frequency table
word_freq <- table(words)
word_freq_df <- as.data.frame(word_freq)
colnames(word_freq_df) <- c("word", "freq")

# Filter words with a minimum frequency
word_freq_df <- word_freq_df[word_freq_df$freq >= 100,]

# Plot word cloud using ggwordcloud
ggplot(word_freq_df, aes(label = word, size = freq, color = freq)) +
  geom_text_wordcloud_area(rm_outside = TRUE) +  # Ensure words don't overlap and stay within bounds
  scale_size_area(max_size = 30) +
  scale_color_viridis_c() +  # Set color scheme for the text
  theme_elegante() +
  labs(title = "Unemployment and mental health", 
       subtitle = "M√≠nima frecuencia: 100") +
  theme(
    plot.title = element_text(size = 20, hjust = 0.5, vjust = 1, face = "bold"),  # Centered title at the top
    plot.subtitle = element_text(size = 12, hjust = 0.5, color = "darkgrey"),  # Centered subtitle below the title
    plot.margin = margin(10, 10, 10, 10)
  )

```





#NUBE DE PALABRAS 
```{r}
#fijaci√≥n de semilla 
set.seed(13)
```

##Nube de palabras completa
```{r}
desempleo$comment %>% VectorSource() %>% VCorpus() %>% tm_map(removeWords,
stopwords("english")) %>%
wordcloud(scale=c(3, 0.4), min.freq = 70, max.words=100, color = brewer.pal(8, "Dark2"))
```
```{r}
# Sumarizar las emociones
emotion_counts <- colSums(Sentimiento_desempleo[, 1:8])

# Crear un dataframe para facilitar la visualizaci√≥n
emotion_df <- data.frame(emotion = names(emotion_counts), count = emotion_counts)

# Crear un gr√°fico de barras para las emociones
ggplot(emotion_df, aes(x = reorder(emotion, -count), y = count)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  labs(title = "Distribuci√≥n de Emociones en los Comentarios de mental",
       x = "Emoci√≥n", y = "Frecuencia") +
  theme_minimal()
```

```{r}

# Sumarizar las polaridades
polarity_counts <- colSums(Sentimiento_desempleo[, 9:10])

# Crear un dataframe para facilitar la visualizaci√≥n
polarity_df <- data.frame(polarity = names(polarity_counts), count = polarity_counts)

# Crear un gr√°fico de barras para las polaridades
ggplot(polarity_df, aes(x = reorder(polarity, -count), y = count)) +
  geom_bar(stat = "identity", fill = "lightgreen") +
  labs(title = "Distribuci√≥n de Polaridades en los Comentarios de mental",
       x = "Polaridad", y = "Frecuencia") +
  theme_minimal()
```


```{r}
# Supongamos que tienes una columna "date" en el dataframe mental
# Convertir las fechas a formato Date
desempleo$date <- as.Date(desempleo$date)

# A√±adir las emociones al dataframe original
desempleo <- cbind(desempleo, Sentimiento_desempleo)

# Calcular la media de cada emoci√≥n por fecha
emotion_trends <- desempleo%>%
  group_by(date) %>%
  summarise(across(anger:positive, mean))

# Convertir a formato largo para ggplot2
emotion_trends_long <- emotion_trends %>%
  gather(key = "emotion", value = "average_score", -date)

# Crear un gr√°fico de l√≠neas para las emociones a lo largo del tiempo
ggplot(emotion_trends_long, aes(x = date, y = average_score, color = emotion)) +
  geom_line() +
  labs(title = "Tendencias de Emociones en los Comentarios de mental a lo largo del Tiempo",
       x = "Fecha", y = "Puntuaci√≥n Promedio de Emoci√≥n") +
  theme_minimal()
```


```{r}

# Gr√°fico de √°rea para las tendencias de emociones a lo largo del tiempo
ggplot(emotion_trends_long, aes(x = date, y = average_score, fill = emotion)) +
  geom_area() +
  labs(title = "Tendencias de Emociones en los Comentarios de mental a lo largo del Tiempo",
       x = "Fecha", y = "Puntuaci√≥n Promedio de Emoci√≥n", fill = "Emoci√≥n") +
  theme_minimal()
```
```{r}
library(tidytext)
library(igraph)
library(ggraph)




# Funci√≥n para limpiar los comentarios
clean_comment <- function(comment) {
  comment %>%
    tolower() %>%
    removeWords(stopwords) %>%           # Remover stopwords
    removePunctuation() %>%              # Remover puntuaci√≥n
    removeNumbers() %>%                  # Remover n√∫meros
    str_replace_all("http[^[:space:]]*", "")  # Remover URL
}

# Limpiar los comentarios y crear bigramas
cleaned_bigramas <- comments1$comment %>%
  clean_comment() %>%
  tibble(comment = .) %>%
  unnest_tokens(bigram, comment, token = "ngrams", n = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  filter(!word1 %in% stopwords & !word2 %in% stopwords)  # Filtrar stopwords en los bigramas

# Filtrar los bigramas con frecuencia mayor o igual a 5
filtered_bigramas <- cleaned_bigramas %>%
  count(word1, word2) %>%
  filter(n > 50)

# Crear el gr√°fico de red
set.seed(175)
graph <- filtered_bigramas %>%
  graph_from_data_frame()

# Visualizar el gr√°fico de red
ggraph(graph, layout = "auto") +
  geom_edge_link(arrow = arrow(type = "closed", length = unit(.075, "inches")), color = "gray70", size = 0.5) +
  geom_node_point(color = "lightgray", size = 6, alpha = 0.8) +
  geom_node_text(aes(label = name), vjust = 0, hjust = 0, size = 4, color = "black") +
  theme_void() +
  labs(title = "Red de Bigramas en los Comentarios de Reddit (Limpiada)",
       subtitle = "Bigramas con una frecuencia mayor o igual a 5 y sin stopwords",
       caption = "Fuente: Comentarios de Reddit en el subreddit 'work' (Limpio)") +
  theme(plot.title = element_text(size = 14, face = "bold"),
        plot.subtitle = element_text(size = 10),
        plot.caption = element_text(size = 8))

```

```{r}

library(tidytext)
library(reshape2)

# Convertir el corpus a un dataframe
comments_df <- data.frame(text = desempleo$comment)

# Tokenizar los comentarios
comments_tokens <- comments_df %>%
  unnest_tokens(word, text)

# Unir los comentarios con el sentimiento de las palabras
comments_sentiment <- comments_tokens %>%
  inner_join(get_sentiments("bing"))

# Contar las palabras por sentimiento
word_sentiment_count <- comments_sentiment %>%
  count(word, sentiment, sort = TRUE)

# Convertir los datos a un formato adecuado para comparison.cloud
word_sentiment_matrix <- word_sentiment_count %>%
  acast(word ~ sentiment, value.var = "n", fill = 0)

# Crear la nube de palabras
comparison.cloud(word_sentiment_matrix,
                  colors = c("#F8766D", "#00BFC4"),
                  max.words = 100)
```


```{r}
sentimientos <- get_sentiment(desempleo$comment)
summary(sentimientos)
```


```{r}
# Aseg√∫rate de que la columna date est√© en formato POSIXct
desempleo$date <- as.POSIXct(desempleo$date)

# Crear el data frame sentimientosdf y ordenar las fechas
sentimientosdf <- data.frame(sentimientos)
sentimientosdf$date <- sort(desempleo$date)

# Crear el gr√°fico
ggplot(sentimientosdf, aes(x = date, y = sentimientos)) +
  geom_line(linewidth = 1.0, colour = "grey") +
  geom_smooth(method = "loess", se = FALSE, colour = "red", linewidth = 1.2) + 
  theme_bw() +
  scale_x_datetime(date_breaks = "1 month") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.3)) +
  labs(title = "Trayectoria emocional de los comentarios", x = "Fecha", y = "Grado emocional")
```


```{r}
porcent_val <- get_percentage_values(sentimientos, bins = 50)
plot(porcent_val, type = "l", col = "blue", 
     main = "Comentarios empleando medias basadas en %", 
     xlab = "Evoluci√≥n", ylab = "Grado emocional") 
abline(h = 0, lty = 2, col = "grey")
```


```{r}
simple_plot(sentimientos)
```

```{r}
emociones <- get_nrc_sentiment(desempleo$comment) 
ggplot(emociones, aes(x = desempleo$date)) +
  geom_smooth(aes(y = positive, colour = "Positivo"), method = "loess", size = 1.2, se = FALSE) +
  geom_smooth(aes(y = negative, colour = "Negativo"), method = "loess", size = 1.2, se = FALSE) +
  scale_colour_manual(values = c("Positivo" = "#62ca35", "Negativo" = "#dc143c")) +
  theme_bw() +
  scale_x_datetime(date_breaks = "1 month") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.3)) +
  labs(title = "Sentimientos", x = "Fecha", y = "Grado Emocional")
```

```{r}
emociones2 <- emociones
colnames(emociones2) <- c("Ira", "Anticipacion", "Disgusto", "Miedo", "Alegria", "Tristeza",
                          "Sorpresa", "Confianza", "Negativo", "Positivo")
emociones2$fecha <- desempleo$date

# Transformar los datos a formato largo
emociones2_long <- gather(emociones2, key = "emocion", value = "sentimiento", -fecha)

# Filtrar las emociones que no sean "Negativo" y "Positivo"
emociones2_long <- subset(emociones2_long, emocion != "Negativo" & emocion != "Positivo")

# Convertir 'emocion' a factor
emociones2_long$emocion <- as.factor(emociones2_long$emocion)

# Crear el gr√°fico
ggplot(emociones2_long, aes(x = fecha, y = sentimiento, color = emocion)) +
  geom_smooth(method = "loess", size = 1.2, se = FALSE) + 
  theme_bw() +
  scale_color_manual(values = brewer.pal(8, "Dark2"), name = "Emoci√≥n") +
  scale_x_datetime(date_breaks = "1 month") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.3)) +
  labs(title = "Trayectoria emocional en los comentarios", x = "Fecha", y = "Grado emocional")
```

```{r}
# Cargar las bibliotecas necesarias
library(dplyr)
library(ggplot2)
library(udpipe)
library(textrank)
library(tidytext)
library(lubridate)

# Procesamiento del texto con UDPipe
ud_model <- udpipe_download_model(language = "english")
udpipe_model <- udpipe_load_model(ud_model$file_model)

# Anotaci√≥n del texto
anot <- udpipe_annotate(udpipe_model, x = desempleo$comment)
anot <- as.data.frame(anot)

# Filtrar sustantivos
terminos_n <- subset(anot, upos == "NOUN")

# Calcular la frecuencia de los lemas de sustantivos
terminos_n <- txt_freq(terminos_n$lemma)

# Convertir los t√©rminos a factores con niveles ordenados
terminos_n$key <- factor(terminos_n$key, levels = rev(terminos_n$key))

# Graficar los sustantivos m√°s frecuentes
ggplot(head(terminos_n, 20), aes(x = key, y = freq)) +
  geom_bar(stat = "identity", fill = "cadetblue") +
  coord_flip() +
  labs(title = "Sustantivos m√°s frecuentes", x = "Frecuencia", y = "Sustantivos") +
  theme_minimal()
```

```{r}

# Filtrar nombres propios
terminos_np <- subset(anot, upos == "PROPN")

# Calcular la frecuencia de los lemas de nombres propios
terminos_np <- txt_freq(terminos_np$lemma)

# Convertir los t√©rminos a factores con niveles ordenados
terminos_np$key <- factor(terminos_np$key, levels = rev(terminos_np$key))

# Graficar los nombres propios m√°s frecuentes
grafico_nombres_propios <- ggplot(head(terminos_np, 20), aes(x = key, y = freq)) +
  geom_bar(stat = "identity", fill = "cadetblue") +
  coord_flip() +
  labs(title = "Nombres propios m√°s frecuentes", x = "Frecuencia", y = "Nombres propios") +
  theme_minimal()
grafico_nombres_propios
```

```{r}
# Filtrar adjetivos
terminos_a <- subset(anot, upos == "ADJ")

# Calcular la frecuencia de los lemas de adjetivos
terminos_a <- txt_freq(terminos_a$lemma)

# Convertir los t√©rminos a factores con niveles ordenados
terminos_a$key <- factor(terminos_a$key, levels = rev(terminos_a$key))

# Graficar los adjetivos m√°s frecuentes
grafico_adjetivos <- ggplot(head(terminos_a, 20), aes(x = key, y = freq)) +
  geom_bar(stat = "identity", fill = "cadetblue") +
  coord_flip() +
  labs(title = "Adjetivos m√°s frecuentes", x = "Frecuencia", y = "Adjetivos") +
  theme_minimal()
grafico_adjetivos
```


```{r}
terminos_v <- subset(anot, upos == "VERB")
terminos_v <- txt_freq(terminos_v$lemma)
terminos_v$key <- factor(terminos_v$key, levels = rev(terminos_v$key))
# Graficar los verbos m√°s frecuentes
grafico_verbos <- ggplot(head(terminos_v, 20), aes(x = key, y = freq)) +
  geom_bar(stat = "identity", fill = "cadetblue") +
  coord_flip() +
  labs(title = "Verbos m√°s frecuentes", x = "Frecuencia", y = "Verbos") +
  theme_minimal()
grafico_verbos
```


```{r}
kwrake <- keywords_rake(x = anot, term = "lemma", group = "doc_id", # Agrupamos por documento (tweet)
                    relevant = anot$upos %in% c("NOUN", "ADJ","PROPN")) # S√≥lo sustantivos, adjetivos y
                                                                        #  nombres propios
kwrake$key <- factor(kwrake$keyword, levels = rev(kwrake$keyword))

# Graficamos palabras clave que aparezcan al menos 5 veces
barchart(key ~ rake, data = head(subset(kwrake, freq > 4), 20), col = "cadetblue",
         main = list("Identificar palabras clave mediante RAKE", cex="1"), xlab = "RAKE")
```


```{r}
kwpmi <- keywords_collocation(x = anot, term = "lemma", group = "doc_id")
kwpmi$key <- factor(kwpmi$keyword, levels = rev(kwpmi$keyword))

barchart(key ~ pmi, data = head(subset(kwpmi, freq > 9), 20), col = "cadetblue",
         main = list("Identificar palabras clave mediante Colocaci√≥n PMI", cex="1"),
         xlab = "PMI (Pointwise Mutual Information)")
```

```{r}
anot$phrase_tag <- as_phrasemachine(anot$upos, type = "upos")

# Cambiar etiqueta a "pronombres" y "n√∫meros"
anot$phrase_tag[anot$upos=="PRON"] <- "O"
anot$phrase_tag[anot$upos=="NUM"] <- "O"

# Obtenemos las frases nominales simples (expresi√≥n regular)
kwphrases <- keywords_phrases(x = anot$phrase_tag, term = tolower(anot$token),
                              pattern = "(A|N)*N(P+D*(A|N)*N)*",
                              is_regex = TRUE, detailed = FALSE)

# Filtramos aquellas que contienen m√°s de una palabra y aparecen m√°s de 3 veces
kwphrases <- subset(kwphrases, ngram > 1 & freq > 3)
kwphrases$key <- factor(kwphrases$keyword, levels = rev(kwphrases$keyword))

barchart(key ~ freq, data = head(kwphrases, 20), col = "cadetblue",
         main = list("Palabras Clave - Frases nominales simples", cex="1"), xlab = "Frecuencia")
```


```{r}

keywords <- textrank_keywords(anot$lemma, relevant = anot$upos %in% c("NOUN", "ADJ","PROPN"),
                              ngram_max = 8, sep = " ") # S√≥lo sustantivos, adjetivos y nombres propios

# Filtramos aquellas que contienen m√°s de una palabra y aparecen m√°s de 3 veces
keywords <- subset(keywords$keywords, ngram > 1 & freq > 5) 
keywords$keyword <- factor(keywords$keyword, levels = rev(keywords$keyword))

# Nube de palabras clave m√°s frecuentes
wordcloud(words=keywords$keyword, freq=keywords$freq, random.order=F, colors=brewer.pal(8,"Dark2"))
```


```{r}
library(formattable)
terminos <- subset(anot, upos %in% c("ADJ", "NOUN", "PROPN") & 
                     !lemma %in% c("birth control"))

# Qu√© palabras coocurren por tweet
cooc <- cooccurrence(terminos, group = "doc_id", term = "lemma")
head(data.frame(cooc)) %>% formattable()    
```

```{r}
wordnetwork <- graph_from_data_frame(head(cooc, 50))
ggraph(wordnetwork, layout = "fr") +
  geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = "pink") +
  geom_node_text(aes(label = name), col = "darkgreen", size = 3.5) +
  theme_graph(base_family = "sans") + theme(legend.position = "none") +
  labs(title = "Coocurrencias tweet/oraci√≥n", 
       subtitle = "Sustantivos, Adjetivos y Nombres Propios") +
  theme(plot.title = element_text(size = 14), plot.margin = margin(0,1,0,1))
```

```{r}
# Graficamos coocurrencias con skipgram = 1
wordnetwork <- graph_from_data_frame(head(cooc, 50))
ggraph(wordnetwork, layout = "fr") +
  geom_edge_link(aes(width = cooc, edge_alpha = cooc),edge_colour = "pink") +
  geom_node_text(aes(label = name), col = "darkgreen", size = 3.5) +
  theme_graph(base_family = "sans") +
  labs(title = "Qu√© palabras est√°n pr√≥ximas", subtitle = "Sustantivos, Adjetivos y Nombres Propios") +
  theme(plot.title = element_text(size = 14), plot.margin = margin(0,1,0,1))
```

```{r}
# Identificador √∫nico de cada oraci√≥n del corpus
anot$id <- unique_identifier(anot, fields = c("doc_id", "sentence_id"))

# Seleccionamos s√≥lo sustantivos, adjetivos y nombres propios
dtf <- subset(anot, upos %in% c("NOUN", "ADJ", "PROPN"))

# Creamos matriz documento-t√©rmino
dtf <- document_term_frequencies(dtf, document = "id", term = "lemma")
dtm <- document_term_matrix(dtf)

# Eliminamos t√©rminos que aparezcan menos de 10 veces
dtm <- dtm_remove_lowfreq(dtm, minfreq = 10)

# Eliminamos algunas palabras que no aportan nada al an√°lisis
dtm <- dtm_remove_terms(dtm, terms = c("birth control"))
```

```{r}
termcorr <- dtm_cor(dtm)
dim(termcorr)
```

```{r}
library(formattable)
library(dplyr)

# Crear un ejemplo de termcorr
set.seed(123)
termcorr <- matrix(runif(100, min = -1, max = 1), ncol = 10)
colnames(termcorr) <- paste0("Term", 1:10)
rownames(termcorr) <- paste0("Term", 1:10)

# Redondear y convertir a data frame
df <- as.data.frame(round(termcorr[1:10, 1:5], 2))

# Verificar si los datos est√°n correctamente cargados
print(df)

# Aplicar formattable con estilo
formatted_df <- formattable(df, list(
  area(col = 1:5) ~ color_tile("white", "blue"),
  area(row = 1:10, col = 1:5) ~ formatter(
    "span", style = x ~ style(display = "block", padding = "0 4px"))
))

# Mostrar el resultado
formatted_df

```


# WORD EMBEDDINGS 

```{r}
terminos <- subset(anot, upos %in% c("NOUN", "ADJ", "PROPN"), select = c("doc_id", "lemma"))
terminos <- split(terminos$lemma, terminos$doc_id)
terminos <- lapply(terminos, paste, collapse = " ")
docs <- do.call(rbind.data.frame, c(terminos, stringsAsFactors=FALSE))
colnames(docs) <- "texto"
docs$id <- as.numeric(str_sub(names(terminos),start = 4))
docs <- docs[order(docs$id), c("id", "texto")]
row.names(docs) <- NULL
rm(terminos)
```

```{r}
library(text2vec)
tokens <- space_tokenizer(docs$texto)
```

```{r}
# Crear iterador
it <- itoken(tokens, progressbar = FALSE)

# Crear el vocabulario
vocab <- create_vocabulary(it)
vocab <- vocab[order(vocab$term_count, decreasing = T),]
vocab$doc_porc <- vocab$doc_count/length(docs$texto)  # Porcentaje

# Filtramos los t√©rminos del vocabulario en aquellos que aparecen al menos 7 veces y que no aparezcan 
# en m√°s del 50% de los documentos (tweets)
vocab <- prune_vocabulary(vocab, term_count_min = 7, doc_proportion_max = 0.5)
```

```{r}
# Definir la funci√≥n de vectorizaci√≥n
vectorizer <- vocab_vectorizer(vocab)

# Construir la matriz de coocurrencia de t√©rminos
tcm <- create_tcm(it, vectorizer, skip_grams_window = 5)
```

```{r}
glove <- GlobalVectors$new(rank = 20, x_max = 10)
```

```{r}
wv_main <- glove$fit_transform(tcm, n_iter = 50, convergence_tol = 0.005)
```

```{r}
wv_context <- glove$components
dim(wv_context)
```
```{r}
word_vectors <- wv_main + t(wv_context)
```

```{r}
# vector("exposici√≥n")
wv <- word_vectors["job", ,drop = FALSE]
# Palabras relacionadas con "exposici√≥n"
cos_sim <- sim2(x = word_vectors, y = wv, method = "cosine", norm = "l2")
head(sort(cos_sim[,1], decreasing = TRUE), 5)
```
```{r}
# vector("anunciaci√≥n")
wv <- word_vectors["stress", ,drop = FALSE]
# Palabras relacionadas con "anunciaci√≥n"
cos_sim <- sim2(x = word_vectors, y = wv, method = "cosine", norm = "l2")
head(sort(cos_sim[,1], decreasing = TRUE), 5)
```
```{r}
# vector("mujer")
wv <- word_vectors["benefit", ,drop = FALSE]
# Palabras relacionadas con "mujer"
cos_sim <- sim2(x = word_vectors, y = wv, method = "cosine", norm = "l2")
head(sort(cos_sim[,1], decreasing = TRUE), 5)
```


```{r}
library(Rtsne)
library(ggplot2)
library(ggrepel)

# Suponiendo que word_vectors ya est√° definido
tsne <- Rtsne(word_vectors, dims = 2, perplexity = 20, max_iter = 2000)
tsne_plot <- data.frame(x = tsne$Y[,1], y = tsne$Y[,2], palabra = rownames(word_vectors))

# Crear el gr√°fico
ggplot(tsne_plot, aes(x, y)) +
  geom_text_repel(aes(label = palabra, color = palabra), size = 3.5, box.padding = 0.3, max.overlaps = 10) +
  scale_color_viridis_d() +  # Utilizar una paleta de colores adecuada
  labs(title = "Modelo GloVe (t-SNE)", x = "Dimensi√≥n 1", y = "Dimensi√≥n 2") +
  theme_minimal(base_size = 15) +  # Utilizar un tema minimalista con tama√±o base mayor
  theme(legend.position = "none",  # Quitar la leyenda si no es necesaria
        plot.title = element_text(hjust = 0.5),  # Centrar el t√≠tulo
        plot.background = element_rect(fill = "white"),
        panel.grid.major = element_line(color = "gray85"),
        panel.grid.minor = element_blank(),
        panel.border = element_blank())

```



